import os
import cv2
import numpy as np
import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from skimage.feature import hog, graycomatrix, graycoprops
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
from keras.preprocessing.image import ImageDataGenerator

def compute_glrlm_features(image):
    """Calcule les caractéristiques GLRLM d'une image en niveaux de gris."""
    gray_levels = 256
    height, width = image.shape
    glrlm = np.zeros((gray_levels, width), dtype=np.int32)
    
    for i in range(height):
        run_length = 0
        previous_pixel = image[i, 0]
        
        for j in range(width):
            current_pixel = image[i, j]
            if current_pixel == previous_pixel:
                run_length += 1
            else:
                if run_length > 0:
                    glrlm[previous_pixel, run_length - 1] += 1
                run_length = 1
                previous_pixel = current_pixel
        
        if run_length > 0:
            glrlm[previous_pixel, run_length - 1] += 1
    
    # Extraction des caractéristiques
    short_run_emphasis = np.sum(glrlm / (1 + np.arange(width))**2)
    long_run_emphasis = np.sum(glrlm * (1 + np.arange(width))**2)
    run_percentage = np.sum(glrlm) / (height * width)
    run_entropy = -np.sum(glrlm * np.log2(glrlm + 1e-10))
    
    return np.array([short_run_emphasis, long_run_emphasis, run_percentage, run_entropy])

def extract_hog_features(image):
    fd, _ = hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)
    return fd

def extract_orb_features(image, max_features=500):
    orb = cv2.ORB_create(nfeatures=max_features)
    keypoints, descriptors = orb.detectAndCompute(image, None)
    
    if descriptors is None:
        descriptors = np.zeros((max_features, 32), dtype=np.uint8)
    
    if descriptors.shape[0] < max_features:
        padding = np.zeros((max_features - descriptors.shape[0], 32), dtype=np.uint8)
        descriptors = np.vstack((descriptors, padding))
    
    return descriptors.flatten()

def extract_glcm_features(image):
    glcm = graycomatrix(image, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)
    contrast = graycoprops(glcm, 'contrast')[0, 0]
    homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]
    energy = graycoprops(glcm, 'energy')[0, 0]
    correlation = graycoprops(glcm, 'correlation')[0, 0]
    return np.array([contrast, homogeneity, energy, correlation])

def augment_image(image):
    datagen = ImageDataGenerator(rotation_range=20, horizontal_flip=True, zoom_range=0.2)
    image = np.expand_dims(image, axis=0)
    augmented_images = [image]
    
    for batch in datagen.flow(image, batch_size=1):
        augmented_images.append(batch[0])
        if len(augmented_images) >= 3:
            break
    
    return [img.squeeze() for img in augmented_images]

def create_dataset(image_dir, annotation_dir):
    features = []
    labels = []
    
    image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')]
    
    for image_path in image_paths:
        txt_file = os.path.join(annotation_dir, os.path.basename(image_path).replace('.jpg', '.txt').replace('.png', '.txt'))
        
        if os.path.exists(txt_file):
            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
            augmented_images = augment_image(img)
            
            for aug_img in augmented_images:
                hog_features = extract_hog_features(aug_img)
                orb_features = extract_orb_features(aug_img)
                glcm_features = extract_glcm_features(aug_img)
                glrlm_features = compute_glrlm_features(aug_img)
                
                feature_vector = np.hstack((hog_features, orb_features, glcm_features, glrlm_features))
                features.append(feature_vector)
                
                num_objects = count_objects_in_dota_annotation(txt_file)
                labels.append(num_objects)
    
    return np.array(features, dtype=np.float32), np.array(labels, dtype=np.float32)

def train_model(features, labels):
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    model = xgb.XGBRegressor(objective='reg:squarederror', colsample_bytree=0.3, learning_rate=0.1,
                             max_depth=5, alpha=10, n_estimators=100)
    model.fit(features_scaled, labels)
    
    return model, scaler

def main():
    image_dir = 'path_to_images'
    annotation_dir = 'path_to_annotations'
    
    features, labels = create_dataset(image_dir, annotation_dir)
    features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2, random_state=42)
    
    model, scaler = train_model(features_train, labels_train)
    
    features_test_scaled = scaler.transform(features_test)
    predictions = model.predict(features_test_scaled)
    
    print(f"MAE : {mean_absolute_error(labels_test, predictions):.2f}")
    print(f"R² Score : {r2_score(labels_test, predictions):.2f}")
    
    import joblib
    joblib.dump(model, 'xgboost_model.pkl')
    joblib.dump(scaler, 'scaler.pkl')

if __name__ == '__main__':
    main()
