import os
import cv2
import numpy as np
from keras.models import Model
from keras.layers import Input, Dense, GlobalAveragePooling2D, Concatenate
from keras.applications import DenseNet121
from keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
from skimage.feature import hog, graycomatrix, graycoprops
import tensorflow as tf

# Fonction pour compter le nombre d'objets dans un fichier d'annotation DOTA
def count_objects_in_dota_annotation(txt_file):
    """Compter le nombre d'objets dans un fichier d'annotation DOTA."""
    with open(txt_file, 'r') as f:
        annotations = f.readlines()
    return len(annotations)  # Nombre d'objets détectés

# Fonction pour extraire les caractéristiques HOG
def extract_hog_features(image):
    """Extraire les caractéristiques HOG d'une image en niveaux de gris."""
    fd, _ = hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)
    return fd

# Fonction pour extraire les caractéristiques ORB
def extract_orb_features(image, max_features=500):
    """Extraire les caractéristiques ORB d'une image."""
    orb = cv2.ORB_create(nfeatures=max_features)
    keypoints, descriptors = orb.detectAndCompute(image, None)

    if descriptors is None:
        descriptors = np.zeros((max_features, 32), dtype=np.uint8)
    
    if descriptors.shape[0] < max_features:
        padding = np.zeros((max_features - descriptors.shape[0], 32), dtype=np.uint8)
        descriptors = np.vstack((descriptors, padding))

    return descriptors.flatten()

# Fonction pour extraire les caractéristiques GLCM
def extract_glcm_features(image):
    """Extraire les caractéristiques GLCM d'une image."""
    if image.dtype != np.uint8:
        image = (image * 255).astype(np.uint8)
    
    glcm = graycomatrix(image, distances=[1], angles=[0], levels=256, symmetric=True, normed=True)
    
    contrast = graycoprops(glcm, 'contrast')[0, 0]
    homogeneity = graycoprops(glcm, 'homogeneity')[0, 0]
    energy = graycoprops(glcm, 'energy')[0, 0]
    correlation = graycoprops(glcm, 'correlation')[0, 0]
    
    return np.array([contrast, homogeneity, energy, correlation])

# Fonction pour extraire les caractéristiques Hu Moments
def extract_hu_moments(image):
    """Extraire les moments de Hu d'une image."""
    moments = cv2.moments(image)
    hu_moments = cv2.HuMoments(moments)
    return hu_moments.flatten()

# Data Augmentation
def augment_image(image):
    """Effectue des transformations pour augmenter les données."""
    datagen = ImageDataGenerator(rotation_range=20, horizontal_flip=True, zoom_range=0.2)
    image = np.expand_dims(image, axis=0)  # Ajouter une dimension pour le générateur
    augmented_images = [image]
    
    for batch in datagen.flow(image, batch_size=1):
        augmented_images.append(batch[0])
        if len(augmented_images) >= 3:
            break

    return [img.squeeze() for img in augmented_images]

# Fonction pour créer le dataset
def create_dataset(image_dir, annotation_dir):
    features_cnn = []
    features_manual = []
    labels = []
    
    image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')]
    
    for image_path in image_paths:
        txt_file = os.path.join(annotation_dir, os.path.basename(image_path).replace('.jpg', '.txt').replace('.png', '.txt'))
        
        if os.path.exists(txt_file):
            img = cv2.imread(image_path)
            img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convertir en niveaux de gris

            augmented_images = augment_image(img_gray)  # Augmentation

            for aug_img in augmented_images:
                # Extraction des caractéristiques manuelles
                hog_features = extract_hog_features(aug_img)
                orb_features = extract_orb_features(aug_img)
                glcm_features = extract_glcm_features(aug_img)
                hu_moments = extract_hu_moments(aug_img)

                # Fusion des caractéristiques manuelles
                feature_vector = np.hstack((hog_features, orb_features, glcm_features, hu_moments))
                features_manual.append(feature_vector)

                # Ajout des images pour DenseNet
                resized_img = cv2.resize(img, (128, 128))  # Adapter pour DenseNet
                features_cnn.append(resized_img)

                # Nombre d'objets
                num_objects = count_objects_in_dota_annotation(txt_file)
                labels.append(num_objects)

    return np.array(features_cnn), np.array(features_manual), np.array(labels)

# Fonction pour créer le modèle combiné
def create_combined_model(input_shape_cnn, input_shape_manual):
    """Créer un modèle combinant DenseNet avec les caractéristiques manuelles."""
    # Base CNN (DenseNet)
    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape_cnn)
    x = GlobalAveragePooling2D()(base_model.output)

    # Entrée pour les caractéristiques manuelles
    input_manual = Input(shape=(input_shape_manual,))
    
    # Fusion des caractéristiques CNN + Manuelles
    merged = Concatenate()([x, input_manual])
    merged = Dense(64, activation='relu')(merged)
    output = Dense(1, activation='linear')(merged)  # Prédiction du nombre d'objets

    model = Model(inputs=[base_model.input, input_manual], outputs=output)

    # Geler DenseNet
    for layer in base_model.layers:
        layer.trainable = False

    model.compile(optimizer='adam', loss='mean_squared_error')
    return model

def main():
    image_dir = 'path_to_images'
    annotation_dir = 'path_to_annotations'
    
    # Création du dataset
    features_cnn, features_manual, labels = create_dataset(image_dir, annotation_dir)

    # Normalisation des caractéristiques manuelles
    scaler = StandardScaler()
    features_manual = scaler.fit_transform(features_manual)

    # Séparer en train/test
    X_train_cnn, X_test_cnn, X_train_manual, X_test_manual, y_train, y_test = train_test_split(
        features_cnn, features_manual, labels, test_size=0.2, random_state=42
    )

    # Adapter pour DenseNet (3 canaux)
    X_train_cnn = X_train_cnn / 255.0
    X_test_cnn = X_test_cnn / 255.0

    # Créer le modèle
    model = create_combined_model((128, 128, 3), X_train_manual.shape[1])
    
    # Entraînement
    model.fit([X_train_cnn, X_train_manual], y_train, epochs=10, batch_size=16, validation_split=0.1)
    
    # Prédiction
    predictions = model.predict([X_test_cnn, X_test_manual])

    print(f"MAE : {mean_absolute_error(y_test, predictions):.2f}")
    print(f"R² Score : {r2_score(y_test, predictions):.2f}")

    # Sauvegarde
    model.save('combined_densenet_model.h5')

if __name__ == '__main__':
    main()
def main():
    image_dir = 'path_to_images'
    annotation_dir = 'path_to_annotations'
    
    # Chargement et prétraitement des données
    X_cnn, y = create_dataset(image_dir, annotation_dir)
    
    # Vérification des dimensions
    print(f"Shape des images: {X_cnn.shape}")  # Doit être (N, 128, 128, 3)

    # Séparation en train/test
    X_train, X_test, y_train, y_test = train_test_split(X_cnn, y, test_size=0.2, random_state=42)

    # Création et entraînement du modèle CNN
    model = create_densenet_model((128, 128, 3))
    model.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.1)

    # Prédictions
    predictions = model.predict(X_test)
    
    print(f"MAE : {mean_absolute_error(y_test, predictions):.2f}")
    print(f"R² Score : {r2_score(y_test, predictions):.2f}")

    model.save('densenet_model.h5')

if __name__ == '__main__':
    main()