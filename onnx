#!/usr/bin/env python
# Copyright (c) Facebook, Inc. and its affiliates.
import argparse
import os
from typing import Dict, List, Tuple

import detectron2.data.transforms as T
import torch
from detectron2.checkpoint import DetectionCheckpointer
from detectron2.config import get_cfg
from detectron2.data import build_detection_test_loader, detection_utils
from detectron2.evaluation import COCOEvaluator, inference_on_dataset, print_csv_format
from detectron2.export import (
    dump_torchscript_IR,
    scripting_with_instances,
    STABLE_ONNX_OPSET_VERSION,
    TracingAdapter,
)
from detectron2.modeling import build_model, GeneralizedRCNN, RetinaNet
from detectron2.modeling.postprocessing import detector_postprocess
from detectron2.projects.point_rend import add_pointrend_config
from detectron2.structures import Boxes
from detectron2.utils.env import TORCH_VERSION
from detectron2.utils.file_io import PathManager
from detectron2.utils.logger import setup_logger
from torch import nn, Tensor

import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()
import numpy as np
import os, json, cv2, random
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer
from detectron2.data import MetadataCatalog, DatasetCatalog
from matplotlib import pyplot as plt
import torch
from detectron2.utils.logger import setup_logger
from detectron2.data import build_detection_test_loader
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.data.datasets import register_coco_instances
from PIL import Image
import distutils.core
from detectron2.engine import DefaultTrainer

def setup_cfg(args):
    cfg = get_cfg()
    # cuda context is initialized before creating dataloader, so we don't fork anymore
    cfg.DATALOADER.NUM_WORKERS = 0
    add_pointrend_config(cfg)
    cfg.merge_from_file(args.config_file)
    cfg.merge_from_list(args.opts)
    cfg.freeze()
    return cfg


def export_caffe2_tracing(cfg, torch_model, inputs):
    from detectron2.export import Caffe2Tracer

    tracer = Caffe2Tracer(cfg, torch_model, inputs)
    if args.format == "caffe2":
        caffe2_model = tracer.export_caffe2()
        caffe2_model.save_protobuf(args.output)
        # draw the caffe2 graph
        caffe2_model.save_graph(os.path.join(args.output, "model.svg"), inputs=inputs)
        return caffe2_model
    elif args.format == "onnx":
        import onnx

        onnx_model = tracer.export_onnx()
        onnx.save(onnx_model, os.path.join(args.output, "model.onnx"))
    elif args.format == "torchscript":
        ts_model = tracer.export_torchscript()
        with PathManager.open(os.path.join(args.output, "model.ts"), "wb") as f:
            torch.jit.save(ts_model, f)
        dump_torchscript_IR(ts_model, args.output)


# experimental. API not yet final
def export_scripting(torch_model):
    assert TORCH_VERSION >= (1, 8)
    fields = {
        "proposal_boxes": Boxes,
        "objectness_logits": Tensor,
        "pred_boxes": Boxes,
        "scores": Tensor,
        "pred_classes": Tensor,
        "pred_masks": Tensor,
        "pred_keypoints": torch.Tensor,
        "pred_keypoint_heatmaps": torch.Tensor,
    }
    assert args.format == "torchscript", "Scripting only supports torchscript format."

    class ScriptableAdapterBase(nn.Module):
        # Use this adapter to workaround https://github.com/pytorch/pytorch/issues/46944
        # by not retuning instances but dicts. Otherwise the exported model is not deployable
        def __init__(self):
            super().__init__()
            self.model = torch_model
            self.eval()

    if isinstance(torch_model, GeneralizedRCNN):

        class ScriptableAdapter(ScriptableAdapterBase):
            def forward(
                self, inputs: Tuple[Dict[str, torch.Tensor]]
            ) -> List[Dict[str, Tensor]]:
                instances = self.model.inference(inputs, do_postprocess=False)
                return [i.get_fields() for i in instances]

    else:

        class ScriptableAdapter(ScriptableAdapterBase):
            def forward(
                self, inputs: Tuple[Dict[str, torch.Tensor]]
            ) -> List[Dict[str, Tensor]]:
                instances = self.model(inputs)
                return [i.get_fields() for i in instances]

    ts_model = scripting_with_instances(ScriptableAdapter(), fields)
    with PathManager.open(os.path.join(args.output, "model.ts"), "wb") as f:
        torch.jit.save(ts_model, f)
    dump_torchscript_IR(ts_model, args.output)
    # TODO inference in Python now missing postprocessing glue code
    return None


# experimental. API not yet final
def export_tracing(torch_model, inputs):
    assert TORCH_VERSION >= (1, 8)
    image = inputs[0]["image"]
    inputs = [{"image": image}]  # remove other unused keys

    if isinstance(torch_model, GeneralizedRCNN):

        def inference(model, inputs):
            # use do_postprocess=False so it returns ROI mask
            inst = model.inference(inputs, do_postprocess=False)[0]
            return [{"instances": inst}]

    else:
        inference = None  # assume that we just call the model directly

    traceable_model = TracingAdapter(torch_model, inputs, inference)

    if args.format == "torchscript":
        ts_model = torch.jit.trace(traceable_model, (image,))
        with PathManager.open(os.path.join(args.output, "model.ts"), "wb") as f:
            torch.jit.save(ts_model, f)
        dump_torchscript_IR(ts_model, args.output)
    elif args.format == "onnx":
        with PathManager.open(os.path.join(args.output, "model.onnx"), "wb") as f:
            torch.onnx.export(
                traceable_model, (image,), f, opset_version=16
            )
    logger.info("Inputs schema: " + str(traceable_model.inputs_schema))
    logger.info("Outputs schema: " + str(traceable_model.outputs_schema))

    if args.format != "torchscript":
        return None
    if not isinstance(torch_model, (GeneralizedRCNN, RetinaNet)):
        return None

    def eval_wrapper(inputs):
        """
        The exported model does not contain the final resize step, which is typically
        unused in deployment but needed for evaluation. We add it manually here.
        """
        input = inputs[0]
        instances = traceable_model.outputs_schema(ts_model(input["image"]))[0][
            "instances"
        ]
        postprocessed = detector_postprocess(instances, input["height"], input["width"])
        return [{"instances": postprocessed}]

    return eval_wrapper


def get_sample_inputs(args):

    if args.sample_image is None:
        # get a first batch from dataset
        data_loader = build_detection_test_loader(cfg, cfg.DATASETS.TEST[0])
        first_batch = next(iter(data_loader))
        return first_batch
    else:
        # get a sample data
        original_image = detection_utils.read_image(
            args.sample_image, format=cfg.INPUT.FORMAT
        )
        # Do same preprocessing as DefaultPredictor
        aug = T.ResizeShortestEdge(
            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST
        )
        height, width = original_image.shape[:2]
        image = aug.get_transform(original_image).apply_image(original_image)
        image = torch.as_tensor(image.astype("float32").transpose(2, 0, 1))

        inputs = {"image": image, "height": height, "width": width}

        # Sample ready
        sample_inputs = [inputs]
        return sample_inputs

from torch.autograd import Caffe2Tracer

global logger, cfg, args
parser = argparse.ArgumentParser(description="Export a model for deployment.")
parser.add_argument(
    "--format",
    choices=["caffe2", "onnx", "torchscript"],
    help="output format",
    default="onnx",
)
parser.add_argument(
    "--export-method",
    choices=["caffe2_tracing", "tracing", "scripting"],
    help="Method to export models",
    default="caffe2_tracing",
)
parser.add_argument(
    "--config-file", default="COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml", metavar="FILE", help="path to config file"
)
parser.add_argument(
    "--sample-image", default="projetm1_data/test/130423_WT_pyruvate_1D-vide_ch00_part_1_jpg.rf.21a235950e2f2628d1b9d15055fb2507.jpg", type=str, help="sample image for input"
)
parser.add_argument("--run-eval", action="store_true")
parser.add_argument("--output", default="onnx", help="output directory for the converted model")
parser.add_argument(
    "opts",
    help="Modify config options using the command-line",
    default=None,
    nargs=argparse.REMAINDER,
)
args = parser.parse_args()
logger = setup_logger()
logger.info("Command line arguments: " + str(args))
PathManager.mkdirs(args.output)
# Disable re-specialization on new shapes. Otherwise --run-eval will be slow
torch._C._jit_set_bailout_depth(1)

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("my_dataset_train",)
cfg.DATASETS.TEST = ("my_dataset_val",) 
cfg.DATALOADER.NUM_WORKERS = 0
cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS=False
cfg.MODEL.MASK_ON = True
cfg.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION = 20
cfg.MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION = 20
cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[8], [16], [32], [64], [128]]
cfg.MODEL.ROI_HEADS.POSITIVE_FRACTION = 0.5 
cfg.MODEL.WEIGHTS = "data/projet_m1/models/Detectron2_Models6/model_final.pth"
cfg.SOLVER.IMS_PER_BATCH = 4
cfg.SOLVER.BASE_LR = 0.001 
cfg.SOLVER.MAX_ITER = 5000
cfg.SOLVER.CHECKPOINT_PERIOD = 1000
cfg.SOLVER.STEPS = []
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.15
cfg.TEST.PRECISE_BN.ENABLED = True
cfg.TEST.PRECISE_BN.NUM_ITER = 1000
cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST= 0.59
cfg.TEST.DETECTIONS_PER_IMAGE = 1000
cfg.TEST.EVAL_PERIOD = 159

add_pointrend_config(cfg)
cfg.merge_from_list(args.opts)
cfg.freeze()
# create a torch model
torch_model = build_model(cfg)
DetectionCheckpointer(torch_model).resume_or_load(cfg.MODEL.WEIGHTS)
torch_model.eval()

# convert and save model
if args.export_method == "caffe2_tracing":
    sample_inputs = get_sample_inputs(args)
    exported_model = export_caffe2_tracing(cfg, torch_model, sample_inputs)
elif args.export_method == "scripting":
    exported_model = export_scripting(torch_model)
elif args.export_method == "tracing":
    sample_inputs = get_sample_inputs(args)
    exported_model = export_tracing(torch_model, sample_inputs)

# run evaluation with the converted model
if args.run_eval:
    assert exported_model is not None, (
        "Python inference is not yet implemented for "
        f"export_method={args.export_method}, format={args.format}."
    )
    logger.info(
        "Running evaluation ... this takes a long time if you export to CPU."
    )
    dataset = cfg.DATASETS.TEST[0]
    data_loader = build_detection_test_loader(cfg, dataset)
    # NOTE: hard-coded evaluator. change to the evaluator for your dataset
    evaluator = COCOEvaluator(dataset, output_dir=args.output)
    metrics = inference_on_dataset(exported_model, data_loader, evaluator)
    print_csv_format(metrics)
logger.info("Success.")

from detectron2.checkpoint import DetectionCheckpointer
from detectron2.modeling import build_model


from torchvision.transforms import transforms

import onnx
model = onnx.load("onnx/model.onnx")  # or use the `onnx_model` from above
onnx.checker.check_model(model)

# To print a human readable representation of the graph use:
print(onnx.helper.printable_graph(model.graph))


import cv2
import numpy as np
import onnxruntime as ort
from detectron2.utils.visualizer import Visualizer, ColorMode
from detectron2.data import MetadataCatalog
from detectron2.structures import Boxes, Instances
import torch

# Charger l'image pour l'inférence
image_path = 'projetm1_data/test/Tbaro-WT-pyruvte-point-final_5C_ch00_part_7_jpg.rf.e8965bf5c4d972f9f102c03d4722fed8.jpg'
image = cv2.imread(image_path)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Prétraitement de l'image (redimensionner et normaliser si nécessaire)
input_image = cv2.resize(image_rgb, (667, 1333))  # Redimensionner à la taille d'entrée attendue
input_image = input_image.astype(np.float32)
input_image = np.transpose(input_image, (2, 0, 1))  # Changer de HWC à CHW

# Ajouter une dimension pour le batch size (forme: [1, C, H, W])
input_image = np.expand_dims(input_image, axis=0)

# Retirer la dimension supplémentaire pour correspondre à l'entrée attendue par le modèle (forme: [C, H, W])
input_image = np.squeeze(input_image, axis=0)  # Retirer la dimension du batch

# Charger le modèle ONNX
onnx_model_path = 'onnx/model.onnx'
session = ort.InferenceSession(onnx_model_path)

# Préparer les entrées pour le modèle
input_name = session.get_inputs()[0].name  # Nom de l'entrée du modèle
inputs = {input_name: input_image}

# Exécuter l'inférence
outputs = session.run(None, inputs)

# Afficher les sorties pour inspection
print("Outputs:", outputs)
for i, output in enumerate(outputs):
    print(f"Output {i} shape: {output.shape}")

# Extraire les informations pertinentes
boxes = outputs[0]  # Boîtes de détection
classes = outputs[1]  # Classes de détection (toutes 0 dans ton cas)
masks = outputs[2]   # Masques de segmentation

# Formater les prédictions pour Detectron2 Visualizer
# Convertir les boîtes en objets "Boxes" de Detectron2
boxes = Boxes(torch.tensor(boxes))  # Convertir en tensor PyTorch et encapsuler dans un objet "Boxes"

# Créer un objet "Instances" de Detectron2
instances = Instances(image_rgb.shape[:2])  # Créer un objet "Instances" vide avec la taille de l'image
instances.pred_boxes = boxes  # Ajouter les boîtes détectées
instances.pred_classes = torch.tensor(classes, dtype=torch.int64)  # Ajouter les classes détectées

# Charger les métadonnées
metadata = MetadataCatalog.get("my_dataset_train")  # Remplacez par votre dataset

# Visualisation avec Detectron2 Visualizer
v = Visualizer(image_rgb[:, :, ::-1],  # Convertir de RGB à BGR pour OpenCV
               metadata=metadata,
               scale=1.0,
               instance_mode=ColorMode.IMAGE
)

# Dessiner les prédictions d'instances sur l'image
v = v.draw_instance_predictions(instances)

img = v.get_image()[:,:,[2,1,0]]
img = Image.fromarray(img)
plt.figure(figsize=(10, 10))
plt.imshow(img)