import tensorflow as tf
from tensorflow.keras import Model, initializers
from tensorflow.keras.regularizers import L2
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, LSTM, Bidirectional
from tensorflow.keras.layers import Dropout, Dense, LeakyReLU, Input

class BasicConv1D(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_size, strides=1, **kwargs):
        super(BasicConv1D, self).__init__()
        self.conv = Conv1D(filters, kernel_size=kernel_size, strides=strides, **kwargs)
        self.activation = LeakyReLU()

    def call(self, x):
        x = self.conv(x)
        x = self.activation(x)
        return x

class Module_35x35(tf.keras.layers.Layer):
    def __init__(self, in_channels: int, regularization_factor: float, seed_value: int):
        super(Module_35x35, self).__init__()
        self.branch1 = tf.keras.Sequential([
            MaxPooling1D(pool_size=2),
            BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value))
        ])
        self.branch2 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch3 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch4 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])

    def call(self, x):
        branch1 = self.branch1(x)
        branch2 = self.branch2(x)
        branch3 = self.branch3(x)
        branch4 = self.branch4(x)
        out = tf.concat([branch1, branch2, branch3, branch4], axis=1)
        return out

class AttentionLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1]), initializer="random_normal", trainable=True)
        self.b = self.add_weight(shape=(input_shape[-1],), initializer="zeros", trainable=True)
        self.u = self.add_weight(shape=(input_shape[-1],), initializer="random_normal", trainable=True)

    def call(self, inputs):
        u_t = tf.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)
        attention_weights = tf.nn.softmax(tf.tensordot(u_t, self.u, axes=1), axis=1)
        output = inputs * tf.expand_dims(attention_weights, -1)
        return tf.reduce_sum(output, axis=1)

class IPA_LSTM_Variable(tf.keras.Model):
    def __init__(self, seed_value, regularization_factor, dropout_rate=0.2):
        super(IPA_LSTM_Variable, self).__init__()

        # Convolution initiale
        self.stem = tf.keras.Sequential([
            BasicConv1D(filters=16, kernel_size=3, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=16, kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=16, kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])

        # Module convolutionnel
        self.module_35x35 = Module_35x35(in_channels=32, regularization_factor=regularization_factor, seed_value=seed_value)

        # Encodeur LSTM bidirectionnel
        self.lstm1 = Bidirectional(LSTM(128, return_sequences=True, dropout=dropout_rate))
        self.lstm2 = Bidirectional(LSTM(64, return_sequences=True, dropout=dropout_rate))

        # Couche d'attention
        self.attention = AttentionLayer()

        # Branches de sortie
        self.dense_height = Dense(64, activation="relu")
        self.output_height = Dense(1, activation="linear", name="height_output")  # Hauteur du pic

        self.dense_bounds = Dense(64, activation="relu")
        self.output_bounds = Dense(2, activation="linear", name="bounds_output")  # Bornes d'intégration

        self.dense_component = Dense(64, activation="relu")
        self.output_component = Dense(10, activation="softmax", name="component_output")  # Classification

    def call(self, x):
        # Passage dans les modules convolutionnels
        x = self.stem(x)
        x = self.module_35x35(x)

        # Passage dans le LSTM
        x = self.lstm1(x)
        x = self.lstm2(x)

        # Attention
        x = self.attention(x)

        # Hauteur du pic
        height = self.dense_height(x)
        height = self.output_height(height)

        # Bornes d'intégration
        bounds = self.dense_bounds(x)
        bounds = self.output_bounds(bounds)

        # Composant (classification)
        component = self.dense_component(x)
        component = self.output_component(component)

        return {"height_output": height, "bounds_output": bounds, "component_output": component}

# Test du modèle
if __name__ == '__main__':
    ipa_lstm_variable = IPA_LSTM_Variable(seed_value=1, regularization_factor=0.0095)
    ipa_lstm_variable.build((None, 3890, 1))  # Exemple avec une séquence de longueur 3890
    ipa_lstm_variable.summary()
