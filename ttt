import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# üîπ Charger les donn√©es JSON en DataFrame
file_path = "chromatogrammes.json"  # Remplace par ton fichier r√©el
df = pd.read_json(file_path)

# üîπ Suppression des chromatogrammes sans pics
df = df.dropna(subset=['pics'])

# üîπ Cr√©ation de X avec 2 colonnes (temps de r√©tention et intensit√©)
X = np.array([np.column_stack((row["x"], row["y"])) for _, row in df.iterrows()])  # (319, 71840, 2)

# üîπ Construction de Y en s√©quences de cha√Ænes de caract√®res
Y = []
for _, row in df.iterrows():
    sequence = []
    for pic in row["pics"]:
        sequence.append(f"{pic['valeur']}/{pic['borne_avant']}/{pic['borne_apres']}/{pic['nom']}")
    Y.append(" ".join(sequence))  # S√©parer chaque pic par un espace

# üîπ S√©parer en train/test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# üîπ Affichage des formes
print(f"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}")
print(f"Exemple Y_train: {Y_train[0]}")  # V√©rifier le format
import tensorflow as tf
from tensorflow.keras import Model, initializers, optimizers
from tensorflow.keras.regularizers import L2
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, Dense, Input, Embedding, LSTM, Bidirectional

# üîπ D√©finition de la couche de convolution
class BasicConv1D(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_size, strides=1, **kwargs):
        super(BasicConv1D, self).__init__()
        self.conv = Conv1D(filters, kernel_size=kernel_size, strides=strides, **kwargs)
        self.activation = tf.keras.layers.LeakyReLU()

    def call(self, x):
        x = self.conv(x)
        x = self.activation(x)
        return x

# üîπ Module 35x35 du mod√®le IPA
class Module_35x35(tf.keras.layers.Layer):
    def __init__(self, in_channels: int, regularization_factor: float, seed_value: int):
        super(Module_35x35, self).__init__()
        self.branch1 = tf.keras.Sequential([
            MaxPooling1D(pool_size=2),
            BasicConv1D(filters=in_channels * 2,
                        kernel_size=1,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value))
        ])
        self.branch2 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch3 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch4 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])

    def call(self, x):
        branch1 = self.branch1(x)
        branch2 = self.branch2(x)
        branch3 = self.branch3(x)
        branch4 = self.branch4(x)
        out = tf.concat([branch1, branch2, branch3, branch4], axis=1)
        return out

# üîπ D√©finition du mod√®le IPA avec BiLSTM
class IPA_BiLSTM(tf.keras.Model):
    def __init__(self, seed_value, regularization_factor, dropout_rate=0.2):
        super(IPA_BiLSTM, self).__init__()

        # Bloc IPA (Feature Extractor)
        self.stem = tf.keras.Sequential([
            BasicConv1D(filters=16, kernel_size=3, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=16, kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=16, kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.module_35x35 = Module_35x35(in_channels=32,
                                         regularization_factor=regularization_factor,
                                         seed_value=seed_value)
        
        self.flatten = Flatten()
        self.dropout = Dropout(rate=dropout_rate)

        # BiLSTM pour g√©rer la s√©quence de sortie variable
        self.bilstm = Bidirectional(LSTM(128, return_sequences=True))
        self.output_layer = Dense(256, activation="relu")  # Encoder cach√©

        # D√©codage en sortie s√©quentielle (caract√®res/jetons)
        self.decoder_lstm = LSTM(128, return_sequences=True)
        self.output_dense = Dense(1)  # Sortie finale en s√©quence

    def call(self, x):
        out = self.stem(x)
        out = self.module_35x35(out)
        out = self.flatten(out)
        out = self.dropout(out)

        # Passer par BiLSTM pour capturer les relations temporelles
        out = tf.expand_dims(out, axis=1)  # Reshape pour BiLSTM
        out = self.bilstm(out)
        out = self.output_layer(out)

        # D√©codage s√©quentiel
        out = self.decoder_lstm(out)
        out = self.output_dense(out)

        return out


# üîπ Param√®tres du mod√®le
SEED_VALUE = 1
REGULARIZATION_COEF = 0.0095
DROPOUT_RATE = 0.2

# üîπ Instanciation et compilation du mod√®le
model = IPA_BiLSTM(seed_value=SEED_VALUE, regularization_factor=REGULARIZATION_COEF)

# üîπ D√©finition de l'optimiseur et de la fonction de perte
optimizer = optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='mae', metrics=['mse'])

# üîπ R√©sum√© du mod√®le
model.build((None, X_train.shape[1], X_train.shape[2]))
model.summary()

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Charger les donn√©es JSON en DataFrame
file_path = "chromatogrammes.json"  # Remplace par ton fichier r√©el
df = pd.read_json(file_path)

# Supprimer les chromatogrammes sans pics
df = df.dropna(subset=['pics'])

# Cr√©ation de X avec 2 colonnes (temps de r√©tention et intensit√©)
X = np.array([np.column_stack((row["x"], row["y"])) for _, row in df.iterrows()])  # (319, 71840, 2)

# Cr√©ation de Y en s√©quences de cha√Ænes de caract√®res
Y = []
for _, row in df.iterrows():
    sequence = []
    for pic in row["pics"]:
        sequence.append(f"{pic['valeur']}/{pic['borne_avant']}/{pic['borne_apres']}/{pic['nom']}")
    Y.append(" ".join(sequence))  # S√©parer chaque pic par un espace

# Tokenization de Y (convertir en indices)
tokenizer = Tokenizer(char_level=False)
tokenizer.fit_on_texts(Y)  # Cr√©er un vocabulaire bas√© sur Y
Y_sequences = tokenizer.texts_to_sequences(Y)  # Convertir Y en s√©quences d'indices

# S√©parer en train/test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y_sequences, test_size=0.2, random_state=42)

# Affichage des formes
print(f"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}")
print(f"Exemple Y_train: {Y_train[0]}")  # V√©rifier le format apr√®s conversion
import tensorflow as tf
from tensorflow.keras import Model, initializers, optimizers
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, Dense, Input, LSTM, Bidirectional, TimeDistributed

class IPA_BiLSTM(tf.keras.Model):
    def __init__(self, seed_value, regularization_factor, num_classes, dropout_rate=0.2):
        super(IPA_BiLSTM, self).__init__()

        # Bloc IPA (Feature Extractor)
        self.stem = tf.keras.Sequential([
            Conv1D(filters=16, kernel_size=3, strides=2, kernel_regularizer=tf.keras.regularizers.L2(regularization_factor),
                   kernel_initializer=initializers.HeNormal(seed_value)),
            Conv1D(filters=16, kernel_size=3, kernel_regularizer=tf.keras.regularizers.L2(regularization_factor),
                   kernel_initializer=initializers.HeNormal(seed_value)),
            Conv1D(filters=16, kernel_size=3, kernel_regularizer=tf.keras.regularizers.L2(regularization_factor),
                   kernel_initializer=initializers.HeNormal(seed_value)),
        ])

        # Module pour extraire les caract√©ristiques
        self.flatten = Flatten()
        self.dropout = Dropout(rate=dropout_rate)

        # BiLSTM pour g√©rer la s√©quence de sortie variable
        self.bilstm = Bidirectional(LSTM(128, return_sequences=True))
        self.output_layer = Dense(256, activation="relu")

        # D√©codage de la sortie avec LSTM pour g√©n√©rer des s√©quences de texte
        self.decoder_lstm = LSTM(128, return_sequences=True)
        self.output_dense = Dense(num_classes, activation="softmax")  # Softmax pour pr√©dire des indices de mots/valeurs

    def call(self, x):
        # Passer par les couches IPA
        out = self.stem(x)
        out = self.flatten(out)
        out = self.dropout(out)

        # Passer par BiLSTM pour capturer la s√©quence
        out = tf.expand_dims(out, axis=1)  # Reshape pour BiLSTM
        out = self.bilstm(out)
        out = self.output_layer(out)

        # D√©codage s√©quentiel pour pr√©dire des s√©quences de texte
        out = self.decoder_lstm(out)
        out = self.output_dense(out)

        return out


# üîπ Param√®tres du mod√®le
SEED_VALUE = 1
REGULARIZATION_COEF = 0.0095
DROPOUT_RATE = 0.2
NUM_CLASSES = len(tokenizer.word_index) + 1  # Nombre de classes pour la tokenisation

# Instanciation et compilation du mod√®le
model = IPA_BiLSTM(seed_value=SEED_VALUE, regularization_factor=REGULARIZATION_COEF, num_classes=NUM_CLASSES)

# Optimiseur et perte
optimizer = optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# R√©sum√© du mod√®le
model.build((None, X_train.shape[1], X_train.shape[2]))  # Adapter la forme de l'entr√©e
model.summary()
# Entra√Ænement du mod√®le
history = model.fit(X_train, Y_train, epochs=10, batch_size=32, validation_data=(X_test, Y_test))
# Faire une pr√©diction
predictions = model.predict(X_test)

# D√©coder les indices pr√©dits en texte
predicted_sequences = tokenizer.sequences_to_texts(predictions.argmax(axis=-1))

# Affichage des pr√©dictions
for i in range(3):
    print(f"True: {Y_test[i]}")
    print(f"Predicted: {predicted_sequences[i]}")
