import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# ðŸ”¹ Charger les donnÃ©es JSON en DataFrame
file_path = "chromatogrammes.json"  # Remplace par ton fichier rÃ©el
df = pd.read_json(file_path)

# ðŸ”¹ Suppression des chromatogrammes sans pics
df = df.dropna(subset=['pics'])

# ðŸ”¹ Suppression des chromatogrammes qui ne font pas exactement 71 840 points
df = df[df["x"].apply(len) == 71840]

# ðŸ”¹ CrÃ©ation de X avec 2 colonnes (temps de rÃ©tention et intensitÃ©) sous forme 2D
X = np.array([np.column_stack((row["x"], row["y"])) for _, row in df.iterrows()])  # (n, 71840, 2)

# ðŸ”¹ PrÃ©parer Y (valeur du pic, bornes, nom du composant)
y_list = []
component_names = []  # Stocker les noms des composÃ©s pour conversion en classes

for _, row in df.iterrows():
    for pic in row["pics"]:
        valeur_pic = pic["valeur"]
        borne_avant = pic["borne_avant"]
        borne_apres = pic["borne_apres"]
        nom_composant = pic["nom"]
        
        y_list.append([valeur_pic, borne_avant, borne_apres, nom_composant])
        component_names.append(nom_composant)

# ðŸ”¹ Convertir les noms des composants en classes entiÃ¨res
label_encoder = LabelEncoder()
component_classes = label_encoder.fit_transform(component_names)

# ðŸ”¹ Remplacer les noms dans Y par les classes correspondantes
for i in range(len(y_list)):
    y_list[i][3] = component_classes[i]  # Remplace le nom par l'entier correspondant

# ðŸ”¹ Transformer en array
Y = np.array(y_list)

# ðŸ”¹ SÃ©parer en train/test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ðŸ”¹ Affichage des tailles
print(f"Taille X_train : {X_train.shape}, Y_train : {Y_train.shape}")
print(f"Taille X_test : {X_test.shape}, Y_test : {Y_test.shape}")
print(f"Classes des composants : {label_encoder.classes_}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# ðŸ”¹ Charger les donnÃ©es JSON en DataFrame
file_path = "chromatogrammes.json"  # Remplace par ton fichier rÃ©el
df = pd.read_json(file_path)

# ðŸ”¹ Suppression des chromatogrammes sans pics
df = df.dropna(subset=['pics'])

# ðŸ”¹ Suppression des chromatogrammes qui ne font pas exactement 71 840 points
df = df[df["x"].apply(len) == 71840]

# ðŸ”¹ CrÃ©ation de X avec 2 colonnes (temps de rÃ©tention et intensitÃ©) sous forme 2D
X = np.array([np.column_stack((row["x"], row["y"])) for _, row in df.iterrows()])  # (n, 71840, 2)

# ðŸ”¹ PrÃ©parer Y (valeur du pic, bornes, nom du composant) avec indices
y_list = []
component_names = []  # Stocker les noms des composÃ©s pour conversion en classes

for _, row in df.iterrows():
    for pic in row["pics"]:
        if len(pic) >= 4:  # VÃ©rifie que le pic a bien 4 Ã©lÃ©ments
            valeur_pic = pic[0]  # Indice 0 : Valeur du pic
            borne_avant = pic[1]  # Indice 1 : Borne avant
            borne_apres = pic[2]  # Indice 2 : Borne aprÃ¨s
            nom_composant = pic[3]  # Indice 3 : Nom du composant

            y_list.append([valeur_pic, borne_avant, borne_apres, nom_composant])
            component_names.append(nom_composant)

# ðŸ”¹ Convertir les noms des composants en classes entiÃ¨res
label_encoder = LabelEncoder()
component_classes = label_encoder.fit_transform(component_names)

# ðŸ”¹ Remplacer les noms dans Y par les classes correspondantes
for i in range(len(y_list)):
    y_list[i][3] = component_classes[i]  # Remplace le nom par l'entier correspondant

# ðŸ”¹ Transformer en array
Y = np.array(y_list, dtype=np.float32)

# ðŸ”¹ SÃ©parer en train/test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ðŸ”¹ Affichage des tailles
print(f"Taille X_train : {X_train.shape}, Y_train : {Y_train.shape}")
print(f"Taille X_test : {X_test.shape}, Y_test : {Y_test.shape}")
print(f"Classes des composants : {label_encoder.classes_}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# ðŸ”¹ Charger les donnÃ©es JSON en DataFrame
file_path = "chromatogrammes.json"  # Remplace par ton fichier rÃ©el
df = pd.read_json(file_path)

# ðŸ”¹ Suppression des chromatogrammes sans pics
df = df.dropna(subset=['pics'])

# ðŸ”¹ Suppression des chromatogrammes qui ne font pas exactement 71 840 points
df = df[df["x"].apply(len) == 71840]

# ðŸ”¹ Transformation de X en 2D : ConcatÃ©ner (temps de rÃ©tention + intensitÃ©)
X = np.array([np.hstack((row["x"], row["y"])) for _, row in df.iterrows()])  # (n, 2 * 71840)

# ðŸ”¹ PrÃ©parer Y (valeur du pic, bornes, nom du composant)
y_list = []
component_names = []  # Stocker les noms des composÃ©s pour conversion en classes

for _, row in df.iterrows():
    for pic in row["pics"]:
        valeur_pic = pic["valeur"]
        borne_avant = pic["borne_avant"]
        borne_apres = pic["borne_apres"]
        nom_composant = pic["nom"]
        
        y_list.append([valeur_pic, borne_avant, borne_apres, nom_composant])
        component_names.append(nom_composant)

# ðŸ”¹ Convertir les noms des composants en classes entiÃ¨res
label_encoder = LabelEncoder()
component_classes = label_encoder.fit_transform(component_names)

# ðŸ”¹ Remplacer les noms dans Y par les classes correspondantes
for i in range(len(y_list)):
    y_list[i][3] = component_classes[i]  # Remplace le nom par l'entier correspondant

# ðŸ”¹ Transformer en array
Y = np.array(y_list)

# ðŸ”¹ SÃ©parer en train/test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ðŸ”¹ Affichage des tailles
print(f"Taille X_train : {X_train.shape}, Y_train : {Y_train.shape}")
print(f"Taille X_test : {X_test.shape}, Y_test : {Y_test.shape}")
print(f"Classes des composants : {label_encoder.classes_}")

import os
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import optimizers
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

from tqdm.keras import TqdmCallback
from python.IPA_architecture import IPA
from python.regression_utils import cnn_prediction
from python.plot_utils import plot_history

# ðŸ”¹ DÃ©finition des constantes
SEED_VALUE = 1
np.random.seed(SEED_VALUE)
tf.random.set_seed(SEED_VALUE)

# ðŸ”¹ Charger les donnÃ©es JSON en DataFrame
file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

# ðŸ”¹ Suppression des chromatogrammes invalides
df = df.dropna(subset=['pics'])
df = df[df["x"].apply(len) == 71840]

# ðŸ”¹ CrÃ©ation de X avec 2 colonnes (temps de rÃ©tention et intensitÃ©)
X = np.array([np.column_stack((row["x"], row["y"])) for _, row in df.iterrows()])  # (n, 71840, 2)

# ðŸ”¹ CrÃ©ation de Y (valeur du pic, bornes d'intÃ©gration, classe)
y_list = []
for _, row in df.iterrows():
    for pic in row["pics"]:
        if len(pic) >= 4:  # VÃ©rifier la structure
            y_list.append(pic[:4])  # [valeur du pic, borne inf, borne sup, classe]

Y = np.array(y_list, dtype=np.float32)  # (n, 4)

# ðŸ”¹ SÃ©paration en train/test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=SEED_VALUE)

# ðŸ”¹ Normalisation des donnÃ©es d'entrÃ©e X (chaque colonne sÃ©parÃ©ment)
scalerX = StandardScaler()
X_train_scaled = scalerX.fit_transform(X_train.reshape(-1, 2)).reshape(X_train.shape)
X_test_scaled = scalerX.transform(X_test.reshape(-1, 2)).reshape(X_test.shape)

# ðŸ”¹ Normalisation de Y (uniquement les valeurs continues : 0, 1, 2)
scalerY = StandardScaler()
Y_train[:, :3] = scalerY.fit_transform(Y_train[:, :3])
Y_test[:, :3] = scalerY.transform(Y_test[:, :3])

# ðŸ”¹ Transformation de la classe en one-hot encoding
num_classes = len(np.unique(Y[:, 3]))  # Nombre de classes
Y_train_classes = tf.keras.utils.to_categorical(Y_train[:, 3], num_classes)
Y_test_classes = tf.keras.utils.to_categorical(Y_test[:, 3], num_classes)

# ðŸ”¹ Regroupement des sorties
Y_train = np.hstack((Y_train[:, :3], Y_train_classes))  # (n, 3 + num_classes)
Y_test = np.hstack((Y_test[:, :3], Y_test_classes))  # (n, 3 + num_classes)

# ðŸ”¹ Reshape pour le modÃ¨le
X_train_reshaped = X_train_scaled  # (n, 71840, 2)
X_test_reshaped = X_test_scaled

""" ðŸŽ¯ EntraÃ®nement du modÃ¨le IPA avec 4 sorties """
MODEL_NAME = 'IPA_multi_output'
path_model = 'model/'

LEARNING_RATE = .001
BATCH_SIZE = 16
EPOCHS = 500
REGULARIZATION_COEF = .0095
DROPOUT_RATE = .2

if not os.path.exists(path_model + MODEL_NAME):
    print('ðŸš€ ModÃ¨le non trouvÃ©, entraÃ®nement en cours...')

    model = IPA(seed_value=SEED_VALUE,
                regularization_factor=REGULARIZATION_COEF,
                num_outputs=3 + num_classes)  # ðŸŽ¯ Ajout de la nouvelle sortie

    # ðŸ”¹ Configuration de l'optimiseur et du modÃ¨le
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=LEARNING_RATE,
        decay_steps=10000, decay_rate=.001)
    optimizer = optimizers.Adam(learning_rate=lr_schedule)
    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])

    stop_early = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50)

    # ðŸ”¹ EntraÃ®nement
    history = model.fit(
        X_train_reshaped, Y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_test_reshaped, Y_test),
        callbacks=[stop_early, TqdmCallback(verbose=0)],
        verbose=0
    )

    # ðŸ”¹ Ã‰valuation et enregistrement du modÃ¨le
    plot_history(history)
    print('ðŸ’¾ Sauvegarde du modÃ¨le')
    model.save(path_model + MODEL_NAME, overwrite=True, save_format='tf')

else:
    print('âœ… ModÃ¨le trouvÃ©, chargement...')
    model = tf.keras.models.load_model(path_model + MODEL_NAME)

# ðŸ”¹ Affichage du rÃ©sumÃ© du modÃ¨le
model.summary()
class IPA(tf.keras.Model):
    def __init__(self,
                 seed_value,
                 regularization_factor,
                 dropout_rate=0.2,
                 num_outputs=4):  # ðŸ†• Ajout du nombre de sorties
        super(IPA, self).__init__()
        self.stem = tf.keras.Sequential([
            BasicConv1D(filters=16,
                        kernel_size=3,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=16,
                        kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=16,
                        kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.module_35x35 = Module_35x35(in_channels=32,
                                         regularization_factor=regularization_factor,
                                         seed_value=seed_value)
        self.flatten = Flatten()
        self.dropout = Dropout(rate=dropout_rate)
        self.regressor = Dense(num_outputs)  # ðŸŽ¯ Sortie avec 3 + num_classes dimensions

    def call(self, x):
        out = self.stem(x)
        out = self.module_35x35(out)
        out = self.flatten(out)
        out = self.dropout(out)
        out = self.regressor(out)  # ðŸŽ¯ Sortie multi-dimensionnelle
        return out


import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import Model, initializers
from tensorflow.keras.regularizers import L2
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten
from tensorflow.keras.layers import Dropout, Dense, LeakyReLU, Input

# ðŸ”¹ Charger les donnÃ©es JSON en DataFrame
file_path = "chromatogrammes.json"  # Remplace par ton fichier rÃ©el
df = pd.read_json(file_path)

# ðŸ”¹ Suppression des chromatogrammes sans pics
df = df.dropna(subset=['pics'])

# ðŸ”¹ Suppression des chromatogrammes qui ne font pas exactement 71 840 points
df = df[df["x"].apply(len) == 71840]

# ðŸ”¹ CrÃ©ation de X avec 2 colonnes (temps de rÃ©tention et intensitÃ©) sous forme 2D
X = np.array([np.column_stack((row["x"], row["y"])) for _, row in df.iterrows()])  # (n, 71840, 2)

# ðŸ”¹ PrÃ©parer Y (valeur du pic, bornes, nom du composant)
y_list = []
component_names = []  # Stocker les noms des composÃ©s pour conversion en classes

for _, row in df.iterrows():
    for pic in row["pics"]:
        valeur_pic = pic["valeur"]
        borne_avant = pic["borne_avant"]
        borne_apres = pic["borne_apres"]
        nom_composant = pic["nom"]
        
        y_list.append([valeur_pic, borne_avant, borne_apres, nom_composant])
        component_names.append(nom_composant)

# ðŸ”¹ Convertir les noms des composants en classes entiÃ¨res
label_encoder = LabelEncoder()
component_classes = label_encoder.fit_transform(component_names)

# ðŸ”¹ Remplacer les noms dans Y par les classes correspondantes
for i in range(len(y_list)):
    y_list[i][3] = component_classes[i]  # Remplace le nom par l'entier correspondant

# ðŸ”¹ Transformer en array
Y = np.array(y_list)

# ðŸ”¹ SÃ©parer en train/test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ðŸ”¹ SÃ©parer Y_train en Y_train_scaled (valeur du pic et bornes) et Y_train_classes (classes entiÃ¨res)
Y_train_scaled = Y_train[:, :3]  # Valeur du pic et bornes
Y_test_scaled = Y_test[:, :3]
Y_train_classes = Y_train[:, 3]  # Classes entiÃ¨res
Y_test_classes = Y_test[:, 3]

class BasicConv1D(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_size, strides=1, **kwargs):
        super(BasicConv1D, self).__init__()
        self.conv = Conv1D(filters, kernel_size=kernel_size, strides=strides, **kwargs)
        self.activation = LeakyReLU()

    def call(self, x):
        x = self.conv(x)
        x = self.activation(x)
        return x


class Module_35x35(tf.keras.layers.Layer):
    def __init__(self, in_channels: int, regularization_factor: float, seed_value: int):
        super(Module_35x35, self).__init__()
        self.branch1 = tf.keras.Sequential([
            MaxPooling1D(pool_size=2),
            BasicConv1D(filters=in_channels * 2,
                        kernel_size=1,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value))
        ])
        self.branch2 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels,
                        kernel_size=1,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2,
                        kernel_size=3,
                        strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch3 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels,
                        kernel_size=1,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2,
                        kernel_size=3,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2,
                        kernel_size=3,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch4 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels * 2,
                        kernel_size=1,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])

    def call(self, x):
        branch1 = self.branch1(x)
        branch2 = self.branch2(x)
        branch3 = self.branch3(x)
        branch4 = self.branch4(x)
        out = tf.concat([branch1, branch2, branch3, branch4], axis=1)
        return out


class IPA(tf.keras.Model):
    def __init__(self,
                 seed_value,
                 regularization_factor,
                 dropout_rate=0.2):
        super(IPA, self).__init__()
        self.stem = tf.keras.Sequential([
            BasicConv1D(filters=16,
                        kernel_size=3,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=16,
                        kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=16,
                        kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.module_35x35 = Module_35x35(in_channels=32,
                                         regularization_factor=regularization_factor,
                                         seed_value=seed_value)
        self.flatten = Flatten()
        self.dropout = Dropout(rate=dropout_rate)
        
        # Sortie pour 4 valeurs : 3 pour les valeurs continues et 1 pour la classe
        self.regressor = Dense(3, name="value_bounds_output")  # Valeur du pic et bornes avant/aprÃ¨s
        self.classifier = Dense(1, activation='softmax', name="class_output")  # Classe du composant

    def call(self, x):
        out = self.stem(x)
        out = self.module_35x35(out)
        out = self.flatten(out)
        out = self.dropout(out)
        
        # PrÃ©dictions sÃ©parÃ©es
        value_bounds = self.regressor(out)  # Sortie pour les valeurs continues (3 valeurs)
        class_pred = self.classifier(out)  # Sortie pour la classe (1 valeur)
        
        return [value_bounds, class_pred]

# ðŸ”¹ CrÃ©er le modÃ¨le IPA
ipa_model = IPA(seed_value=1, regularization_factor=.0095)

# ðŸ”¹ Compiler le modÃ¨le avec deux sorties
ipa_model.compile(optimizer='adam',
                  loss=['mse', 'sparse_categorical_crossentropy'],  # mse pour les valeurs continues, sparse_categorical_crossentropy pour les classes
                  metrics=['accuracy'])

# ðŸ”¹ EntraÃ®ner le modÃ¨le
ipa_model.fit(X_train, 
              [Y_train_scaled, Y_train_classes],  # Les sorties: valeurs continues et classes
              epochs=10,
              batch_size=32,
              validation_data=(X_test, [Y_test_scaled, Y_test_classes]))

class IPA(tf.keras.Model):
    def __init__(self,
                 seed_value,
                 regularization_factor,
                 num_classes,  # Ajouter le nombre de classes en argument
                 dropout_rate=0.2):
        super(IPA, self).__init__()
        self.stem = tf.keras.Sequential([
            BasicConv1D(filters=16,
                        kernel_size=3,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=16,
                        kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=16,
                        kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.module_35x35 = Module_35x35(in_channels=32,
                                         regularization_factor=regularization_factor,
                                         seed_value=seed_value)
        self.flatten = Flatten()
        self.dropout = Dropout(rate=dropout_rate)
        
        # Sortie pour 4 valeurs : 3 pour les valeurs continues et 1 pour la classe
        self.regressor = Dense(3, name="value_bounds_output")  # Valeur du pic et bornes avant/aprÃ¨s
        
        # Modifier la couche de classification pour avoir "num_classes" unitÃ©s
        self.classifier = Dense(num_classes, activation='softmax', name="class_output")  # Nombre de classes

    def call(self, x):
        out = self.stem(x)
        out = self.module_35x35(out)
        out = self.flatten(out)
        out = self.dropout(out)
        
        # PrÃ©dictions sÃ©parÃ©es
        value_bounds = self.regressor(out)  # Sortie pour les valeurs continues (3 valeurs)
        class_pred = self.classifier(out)  # Sortie pour la classe (prÃ©diction de classe parmi "num_classes")
        
        return [value_bounds, class_pred]


import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from python.IPA_architecture import IPA  # Assurez-vous d'importer correctement IPA

# ðŸ”¹ Charger les donnÃ©es JSON en DataFrame
file_path = "chromatogrammes.json"  # Remplace par ton fichier rÃ©el
df = pd.read_json(file_path)

# ðŸ”¹ Suppression des chromatogrammes sans pics
df = df.dropna(subset=['pics'])

# ðŸ”¹ Suppression des chromatogrammes qui ne font pas exactement 71 840 points
df = df[df["x"].apply(len) == 71840]

# ðŸ”¹ CrÃ©ation de X avec 2 colonnes (temps de rÃ©tention et intensitÃ©) sous forme 2D
X = np.array([np.column_stack((row["x"], row["y"])) for _, row in df.iterrows()])  # (n, 71840, 2)

# ðŸ”¹ PrÃ©parer Y (valeur du pic, bornes, nom du composant)
y_list = []
component_names = []  # Stocker les noms des composÃ©s pour conversion en classes

for _, row in df.iterrows():
    for pic in row["pics"]:
        valeur_pic = pic["valeur"]
        borne_avant = pic["borne_avant"]
        borne_apres = pic["borne_apres"]
        nom_composant = pic["nom"]
        
        y_list.append([valeur_pic, borne_avant, borne_apres, nom_composant])
        component_names.append(nom_composant)

# ðŸ”¹ Convertir les noms des composants en classes entiÃ¨res
label_encoder = LabelEncoder()
component_classes = label_encoder.fit_transform(component_names)

# ðŸ”¹ Remplacer les noms dans Y par les classes correspondantes
for i in range(len(y_list)):
    y_list[i][3] = component_classes[i]  # Remplace le nom par l'entier correspondant

# ðŸ”¹ Transformer en array
Y = np.array(y_list)

# ðŸ”¹ SÃ©parer en train/test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ðŸ”¹ SÃ©parer Y_train en Y_train_scaled (valeur du pic et bornes) et Y_train_classes (classes entiÃ¨res)
Y_train_scaled = Y_train[:, :3]  # Valeur du pic et bornes
Y_test_scaled = Y_test[:, :3]
Y_train_classes = Y_train[:, 3]  # Classes entiÃ¨res
Y_test_classes = Y_test[:, 3]

# ðŸ”¹ CrÃ©er le modÃ¨le IPA
ipa_model = IPA(seed_value=1, regularization_factor=.0095)

# ðŸ”¹ Compiler le modÃ¨le avec deux sorties
ipa_model.compile(optimizer='adam',
                  loss={'value_bounds_output': 'mse', 'class_output': 'sparse_categorical_crossentropy'},
                  metrics={'value_bounds_output': 'mse', 'class_output': 'accuracy'})

# ðŸ”¹ EntraÃ®ner le modÃ¨le
ipa_model.fit(X_train, 
              {'value_bounds_output': Y_train_scaled, 'class_output': Y_train_classes},  # Les sorties: valeurs continues et classes
              epochs=10,
              batch_size=32,
              validation_data=(X_test, {'value_bounds_output': Y_test_scaled, 'class_output': Y_test_classes}))

import tensorflow as tf
from tensorflow.keras import layers, initializers
from tensorflow.keras.regularizers import L2
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, Dense, LeakyReLU, Input


# DÃ©finition de la couche de convolution de base
class BasicConv1D(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_size, strides=1, **kwargs):
        super(BasicConv1D, self).__init__()
        self.conv = Conv1D(filters, kernel_size=kernel_size, strides=strides, **kwargs)
        self.activation = LeakyReLU()

    def call(self, x):
        x = self.conv(x)
        x = self.activation(x)
        return x


# Module 35x35 qui contient des branches multiples
class Module_35x35(tf.keras.layers.Layer):
    def __init__(self, in_channels: int, regularization_factor: float, seed_value: int):
        super(Module_35x35, self).__init__()
        self.branch1 = tf.keras.Sequential([
            MaxPooling1D(pool_size=2),
            BasicConv1D(filters=in_channels * 2,
                        kernel_size=1,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value))
        ])
        self.branch2 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels,
                        kernel_size=1,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2,
                        kernel_size=3,
                        strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch3 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels,
                        kernel_size=1,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2,
                        kernel_size=3,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2,
                        kernel_size=3,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch4 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels * 2,
                        kernel_size=1,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])

    def call(self, x):
        branch1 = self.branch1(x)
        branch2 = self.branch2(x)
        branch3 = self.branch3(x)
        branch4 = self.branch4(x)
        out = tf.concat([branch1, branch2, branch3, branch4], axis=1)
        return out


# Classe IPA qui contient le modÃ¨le complet
class IPA(tf.keras.Model):
    def __init__(self, seed_value, regularization_factor, dropout_rate=0.2):
        super(IPA, self).__init__()
        # DÃ©finition des couches de base
        self.stem = tf.keras.Sequential([
            BasicConv1D(filters=16,
                        kernel_size=3,
                        strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=16,
                        kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=16,
                        kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.module_35x35 = Module_35x35(in_channels=32,
                                         regularization_factor=regularization_factor,
                                         seed_value=seed_value)
        self.flatten = Flatten()
        self.dropout = Dropout(rate=dropout_rate)
        
        # Sortie pour 4 valeurs : 3 pour les valeurs continues et 1 pour la classe
        self.regressor = Dense(3, name="value_bounds_output")  # Valeur du pic et bornes avant/aprÃ¨s
        self.classifier = Dense(1, activation='softmax', name="class_output")  # Classe du composant

    def call(self, x):
        out = self.stem(x)
        out = self.module_35x35(out)
        out = self.flatten(out)
        out = self.dropout(out)
        
        # PrÃ©dictions sÃ©parÃ©es
        value_bounds = self.regressor(out)  # Sortie pour les valeurs continues (3 valeurs)
        class_pred = self.classifier(out)  # Sortie pour la classe (1 valeur)
        
        return [value_bounds, class_pred]


if __name__ == '__main__':
    # CrÃ©er une instance du modÃ¨le
    ipa_model = IPA(seed_value=1, regularization_factor=.0095)

    # RÃ©sumer le modÃ¨le pour afficher la structure
    ipa_model.build((None, 71840, 2))  # Ajuster Ã  l'entrÃ©e (n, 71840, 2)
    ipa_model.summary()
