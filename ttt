import cv2
import numpy as np
import os

def preprocess_image(image_path):
    
    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
    
    
    normalized_image = cv2.normalize(image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)
    
    
    blurred_image = cv2.bilateralFilter(normalized_image, 5, 75, 75)
    
    
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    equalized_image = clahe.apply(blurred_image)
    
    
    sharp_kernel = np.array([[-1, -1, -1],
                             [-1,  9, -1],
                             [-1, -1, -1]])
    sharpened_image = cv2.filter2D(equalized_image, -1, sharp_kernel)
    
    
    return sharpened_image


def preprocess_images_in_folder(input_folder, output_folder):
    
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    
    for filename in os.listdir(input_folder):
        if filename.endswith('.jpg') or filename.endswith('.jpeg') or filename.endswith('.png'):
            
            input_image_path = os.path.join(input_folder, filename)
            
            # Prétraitement de l'image
            preprocessed_image = preprocess_image(input_image_path)
            
            
            output_image_path = os.path.join(output_folder, filename)
            
            # Sauvegarder l'image prétraitée dans le dossier de sortie
            cv2.imwrite(output_image_path, preprocessed_image)
            print(f"Image prétraitée sauvegardée: {output_image_path}")





input_folder ='C:/Users/karim/Desktop/train'


output_folder = 'C:/Users/karim/Desktop/output_images'

preprocess_images_in_folder(input_folder, output_folder)

##############################################
import cv2
import numpy as np

def generer_image_difference(image_path, output_path, seuil=30):
    # Charger l'image
    image = cv2.imread(image_path)

    # Convertir l'image en niveaux de gris
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Calculer la différence par rapport à une valeur seuil
    _, difference = cv2.threshold(gray, seuil, 160, cv2.THRESH_BINARY)

    # Enregistrer l'image de différence
    cv2.imwrite(output_path, difference)

# Exemple d'utilisation
generer_image_difference("C:/Users/karim/Desktop/Pour Yann/Tbaro WT pyruvte point final_4D_ch00.jpg", "C:/Users/karim/Desktop/difference_image.jpg")



import cv2
import numpy as np
import matplotlib.pyplot as plt

def generer_carte_difference(image_path1, image_path2, output_path):
    # Charger les deux images
    image1 = cv2.imread(image_path1)
    image2 = cv2.imread(image_path2)

    # Vérifier que les images sont de la même taille
    if image1.shape != image2.shape:
        raise ValueError("Les dimensions des images ne correspondent pas.")

    # Calculer la différence entre les deux images
    difference = cv2.absdiff(image1, image2)

    # Convertir l'image de différence en niveaux de gris
    difference_gray = cv2.cvtColor(difference, cv2.COLOR_BGR2GRAY)

    # Créer une carte de chaleur (heatmap)
    heatmap = cv2.applyColorMap(difference_gray, cv2.COLORMAP_HOT)

    # Enregistrer la carte de différence
    cv2.imwrite(output_path, heatmap)

    # Afficher les images d'origine et la carte de différence
    plt.subplot(131), plt.imshow(cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)), plt.title('Image 1')
    plt.subplot(132), plt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)), plt.title('Image 2')
    plt.subplot(133), plt.imshow(cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)), plt.title('Carte de Différence')
    plt.show()

# Exemple d'utilisation
generer_carte_difference("C:/Users/karim/Desktop/Pour Yann/Tbaro WT pyruvte point final_4D_ch00.jpg", "C:/Users/karim/Desktop/difference_image.jpg", "C:/Users/karim/Desktop/carte_difference.jpg")


import cv2
import numpy as np

def detecter_positions_cellules(image_path1, image_path2, output_path, seuil_contour=30):
    # Charger les deux images
    image1 = cv2.imread(image_path1)
    image2 = cv2.imread(image_path2)

    # Vérifier que les images sont de la même taille
    if image1.shape != image2.shape:
        raise ValueError("Les dimensions des images ne correspondent pas.")

    # Convertir les images en niveaux de gris
    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)

    # Calculer la différence entre les deux images
    difference = cv2.absdiff(gray1, gray2)

    # Appliquer un seuillage pour détecter les variations de pixel
    _, seuil_image = cv2.threshold(difference, seuil_contour, 255, cv2.THRESH_BINARY)

    # Trouver les contours dans l'image seuillée
    contours, _ = cv2.findContours(seuil_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    # Dessiner les contours sur l'image originale
    contours_image = image1.copy()
    cv2.drawContours(contours_image, contours, -1, (0, 255, 0), 2)

    # Enregistrer l'image avec les positions des cellules identifiées
    cv2.imwrite(output_path, contours_image)

# Exemple d'utilisation
detecter_positions_cellules("C:/Users/karim/Desktop/Pour Yann/Tbaro WT pyruvte point final_4D_ch00.jpg", "C:/Users/karim/Desktop/difference_image.jpg", "C:/Users/karim/Desktop/carte_difference.jpg")


##############################3
import os
import cv2
import numpy as np

def total_variation(image):
    """
    Calcul de la variation totale d'une image.
    """
    dx = np.diff(image.astype(np.float64), axis=1)
    dy = np.diff(image.astype(np.float64), axis=0)
    return np.sum(np.abs(dx)) + np.sum(np.abs(dy))

def total_variation_regularization(image, lambda_=0.044, num_iterations=100):
    """
    Régularisation par variation totale (TV) d'une image.
    """
    
    normalized_image = image.astype(np.float64) / 255.0
    
    smoothed_image = np.copy(normalized_image)
    for _ in range(num_iterations):
        dx = cv2.Sobel(smoothed_image, cv2.CV_64F, 1, 0, ksize=3)
        dy = cv2.Sobel(smoothed_image, cv2.CV_64F, 0, 1, ksize=3)
        gradient_magnitude = np.sqrt(dx**2 + dy**2)
        tv_gradient = lambda_ * np.mean(gradient_magnitude)
        smoothed_image -= tv_gradient * (dx + dy)
    
    
    smoothed_image = np.clip(smoothed_image * 255.0, 0, 255).astype(np.uint8)
    
    return smoothed_image


input_folder ='C:/Users/karim/Desktop/train'


output_folder = 'C:/Users/karim/Desktop/output_images'
os.makedirs(output_folder, exist_ok=True)


file_names = os.listdir(input_folder)

for file_name in file_names:
    if file_name.endswith(('.jpg', '.jpeg', '.png')):
        
        image_path = os.path.join(input_folder, file_name)
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)

     
        smoothed_image = total_variation_regularization(image)

      
        output_path = os.path.join(output_folder, file_name)
        cv2.imwrite(output_path, smoothed_image)

print("Traitement terminé. Les images ont été enregistrées dans", output_folder)
#########################################33
# # Créez une grille de valeurs pour les hyperparamètres qu'on souhaite optimiser
# learning_rates = [0.00025, 0.001]
# maxi_iters = [2000, 5000]

# best_ap = 0
# best_cfg = None
# tab=[]

# for lr in learning_rates:
#     for maxi_iter in maxi_iters:
#         cfg = get_cfg()
#         #cfg.MODEL.DEVICE = 'cpu'
#         cfg.MODEL.DEVICE = 'cuda'
#         cfg.OUTPUT_DIR = "data/projet_m1/models/Detectron2_Modelssearch"
#         cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml"))
#         cfg.DATASETS.TRAIN = ("my_dataset_train",)
#         cfg.DATASETS.TEST = ("my_dataset_val",)
#         cfg.DATALOADER.NUM_WORKERS = 0

#         cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS=False
#         cfg.MODEL.MASK_ON = True
#         cfg.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION = 20
#         cfg.MODEL.ROI_MASK_HEAD.POOLER_RESOLUTION = 20
#         cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[8], [16], [32], [64], [128]]
#         cfg.MODEL.ROI_HEADS.POSITIVE_FRACTION = 0.5 #0.5
#         #cfg.MODEL.ROI_MASK_HEAD.POOLER_SAMPLING_RATIO = 1
#         #cfg.MODEL.ROI_HEADS.POOLER_SAMPLING_RATIO= 1


#         cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_X_101_32x8d_FPN_3x.yaml")
#         #cfg.MODEL.WEIGHTS = "data/projet_m1/models/Detectron2_Models7/model_final.pth"
#           # Let training initialize from model zoo
#         cfg.SOLVER.IMS_PER_BATCH = 4
#           # This is the real "batch size" commonly known to deep learning people
#         cfg.SOLVER.BASE_LR = lr #0.001
#         # pick a good LR
#         cfg.SOLVER.MAX_ITER = maxi_iter #2000
#         cfg.SOLVER.CHECKPOINT_PERIOD = 1000
#         # 1000 iterations for demo purposes
#         cfg.SOLVER.STEPS = []
#         # do not decay learning rate
#         cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512
#         # Default is 512, using 256 for this dataset.
#         #cfg.MODEL.RETINANET.NUM_CLASSES = 2 # We have 1 classes. (Nuclei)
#         cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1
#         # NOTE: this config means the number of classes, without the background. Do not use num_classes+1 here.
#         cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.4
#         cfg.TEST.PRECISE_BN.ENABLED = True
#         #cfg.TEST.PRECISE_BN.NUM_ITER = 1000
#         cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST= 0.5
#         cfg.TEST.DETECTIONS_PER_IMAGE = 1000
#         cfg.TEST.EVAL_PERIOD = 159
#         os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
#         trainer = CocoTrainer(cfg)
#         trainer.resume_or_load(resume=False)
#         trainer.train()

#         cfg.MODEL.WEIGHTS = "data/projet_m1/models/Detectron2_Modelssearch/model_final.pth"
#         cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.4
#         cfg.TEST.PRECISE_BN.ENABLED = True
#         cfg.TEST.PRECISE_BN.NUM_ITER = 1000
#         cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST= 0.5
#         cfg.TEST.DETECTIONS_PER_IMAGE = 1000


#         predictor = DefaultPredictor(cfg)
#         evaluator = COCOEvaluator("my_dataset_test", cfg, False, output_dir="./output/",max_dets_per_image=1000)
#         val_loader = build_detection_test_loader(cfg, "my_dataset_test")
#         metrics=inference_on_dataset(trainer.model, val_loader, evaluator)
#         #print(metrics)
#         tab.append(metrics)
#         # Évaluez les performances du modèle (par exemple, AP)
#         ap = metrics["segm"]["AP50"]
#         if ap > best_ap:
#             best_ap = ap
#             best_cfg = cfg.clone()

# print(f"Meilleur AP : {best_ap}")
# print(f"Meilleurs hyperparamètres : {best_cfg}")
################################3
from detectron2 import model_zoo
from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg
from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_train_loader
from detectron2.data import detection_utils as utils
from detectron2.data import transforms as T
from detectron2.evaluation import COCOEvaluator

# Enregistrer les ensembles de données
from detectron2.data.datasets import register_coco_instances
register_coco_instances("my_dataset_train", {}, "C:/Users/karim/Desktop/projetm1_data/train/_annotations.coco.json", "C:/Users/karim/Desktop/projetm1_data/train")
register_coco_instances("my_dataset_val", {}, "C:/Users/karim/Desktop/projetm1_data/valid/_annotations.coco.json", "C:/Users/karim/Desktop/projetm1_data/valid")

# Data Augmentation personnalisée
def custom_mapper(dataset_dict):
    """
    Préparation des données pour inclure des transformations d'augmentation.
    """
    dataset_dict = dataset_dict.copy()  # Créer une copie pour éviter de modifier l'original
    image = utils.read_image(dataset_dict["file_name"], format="BGR")
    # Appliquer des augmentations
    aug = T.AugmentationList([
        T.RandomFlip(prob=0.5, horizontal=True, vertical=False),  # Flip horizontal
        T.RandomFlip(prob=0.5, horizontal=False, vertical=True),  # Flip vertical
        T.RandomRotation(angle=[-90, 90]),  # Rotation aléatoire entre -30° et 30°
    ])
    image, transforms = T.apply_transform_gens(aug, image)
    annos = [
        utils.transform_instance_annotations(obj, transforms, image.shape[:2])
        for obj in dataset_dict["annotations"]
    ]
    dataset_dict["annotations"] = annos
    dataset_dict["image"] = torch.as_tensor(image.transpose(2, 0, 1).astype("float32"))
    return dataset_dict

# Personnalisation de l'entraîneur pour utiliser le mapper personnalisé
class CocoTrainer(DefaultTrainer):
    @classmethod
    def build_evaluator(cls, cfg, dataset_name, output_folder=None):
        if output_folder is None:
            os.makedirs("coco_eval", exist_ok=True)
            output_folder = "coco_eval"
        return COCOEvaluator(dataset_name, cfg, False, output_folder)

    @classmethod
    def build_train_loader(cls, cfg):
        return build_detection_train_loader(cfg, mapper=custom_mapper)
        #########################33

import cv2
import numpy as np
import matplotlib.pyplot as plt
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog
from scipy.ndimage import binary_erosion

# Charger l'image
image_path = "chemin/vers/ton_image.jpg"
image = cv2.imread(image_path)

# Configurer Detectron2
cfg = get_cfg()
cfg.merge_from_file("detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
cfg.MODEL.WEIGHTS = "detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl"
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Seuil de confiance
cfg.MODEL.DEVICE = "cuda"  # Utiliser "cuda" pour GPU ou "cpu" pour CPU

# Prédictions avec Detectron2
predictor = DefaultPredictor(cfg)
outputs = predictor(image)
instances = outputs["instances"].to("cpu")

# Vérifier si des objets ont été détectés
if len(instances) == 0:
    print("Aucun objet détecté.")
else:
    # Extraire les propriétés des objets détectés
    masks = instances.pred_masks.numpy()
    scores = instances.scores.numpy()
    boxes = instances.pred_boxes.tensor.numpy()

    diameters = []  # Diamètre des particules
    ratios = []  # Rapport entre valeur moyenne des pixels objet et pixels voisins

    for i, mask in enumerate(masks):
        # Érosion pour obtenir les pixels voisins immédiats
        eroded_mask = binary_erosion(mask, structure=np.ones((3, 3)))
        border_mask = mask ^ eroded_mask  # Bordure autour de l'objet

        # Vérifier qu'il n'y a pas d'autres objets proches
        for j, other_mask in enumerate(masks):
            if i != j and np.any(mask & other_mask):  # Intersection avec un autre objet
                break
        else:
            # Si aucun voisin proche
            object_pixels = image[mask]
            border_pixels = image[border_mask]

            # Vérifier que la bordure n'est pas vide
            if border_pixels.size > 0:
                mean_object_value = np.mean(object_pixels)
                mean_border_value = np.mean(border_pixels)
                ratio = mean_object_value / mean_border_value
                ratios.append(ratio)

                # Calculer le diamètre (diagonale de la boîte englobante)
                x1, y1, x2, y2 = boxes[i]
                width = x2 - x1
                height = y2 - y1
                diameter = np.sqrt(width**2 + height**2)
                diameters.append(diameter)

    # Tracer la courbe
    if len(diameters) > 0 and len(ratios) > 0:
        plt.figure(figsize=(10, 6))
        plt.scatter(diameters, ratios, color='b', label="Rapport objet/fond")
        plt.title("Rapport de valeur moyenne (objet/fond) en fonction du diamètre")
        plt.xlabel("Diamètre des particules (px)")
        plt.ylabel("Rapport valeur moyenne objet/fond")
        plt.grid()
        plt.legend()
        plt.show()
    else:
        print("Aucun objet valide trouvé après exclusion des voisins.")

import cv2
import numpy as np
import matplotlib.pyplot as plt
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from scipy.ndimage import binary_erosion

# Charger l'image
image_path = "chemin/vers/ton_image.jpg"
image = cv2.imread(image_path)

# Configurer Detectron2
cfg = get_cfg()
cfg.merge_from_file("detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
cfg.MODEL.WEIGHTS = "detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl"
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Seuil de confiance
cfg.MODEL.DEVICE = "cuda"  # Utiliser "cuda" pour GPU ou "cpu" pour CPU

# Prédictions avec Detectron2
predictor = DefaultPredictor(cfg)
outputs = predictor(image)
instances = outputs["instances"].to("cpu")

# Vérifier si des objets ont été détectés
if len(instances) == 0:
    print("Aucun objet détecté.")
else:
    # Extraire les propriétés des objets détectés
    masks = instances.pred_masks.numpy()
    scores = instances.scores.numpy()
    boxes = instances.pred_boxes.tensor.numpy()

    diameters = []  # Diamètre des particules
    ratios = []  # Rapport entre valeur moyenne des pixels objet et pixels voisins

    for i, mask in enumerate(masks):
        # Érosion pour obtenir les pixels voisins immédiats
        eroded_mask = binary_erosion(mask, structure=np.ones((3, 3)))
        border_mask = mask ^ eroded_mask  # Bordure autour de l'objet

        # Vérifier qu'il n'y a pas d'autres objets proches
        for j, other_mask in enumerate(masks):
            if i != j and np.any(mask & other_mask):  # Intersection avec un autre objet
                break
        else:
            # Si aucun voisin proche
            object_pixels = image[mask]
            border_pixels = image[border_mask]

            # Vérifier que la bordure n'est pas vide
            if border_pixels.size > 0:
                mean_object_value = np.mean(object_pixels)
                mean_border_value = np.mean(border_pixels)
                ratio = mean_object_value / mean_border_value
                ratios.append(ratio)

                # Calculer le diamètre (diagonale de la boîte englobante)
                x1, y1, x2, y2 = boxes[i]
                width = x2 - x1
                height = y2 - y1
                diameter = np.sqrt(width**2 + height**2)
                diameters.append(diameter)

    # Regrouper les diamètres en intervalles (bins)
    if len(diameters) > 0 and len(ratios) > 0:
        bin_edges = np.linspace(min(diameters), max(diameters), num=20)  # 20 intervalles
        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2  # Centres des intervalles

        # Calculer la moyenne des ratios pour chaque intervalle
        binned_ratios = []
        for i in range(len(bin_edges) - 1):
            bin_mask = (np.array(diameters) >= bin_edges[i]) & (np.array(diameters) < bin_edges[i + 1])
            if np.any(bin_mask):  # Vérifier s'il y a des points dans cet intervalle
                binned_ratios.append(np.mean(np.array(ratios)[bin_mask]))
            else:
                binned_ratios.append(None)  # Aucun point dans cet intervalle

        # Filtrer les intervalles non vides
        filtered_centers = [c for c, r in zip(bin_centers, binned_ratios) if r is not None]
        filtered_ratios = [r for r in binned_ratios if r is not None]

        # Tracer la courbe lissée
        plt.figure(figsize=(10, 6))
        plt.plot(filtered_centers, filtered_ratios, marker='o', color='b', label="Rapport objet/fond (lissé)")
        plt.title("Rapport de valeur moyenne (objet/fond) en fonction du diamètre (courbe)")
        plt.xlabel("Diamètre des particules (px)")
        plt.ylabel("Rapport valeur moyenne objet/fond")
        plt.grid()
        plt.legend()
        plt.show()
    else:
        print("Aucun objet valide trouvé après exclusion des voisins.")


####################################################################################################
import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog

# Configurer Detectron2
cfg = get_cfg()
cfg.merge_from_file("detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
cfg.MODEL.WEIGHTS = "detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl"
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Seuil de confiance
cfg.MODEL.DEVICE = "cuda"  # Utiliser "cuda" pour GPU ou "cpu" pour CPU

# Initialiser le modèle Detectron2
predictor = DefaultPredictor(cfg)

# Dossier contenant les images
input_folder = "chemin/vers/dossier_images"
output_folder = "chemin/vers/dossier_output"
os.makedirs(output_folder, exist_ok=True)

# Parcourir toutes les images du dossier
for filename in os.listdir(input_folder):
    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
        image_path = os.path.join(input_folder, filename)
        image = cv2.imread(image_path)
        
        # Prédictions avec Detectron2
        outputs = predictor(image)
        instances = outputs["instances"].to("cpu")
        masks = instances.pred_masks.numpy() if instances.has("pred_masks") else None
        
        if masks is not None and len(masks) > 0:
            # Fusionner tous les masques détectés pour créer une image binaire
            combined_mask = np.any(masks, axis=0).astype(np.uint8) * 255
        else:
            # Aucun objet détecté, créer une image noire
            combined_mask = np.zeros(image.shape[:2], dtype=np.uint8)

        # Sauvegarder et afficher le masque
        output_path = os.path.join(output_folder, f"mask_{filename}")
        cv2.imwrite(output_path, combined_mask)

        # Afficher l'image originale et le masque
        plt.figure(figsize=(12, 6))
        plt.subplot(1, 2, 1)
        plt.title("Image originale")
        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        plt.axis('off')

        plt.subplot(1, 2, 2)
        plt.title("Masque détecté")
        plt.imshow(combined_mask, cmap="gray")
        plt.axis('off')

        plt.show()

print(f"Tous les masques ont été sauvegardés dans : {output_folder}")
##################################################################################
import os
import cv2
import numpy as np
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog
from detectron2.utils.visualizer import Visualizer

# Configurer Detectron2
cfg = get_cfg()
cfg.merge_from_file("detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
cfg.MODEL.WEIGHTS = "detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl"
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Seuil de confiance
cfg.MODEL.DEVICE = "cuda"  # Utiliser "cuda" pour GPU ou "cpu" pour CPU

predictor = DefaultPredictor(cfg)

# Dossier contenant les images
input_folder = "chemin/vers/ton_dossier_images"
output_folder = "chemin/vers/ton_dossier_resultats"
os.makedirs(output_folder, exist_ok=True)

# Parcourir les images dans le dossier
for filename in os.listdir(input_folder):
    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # Vérifier les formats d'image
        # Charger l'image
        image_path = os.path.join(input_folder, filename)
        image = cv2.imread(image_path)

        # Faire les prédictions
        outputs = predictor(image)
        instances = outputs["instances"].to("cpu")

        # Créer un masque binaire (1 pour les objets détectés, 0 pour le fond)
        mask = np.zeros(image.shape[:2], dtype=np.uint8)
        if len(instances) > 0:
            for instance_mask in instances.pred_masks.numpy():
                mask[instance_mask] = 255

        # Sauvegarder le masque avec le même nom que l'image d'entrée
        output_path = os.path.join(output_folder, filename)
        cv2.imwrite(output_path, mask)

print("Les masques ont été générés et sauvegardés dans le dossier :", output_folder)
##########################################################################################
import os
import cv2
import json
import numpy as np
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog

# Dossier contenant les images d'entrée
input_folder = "chemin/vers/dossier/images"
output_coco_path = "annotations_coco.json"  # Chemin pour le fichier JSON

# Configurer Detectron2
cfg = get_cfg()
cfg.merge_from_file("detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
cfg.MODEL.WEIGHTS = "detectron2://COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x/137849600/model_final_f10217.pkl"
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Seuil de confiance
cfg.MODEL.DEVICE = "cuda"  # Utiliser "cuda" pour GPU ou "cpu" pour CPU

predictor = DefaultPredictor(cfg)

# Initialiser les données COCO
coco_annotations = {
    "images": [],
    "annotations": [],
    "categories": [{"id": 1, "name": "object", "supercategory": "none"}]  # Ajouter des catégories selon le besoin
}

annotation_id = 1  # Identifiant unique pour chaque annotation
image_id = 1       # Identifiant unique pour chaque image

# Parcourir les images du dossier
for filename in os.listdir(input_folder):
    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
        image_path = os.path.join(input_folder, filename)
        image = cv2.imread(image_path)
        height, width = image.shape[:2]

        # Ajouter les métadonnées de l'image
        coco_annotations["images"].append({
            "id": image_id,
            "file_name": filename,
            "height": height,
            "width": width
        })

        # Prédictions avec Detectron2
        outputs = predictor(image)
        instances = outputs["instances"].to("cpu")

        if len(instances) > 0:
            masks = instances.pred_masks.numpy()
            boxes = instances.pred_boxes.tensor.numpy()
            scores = instances.scores.numpy()

            # Ajouter chaque objet détecté comme annotation
            for i, mask in enumerate(masks):
                # Convertir le masque binaire en polygone (format COCO)
                contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                segmentation = [contour.flatten().tolist() for contour in contours if len(contour.flatten()) > 4]

                if len(segmentation) > 0:  # Ajouter uniquement si un polygone est valide
                    x_min, y_min, x_max, y_max = boxes[i]
                    width = x_max - x_min
                    height = y_max - y_min
                    area = width * height

                    coco_annotations["annotations"].append({
                        "id": annotation_id,
                        "image_id": image_id,
                        "category_id": 1,  # Catégorie unique, adapter selon le besoin
                        "bbox": [float(x_min), float(y_min), float(width), float(height)],
                        "area": float(area),
                        "segmentation": segmentation,
                        "iscrowd": 0
                    })

                    annotation_id += 1

        # Passer à l'image suivante
        image_id += 1

# Sauvegarder les annotations dans un fichier JSON
with open(output_coco_path, "w") as json_file:
    json.dump(coco_annotations, json_file, indent=4)

print(f"Annotations sauvegardées dans {output_coco_path}")

import numpy as np
import matplotlib.pyplot as plt
from scipy.ndimage import sobel, binary_dilation
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2 import model_zoo
import torch

# Assurez-vous que vous avez chargé votre modèle Detectron2 correctement
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"))
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Seulement les objets avec un score de confiance > 0.5
predictor = DefaultPredictor(cfg)

# Chargement d'une image (par exemple, depuis un fichier ou une source)
image_path = "path_to_your_image.jpg"  # Remplacez par le chemin réel
image = cv2.imread(image_path)  # Image au format BGR
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convertir en RGB pour l'affichage

# Prédictions de Detectron2
outputs = predictor(image_rgb)
instances = outputs["instances"]  # Objets détectés

# Extraire les masques des objets détectés
masks = instances.pred_masks.numpy()  # Masques des objets détectés

# Calculer les statistiques globales de l'image (utile pour la normalisation)
image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convertir l'image en niveaux de gris pour la détection de contours
global_mean = np.mean(image_gray)
global_std = np.std(image_gray)

# Fonction pour calculer le contraste local en utilisant Sobel
def compute_local_contrast(image, masks):
    diameters = []  # Diamètre des particules
    contrast_scores = []  # Score de contraste local

    for i, mask in enumerate(masks):
        object_pixels = image[mask]

        # Appliquer un filtre de Sobel (ou Laplacien) pour obtenir les gradients
        sobel_x = sobel(image, axis=0)  # Gradient dans la direction x
        sobel_y = sobel(image, axis=1)  # Gradient dans la direction y
        gradient_magnitude = np.hypot(sobel_x, sobel_y)  # Magnitude du gradient

        # Dilater le masque de l'objet pour définir la zone voisine
        dilated_mask = binary_dilation(mask, structure=np.ones((5, 5)))
        neighbor_mask = dilated_mask & ~mask  # Zone voisine sans l'objet

        # Extraire les pixels des gradients dans le voisinage
        neighbor_gradients = gradient_magnitude[neighbor_mask]

        # Calculer le contraste local entre l'objet et son voisinage
        if neighbor_gradients.size > 0:  # Eviter division par zéro
            object_mean = np.mean(object_pixels)
            neighbor_mean = np.mean(neighbor_gradients)
            contrast_score = (object_mean - neighbor_mean) / (neighbor_mean + 1e-6)  # Éviter diviser par zéro
            contrast_scores.append(contrast_score)

            # Calculer le diamètre (hypothèse : aire circulaire)
            area = np.sum(mask)
            diameter = np.sqrt(4 * area / np.pi)
            diameters.append(diameter)

    return diameters, contrast_scores

# Appliquer l'analyse de contraste local
diameters, contrast_scores = compute_local_contrast(image_gray, masks)

# Afficher un nuage de points des résultats
plt.figure(figsize=(10, 6))
scatter = plt.scatter(diameters, contrast_scores, c=contrast_scores, cmap='coolwarm', alpha=0.7)
plt.colorbar(scatter, label="Score de contraste local")
plt.title("Contraste local des objets détectés")
plt.xlabel("Diamètre de la particule (pixels)")
plt.ylabel("Score de contraste local")
plt.grid()
plt.show()

