import tensorflow as tf
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import EarlyStopping
from tqdm.keras import TqdmCallback

# =========================
# Définir la fonction de perte personnalisée
# =========================

import tensorflow as tf


class CustomLoss(tf.keras.losses.Loss):

    def __init__(
        self,
        n_splits,
        weight_prob=1.0,
        weight_loc=1.0,
        weight_area=1.0,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.n_splits = n_splits
        self.weight_prob = weight_prob
        self.weight_loc = weight_loc
        self.weight_area = weight_area

    def call(self, y_true, y_pred, sample_weight=None):

        # Cast to dtype float32 just in case
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)

        # Unpack trues and preds
        pred_prob, pred_loc, pred_area = tf.split(y_pred, self.n_splits, axis=-1)
        true_prob, true_loc, true_area = tf.split(y_true, self.n_splits, axis=-1)

        # Mask for where y_true is indicating a peak
        mask = tf.math.equal(true_prob, 1.)

        # Compute loss for all instances
        prob_loss = tf.keras.losses.BinaryCrossentropy()(true_prob, pred_prob)
        # Compute loss only for instances in mask
        loc_loss = tf.keras.losses.BinaryCrossentropy()(
            tf.boolean_mask(true_loc, mask), tf.boolean_mask(pred_loc, mask))
        area_loss = MeanRelativeError()(
            tf.boolean_mask(true_area, mask), tf.boolean_mask(pred_area, mask))

        return (
            prob_loss * self.weight_prob +
            loc_loss * self.weight_loc +
            area_loss * self.weight_area
        )

class MeanRelativeError(tf.keras.losses.Loss):

    def call(self, y_true, y_pred, sample_weight=None):
        return tf.math.abs(y_true - y_pred) / y_true


# =========================
# Définir les métriques personnalisées
# =========================

import tensorflow as tf



class CustomTruePositiveRate(tf.keras.metrics.Metric):

    def __init__(self, threshold, **kwargs):
        super().__init__(**kwargs)
        self.threshold = threshold
        self.true_positives = self.add_weight(name='tp', initializer='zeros')
        self.false_negatives = self.add_weight(name='tp', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):

        y_true = tf.split(y_true, 3, axis=-1)[0]
        y_pred = tf.split(y_pred, 3, axis=-1)[0]

        y_pred = y_pred > self.threshold
        y_true = tf.cast(y_true, tf.bool)

        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))
        values = tf.cast(values, dtype=self.dtype)
        self.true_positives.assign_add(tf.reduce_sum(values))

        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, False))
        values = tf.cast(values, dtype=self.dtype)
        self.false_negatives.assign_add(tf.reduce_sum(values))

    def result(self):
        return self.true_positives / (self.true_positives + self.false_negatives)

    def reset_states(self):
        self.true_positives.assign(0)
        self.false_negatives.assign(0)


class CustomFalsePositiveRate(tf.keras.metrics.Metric):

    def __init__(self, threshold, **kwargs):
        super().__init__(**kwargs)
        self.threshold = threshold
        self.false_positives = self.add_weight(name='tp', initializer='zeros')
        self.true_negatives = self.add_weight(name='tp', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):

        y_true = tf.split(y_true, 3, axis=-1)[0]
        y_pred = tf.split(y_pred, 3, axis=-1)[0]

        y_pred = y_pred > self.threshold
        y_true = tf.cast(y_true, tf.bool)

        values = tf.logical_and(tf.equal(y_true, False), tf.equal(y_pred, True))
        values = tf.cast(values, dtype=self.dtype)
        self.false_positives.assign_add(tf.reduce_sum(values))

        values = tf.logical_and(tf.equal(y_true, False), tf.equal(y_pred, False))
        values = tf.cast(values, dtype=self.dtype)
        self.true_negatives.assign_add(tf.reduce_sum(values))

    def result(self):
        return self.false_positives / (self.false_positives + self.true_negatives)

    def reset_states(self):
        self.false_positives.assign(0)
        self.true_negatives.assign(0)


class CustomAUC(tf.keras.metrics.AUC):

    def update_state(self, y_true, y_pred, sample_weight=None):

        y_true = tf.split(y_true, 3, axis=-1)[0]
        y_pred = tf.split(y_pred, 3, axis=-1)[0]

        super().update_state(y_true, y_pred, sample_weight)


class CustomAccuracy(tf.keras.metrics.BinaryAccuracy):

    def update_state(self, y_true, y_pred, sample_weight=None):

        y_true = tf.split(y_true, 3, axis=-1)[0]
        y_pred = tf.split(y_pred, 3, axis=-1)[0]

        super().update_state(y_true, y_pred, sample_weight)


class CustomMREArea(tf.keras.metrics.Mean):

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true_prob, _, y_true = tf.split(y_true, 3, axis=-1)
        y_pred_prob, _, y_pred = tf.split(y_pred, 3, axis=-1)

        mask = tf.math.logical_and(
            tf.math.equal(y_true_prob, 1.), tf.math.greater(y_pred_prob, 0.5))

        y_true = tf.boolean_mask(y_true, mask)
        y_pred = tf.boolean_mask(y_pred, mask)

        error = tf.math.divide_no_nan(
            tf.math.abs(y_true - y_pred),
            tf.math.abs(y_true)
        )
        super().update_state(error, sample_weight=sample_weight)


class CustomMRELoc(tf.keras.metrics.Mean):

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true_prob, y_true, _ = tf.split(y_true, 3, axis=-1)
        y_pred_prob, y_pred, _  = tf.split(y_pred, 3, axis=-1)

        mask = tf.math.logical_and(
            tf.math.equal(y_true_prob, 1.), tf.math.greater(y_pred_prob, 0.5))

        y_true = tf.boolean_mask(y_true, mask)
        y_pred = tf.boolean_mask(y_pred, mask)

        error = tf.math.divide_no_nan(
            tf.math.abs(y_true - y_pred),
            tf.math.abs(y_true)
        )
        super().update_state(error, sample_weight=sample_weight)


class CustomMAELoc(tf.keras.metrics.Mean):

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true_prob, y_true, _ = tf.split(y_true, 3, axis=-1)
        y_pred_prob, y_pred, _  = tf.split(y_pred, 3, axis=-1)

        mask = tf.math.logical_and(
            tf.math.equal(y_true_prob, 1.), tf.math.greater(y_pred_prob, 0.5))

        y_true = tf.boolean_mask(y_true, mask)
        y_pred = tf.boolean_mask(y_pred, mask)

        error = tf.math.abs(y_true - y_pred)

        super().update_state(error, sample_weight=sample_weight)


def get_accuracy_metrics_at_thresholds(
    thresholds=[
        # has been narrowed down from [0.05, 0.95]
        0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55
    ]
):
    return [
        CustomAccuracy(name='acc_' + str(t).split('.')[-1], threshold=t)
        for t in thresholds
    ]

# =========================
# Charger et préparer les données
# =========================

# Charger les chromatogrammes
file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

# Filtrer les chromatogrammes
df = df.dropna(subset=['pics'])
df = df[df["x"].apply(len) == 71840]

# Troncature des chromatogrammes
def truncate(row):
    mask = np.array(row["x"]) <= 150
    row["x"] = np.array(row["x"])[mask].tolist()
    row["y"] = np.array(row["y"])[mask].tolist()
    return row

df = df.apply(truncate, axis=1)

sequence_length = df.iloc[0]["x"].__len__()

# Préparer les données d'entrée X et de sortie Y
X = np.array([np.column_stack((row["x"], row["y"])) for _, row in df.iterrows()])
X[:, :, 0] = X[:, :, 0] / 150.0  # Normalisation du temps
for i in range(X.shape[0]):
    max_intensity = np.max(X[i, :, 1])
    if max_intensity > 0:
        X[i, :, 1] /= max_intensity

Y = np.zeros((len(df), sequence_length, 3), dtype=np.float32)
for i, (_, row) in enumerate(df.iterrows()):
    x_time = np.array(row["x"])
    for pic_time, data in row["pics"].items():
        borne_avant_time = data[1]
        pic_time = float(pic_time)
        borne_apres_time = data[2]
        if pic_time > 150:
            continue
        borne_avant_idx = np.argmin(np.abs(x_time - borne_avant_time))
        pic_idx = np.argmin(np.abs(x_time - pic_time))
        borne_apres_idx = np.argmin(np.abs(x_time - borne_apres_time))

        Y[i, pic_idx, 0] = 1
        Y[i, pic_idx, 1] = pic_time / 150.0
        time_diff = borne_apres_time - borne_avant_time
        Y[i, pic_idx, 2] = time_diff

# Séparer les données en ensembles d'entraînement et de test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# =========================
# Entraînement du modèle
# =========================

model = IPA(seed_value=1, regularization_factor=0.0095, dropout_rate=0.2)

# Compiler le modèle avec la fonction de perte et les métriques personnalisées
optimizer = optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, 
              loss=CustomLoss(n_splits=3),
              metrics=[CustomAccuracy(), CustomAUC(), CustomMREArea(), CustomMAELoc()])

# Callback pour l'arrêt précoce
stop_early = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)

# Entraînement du modèle
history = model.fit(
    X_train, Y_train,
    epochs=500,
    batch_size=16,
    validation_data=(X_test, Y_test),
    callbacks=[stop_early, TqdmCallback(verbose=1)],
    verbose=0
)


    def call(self, x):
        out = self.stem(x)
        out = self.module_35x35(out)
        out = self.lstm(out)
        out = self.dropout(out)
        out = self.classifier(out)  # (batch, seq_len, 3)

        # Séparer la sortie en 3 parties
        pred, loc, area = tf.split(out, 3, axis=-1)

        # Appliquer sigmoid sur pred et loc
        pred = tf.nn.sigmoid(pred)
        loc = tf.nn.sigmoid(loc)

        # Concatenate les résultats
        out = tf.concat([pred, loc, area], axis=-1)
        return out


import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Charger les chromatogrammes
file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

# Filtrer les chromatogrammes
df = df.dropna(subset=['pics'])
df = df[df["x"].apply(len) == 71840]

# Troncature des chromatogrammes
def truncate(row):
    mask = np.array(row["x"]) <= 150
    row["x"] = np.array(row["x"])[mask].tolist()
    row["y"] = np.array(row["y"])[mask].tolist()
    return row

df = df.apply(truncate, axis=1)

sequence_length = 512  # Nouvelle longueur des segments

# Découper les données en segments de 512 points
def segment_data(x, y, pics, segment_size=512):
    segments_X, segments_Y = [], []
    num_segments = len(x) // segment_size  # Nombre de segments complets

    for i in range(num_segments):
        start_idx = i * segment_size
        end_idx = start_idx + segment_size

        segment_x = np.column_stack((x[start_idx:end_idx], y[start_idx:end_idx]))
        segment_y = np.zeros((segment_size, 3), dtype=np.float32)

        x_time = np.array(x[start_idx:end_idx])
        
        for pic_time, data in pics.items():
            pic_time = float(pic_time)
            borne_avant_time, borne_apres_time = data[1], data[2]

            if start_idx <= np.argmin(np.abs(np.array(x) - pic_time)) < end_idx:
                pic_idx = np.argmin(np.abs(x_time - pic_time))
                borne_avant_idx = np.argmin(np.abs(x_time - borne_avant_time))
                borne_apres_idx = np.argmin(np.abs(x_time - borne_apres_time))

                segment_y[pic_idx, 0] = 1  # Marquer la présence d'un pic
                segment_y[pic_idx, 1] = pic_time / 150.0  # Normalisation
                segment_y[pic_idx, 2] = borne_apres_time - borne_avant_time  # Largeur du pic
            
        segments_X.append(segment_x)
        segments_Y.append(segment_y)

    return segments_X, segments_Y

X, Y = [], []

for _, row in df.iterrows():
    segments_X, segments_Y = segment_data(np.array(row["x"]), np.array(row["y"]), row["pics"], sequence_length)
    X.extend(segments_X)
    Y.extend(segments_Y)

X = np.array(X)
Y = np.array(Y)

# Normalisation des intensités
for i in range(X.shape[0]):
    max_intensity = np.max(X[i, :, 1])
    if max_intensity > 0:
        X[i, :, 1] /= max_intensity

# Séparer les données en train/test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)


import matplotlib.pyplot as plt

def plot_segment_with_peak(segment_x, segment_y):
    # Extraire les données X et Y pour le segment
    x_data = segment_x[:, 0]
    y_data = segment_x[:, 1]

    # Créer la figure
    plt.figure(figsize=(10, 6))
    
    # Tracer le chromatogramme
    plt.plot(x_data, y_data, label="Chromatogramme", color='blue')
    
    # Ajouter les pics
    for idx, peak_info in enumerate(segment_y):
        if peak_info[0] == 1:  # Si un pic est présent
            pic_time = peak_info[1] * 150  # Dénomination normalisée à l'échelle originale
            plt.scatter(pic_time, y_data[np.argmin(np.abs(x_data - pic_time))], color='red', label='Pic')

            # Afficher la largeur du pic entre les bornes
            borne_avant = peak_info[1] * 150 - peak_info[2] / 2
            borne_apres = peak_info[1] * 150 + peak_info[2] / 2
            plt.axvline(borne_avant, color='green', linestyle='--', label='Borne avant')
            plt.axvline(borne_apres, color='green', linestyle='--', label='Borne après')

    # Ajouter des labels et titre
    plt.title("Segment de chromatogramme avec pic marqué")
    plt.xlabel("Temps (unités normalisées)")
    plt.ylabel("Intensité")
    plt.legend()

    # Afficher le graphique
    plt.show()

# Exemple d'utilisation pour un segment
plot_segment_with_peak(X[0], Y[0])


import tensorflow as tf

class CustomAUC(tf.keras.metrics.AUC):

    def update_state(self, y_true, y_pred, sample_weight=None):
        # Séparer les classes de y_true et y_pred (on se concentre sur la classe des pics)
        y_true = tf.split(y_true, 3, axis=-1)[0]  # Classe 0 (absence de pic)
        y_pred = tf.split(y_pred, 3, axis=-1)[0]

        # Si y_true est de la classe 1 (pic), augmenter le poids
        if sample_weight is None:
            sample_weight = tf.cast(tf.equal(y_true, 1), tf.float32)
        
        # Optionnel : vous pouvez multiplier le poids par un facteur pour renforcer l'importance des pics
        # Par exemple, si on veut multiplier le poids des pics par 10
        sample_weight = sample_weight * 10.0  # Ajustez ce facteur selon vos besoins

        # Calculer l'AUC avec les poids ajustés
        super().update_state(y_true, y_pred, sample_weight)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Charger les chromatogrammes
file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

# Filtrer les chromatogrammes
df = df.dropna(subset=['pics'])
df = df[df["x"].apply(len) == 71840]

# Troncature des chromatogrammes
def truncate(row):
    mask = np.array(row["x"]) <= 150
    row["x"] = np.array(row["x"])[mask].tolist()
    row["y"] = np.array(row["y"])[mask].tolist()
    return row

df = df.apply(truncate, axis=1)

sequence_length = 512  # Taille du segment
stride = 256  # Chevauchement

# Fonction de segmentation avec padding du dernier segment
def segment_data(x, y, pics, segment_size=512, stride=256):
    segments_X, segments_Y = [], []
    num_segments = (len(x) - segment_size) // stride + 1  # Nombre de segments

    for i in range(num_segments):
        start_idx = i * stride
        end_idx = start_idx + segment_size

        segment_x = np.column_stack((x[start_idx:end_idx], y[start_idx:end_idx]))
        segment_y = np.zeros((segment_size, 3), dtype=np.float32)

        x_time = np.array(x[start_idx:end_idx])
        
        for pic_time, data in pics.items():
            pic_time = float(pic_time)
            borne_avant_time, borne_apres_time = data[1], data[2]

            if start_idx <= np.argmin(np.abs(np.array(x) - pic_time)) < end_idx:
                pic_idx = np.argmin(np.abs(x_time - pic_time))
                borne_avant_idx = np.argmin(np.abs(x_time - borne_avant_time))
                borne_apres_idx = np.argmin(np.abs(x_time - borne_apres_time))

                segment_y[pic_idx, 0] = 1  # Marquer la présence d'un pic
                segment_y[pic_idx, 1] = pic_time / 150.0  # Normalisation
                segment_y[pic_idx, 2] = borne_apres_time - borne_avant_time  # Largeur du pic
            
        segments_X.append(segment_x)
        segments_Y.append(segment_y)

    # Gestion du dernier segment (padding si nécessaire)
    if (len(x) - segment_size) % stride != 0:
        start_idx = len(x) - segment_size
        segment_x = np.column_stack((x[start_idx:], y[start_idx:]))

        # Compléter avec des zéros si la taille est inférieure
        padding_needed = segment_size - len(segment_x)
        if padding_needed > 0:
            pad_x = np.zeros((padding_needed, 2))
            pad_y = np.zeros((padding_needed, 3))
            segment_x = np.vstack((segment_x, pad_x))
            segment_y = np.vstack((segment_y, pad_y))

        segments_X.append(segment_x)
        segments_Y.append(segment_y)

    return segments_X, segments_Y

X, Y = [], []

for _, row in df.iterrows():
    segments_X, segments_Y = segment_data(np.array(row["x"]), np.array(row["y"]), row["pics"], sequence_length, stride)
    X.extend(segments_X)
    Y.extend(segments_Y)

X = np.array(X)
Y = np.array(Y)

# Normalisation des intensités (sur l'ensemble du chromatogramme)
for i in range(len(df)):
    max_intensity = np.max(df.iloc[i]["y"])
    if max_intensity > 0:
        df.iloc[i]["y"] = (np.array(df.iloc[i]["y"]) / max_intensity).tolist()

# Séparer les données en train/test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
