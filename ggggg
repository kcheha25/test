import tensorflow as tf
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import EarlyStopping
from tqdm.keras import TqdmCallback

# =========================
# Définir la fonction de perte personnalisée
# =========================

import tensorflow as tf


class CustomLoss(tf.keras.losses.Loss):

    def __init__(
        self,
        n_splits,
        weight_prob=1.0,
        weight_loc=1.0,
        weight_area=1.0,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.n_splits = n_splits
        self.weight_prob = weight_prob
        self.weight_loc = weight_loc
        self.weight_area = weight_area

    def call(self, y_true, y_pred, sample_weight=None):

        # Cast to dtype float32 just in case
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)

        # Unpack trues and preds
        pred_prob, pred_loc, pred_area = tf.split(y_pred, self.n_splits, axis=-1)
        true_prob, true_loc, true_area = tf.split(y_true, self.n_splits, axis=-1)

        # Mask for where y_true is indicating a peak
        mask = tf.math.equal(true_prob, 1.)

        # Compute loss for all instances
        prob_loss = tf.keras.losses.BinaryCrossentropy()(true_prob, pred_prob)
        # Compute loss only for instances in mask
        loc_loss = tf.keras.losses.BinaryCrossentropy()(
            tf.boolean_mask(true_loc, mask), tf.boolean_mask(pred_loc, mask))
        area_loss = MeanRelativeError()(
            tf.boolean_mask(true_area, mask), tf.boolean_mask(pred_area, mask))

        return (
            prob_loss * self.weight_prob +
            loc_loss * self.weight_loc +
            area_loss * self.weight_area
        )

class MeanRelativeError(tf.keras.losses.Loss):

    def call(self, y_true, y_pred, sample_weight=None):
        return tf.math.abs(y_true - y_pred) / y_true


# =========================
# Définir les métriques personnalisées
# =========================

import tensorflow as tf



class CustomTruePositiveRate(tf.keras.metrics.Metric):

    def __init__(self, threshold, **kwargs):
        super().__init__(**kwargs)
        self.threshold = threshold
        self.true_positives = self.add_weight(name='tp', initializer='zeros')
        self.false_negatives = self.add_weight(name='tp', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):

        y_true = tf.split(y_true, 3, axis=-1)[0]
        y_pred = tf.split(y_pred, 3, axis=-1)[0]

        y_pred = y_pred > self.threshold
        y_true = tf.cast(y_true, tf.bool)

        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))
        values = tf.cast(values, dtype=self.dtype)
        self.true_positives.assign_add(tf.reduce_sum(values))

        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, False))
        values = tf.cast(values, dtype=self.dtype)
        self.false_negatives.assign_add(tf.reduce_sum(values))

    def result(self):
        return self.true_positives / (self.true_positives + self.false_negatives)

    def reset_states(self):
        self.true_positives.assign(0)
        self.false_negatives.assign(0)


class CustomFalsePositiveRate(tf.keras.metrics.Metric):

    def __init__(self, threshold, **kwargs):
        super().__init__(**kwargs)
        self.threshold = threshold
        self.false_positives = self.add_weight(name='tp', initializer='zeros')
        self.true_negatives = self.add_weight(name='tp', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):

        y_true = tf.split(y_true, 3, axis=-1)[0]
        y_pred = tf.split(y_pred, 3, axis=-1)[0]

        y_pred = y_pred > self.threshold
        y_true = tf.cast(y_true, tf.bool)

        values = tf.logical_and(tf.equal(y_true, False), tf.equal(y_pred, True))
        values = tf.cast(values, dtype=self.dtype)
        self.false_positives.assign_add(tf.reduce_sum(values))

        values = tf.logical_and(tf.equal(y_true, False), tf.equal(y_pred, False))
        values = tf.cast(values, dtype=self.dtype)
        self.true_negatives.assign_add(tf.reduce_sum(values))

    def result(self):
        return self.false_positives / (self.false_positives + self.true_negatives)

    def reset_states(self):
        self.false_positives.assign(0)
        self.true_negatives.assign(0)


class CustomAUC(tf.keras.metrics.AUC):

    def update_state(self, y_true, y_pred, sample_weight=None):

        y_true = tf.split(y_true, 3, axis=-1)[0]
        y_pred = tf.split(y_pred, 3, axis=-1)[0]

        super().update_state(y_true, y_pred, sample_weight)


class CustomAccuracy(tf.keras.metrics.BinaryAccuracy):

    def update_state(self, y_true, y_pred, sample_weight=None):

        y_true = tf.split(y_true, 3, axis=-1)[0]
        y_pred = tf.split(y_pred, 3, axis=-1)[0]

        super().update_state(y_true, y_pred, sample_weight)


class CustomMREArea(tf.keras.metrics.Mean):

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true_prob, _, y_true = tf.split(y_true, 3, axis=-1)
        y_pred_prob, _, y_pred = tf.split(y_pred, 3, axis=-1)

        mask = tf.math.logical_and(
            tf.math.equal(y_true_prob, 1.), tf.math.greater(y_pred_prob, 0.5))

        y_true = tf.boolean_mask(y_true, mask)
        y_pred = tf.boolean_mask(y_pred, mask)

        error = tf.math.divide_no_nan(
            tf.math.abs(y_true - y_pred),
            tf.math.abs(y_true)
        )
        super().update_state(error, sample_weight=sample_weight)


class CustomMRELoc(tf.keras.metrics.Mean):

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true_prob, y_true, _ = tf.split(y_true, 3, axis=-1)
        y_pred_prob, y_pred, _  = tf.split(y_pred, 3, axis=-1)

        mask = tf.math.logical_and(
            tf.math.equal(y_true_prob, 1.), tf.math.greater(y_pred_prob, 0.5))

        y_true = tf.boolean_mask(y_true, mask)
        y_pred = tf.boolean_mask(y_pred, mask)

        error = tf.math.divide_no_nan(
            tf.math.abs(y_true - y_pred),
            tf.math.abs(y_true)
        )
        super().update_state(error, sample_weight=sample_weight)


class CustomMAELoc(tf.keras.metrics.Mean):

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true_prob, y_true, _ = tf.split(y_true, 3, axis=-1)
        y_pred_prob, y_pred, _  = tf.split(y_pred, 3, axis=-1)

        mask = tf.math.logical_and(
            tf.math.equal(y_true_prob, 1.), tf.math.greater(y_pred_prob, 0.5))

        y_true = tf.boolean_mask(y_true, mask)
        y_pred = tf.boolean_mask(y_pred, mask)

        error = tf.math.abs(y_true - y_pred)

        super().update_state(error, sample_weight=sample_weight)


def get_accuracy_metrics_at_thresholds(
    thresholds=[
        # has been narrowed down from [0.05, 0.95]
        0.45, 0.46, 0.47, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55
    ]
):
    return [
        CustomAccuracy(name='acc_' + str(t).split('.')[-1], threshold=t)
        for t in thresholds
    ]

# =========================
# Charger et préparer les données
# =========================

# Charger les chromatogrammes
file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

# Filtrer les chromatogrammes
df = df.dropna(subset=['pics'])
df = df[df["x"].apply(len) == 71840]

# Troncature des chromatogrammes
def truncate(row):
    mask = np.array(row["x"]) <= 150
    row["x"] = np.array(row["x"])[mask].tolist()
    row["y"] = np.array(row["y"])[mask].tolist()
    return row

df = df.apply(truncate, axis=1)

sequence_length = df.iloc[0]["x"].__len__()

# Préparer les données d'entrée X et de sortie Y
X = np.array([np.column_stack((row["x"], row["y"])) for _, row in df.iterrows()])
X[:, :, 0] = X[:, :, 0] / 150.0  # Normalisation du temps
for i in range(X.shape[0]):
    max_intensity = np.max(X[i, :, 1])
    if max_intensity > 0:
        X[i, :, 1] /= max_intensity

Y = np.zeros((len(df), sequence_length, 3), dtype=np.float32)
for i, (_, row) in enumerate(df.iterrows()):
    x_time = np.array(row["x"])
    for pic_time, data in row["pics"].items():
        borne_avant_time = data[1]
        pic_time = float(pic_time)
        borne_apres_time = data[2]
        if pic_time > 150:
            continue
        borne_avant_idx = np.argmin(np.abs(x_time - borne_avant_time))
        pic_idx = np.argmin(np.abs(x_time - pic_time))
        borne_apres_idx = np.argmin(np.abs(x_time - borne_apres_time))

        Y[i, pic_idx, 0] = 1
        Y[i, pic_idx, 1] = pic_time / 150.0
        time_diff = borne_apres_time - borne_avant_time
        Y[i, pic_idx, 2] = time_diff

# Séparer les données en ensembles d'entraînement et de test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# =========================
# Entraînement du modèle
# =========================

model = IPA(seed_value=1, regularization_factor=0.0095, dropout_rate=0.2)

# Compiler le modèle avec la fonction de perte et les métriques personnalisées
optimizer = optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, 
              loss=CustomLoss(n_splits=3),
              metrics=[CustomAccuracy(), CustomAUC(), CustomMREArea(), CustomMAELoc()])

# Callback pour l'arrêt précoce
stop_early = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)

# Entraînement du modèle
history = model.fit(
    X_train, Y_train,
    epochs=500,
    batch_size=16,
    validation_data=(X_test, Y_test),
    callbacks=[stop_early, TqdmCallback(verbose=1)],
    verbose=0
)


    def call(self, x):
        out = self.stem(x)
        out = self.module_35x35(out)
        out = self.lstm(out)
        out = self.dropout(out)
        out = self.classifier(out)  # (batch, seq_len, 3)

        # Séparer la sortie en 3 parties
        pred, loc, area = tf.split(out, 3, axis=-1)

        # Appliquer sigmoid sur pred et loc
        pred = tf.nn.sigmoid(pred)
        loc = tf.nn.sigmoid(loc)

        # Concatenate les résultats
        out = tf.concat([pred, loc, area], axis=-1)
        return out


import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Charger les chromatogrammes
file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

# Filtrer les chromatogrammes
df = df.dropna(subset=['pics'])
df = df[df["x"].apply(len) == 71840]

# Troncature des chromatogrammes
def truncate(row):
    mask = np.array(row["x"]) <= 150
    row["x"] = np.array(row["x"])[mask].tolist()
    row["y"] = np.array(row["y"])[mask].tolist()
    return row

df = df.apply(truncate, axis=1)

sequence_length = 512  # Nouvelle longueur des segments

# Découper les données en segments de 512 points
def segment_data(x, y, pics, segment_size=512):
    segments_X, segments_Y = [], []
    num_segments = len(x) // segment_size  # Nombre de segments complets

    for i in range(num_segments):
        start_idx = i * segment_size
        end_idx = start_idx + segment_size

        segment_x = np.column_stack((x[start_idx:end_idx], y[start_idx:end_idx]))
        segment_y = np.zeros((segment_size, 3), dtype=np.float32)

        x_time = np.array(x[start_idx:end_idx])
        
        for pic_time, data in pics.items():
            pic_time = float(pic_time)
            borne_avant_time, borne_apres_time = data[1], data[2]

            if start_idx <= np.argmin(np.abs(np.array(x) - pic_time)) < end_idx:
                pic_idx = np.argmin(np.abs(x_time - pic_time))
                borne_avant_idx = np.argmin(np.abs(x_time - borne_avant_time))
                borne_apres_idx = np.argmin(np.abs(x_time - borne_apres_time))

                segment_y[pic_idx, 0] = 1  # Marquer la présence d'un pic
                segment_y[pic_idx, 1] = pic_time / 150.0  # Normalisation
                segment_y[pic_idx, 2] = borne_apres_time - borne_avant_time  # Largeur du pic
            
        segments_X.append(segment_x)
        segments_Y.append(segment_y)

    return segments_X, segments_Y

X, Y = [], []

for _, row in df.iterrows():
    segments_X, segments_Y = segment_data(np.array(row["x"]), np.array(row["y"]), row["pics"], sequence_length)
    X.extend(segments_X)
    Y.extend(segments_Y)

X = np.array(X)
Y = np.array(Y)

# Normalisation des intensités
for i in range(X.shape[0]):
    max_intensity = np.max(X[i, :, 1])
    if max_intensity > 0:
        X[i, :, 1] /= max_intensity

# Séparer les données en train/test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)


import matplotlib.pyplot as plt

def plot_segment_with_peak(segment_x, segment_y):
    # Extraire les données X et Y pour le segment
    x_data = segment_x[:, 0]
    y_data = segment_x[:, 1]

    # Créer la figure
    plt.figure(figsize=(10, 6))
    
    # Tracer le chromatogramme
    plt.plot(x_data, y_data, label="Chromatogramme", color='blue')
    
    # Ajouter les pics
    for idx, peak_info in enumerate(segment_y):
        if peak_info[0] == 1:  # Si un pic est présent
            pic_time = peak_info[1] * 150  # Dénomination normalisée à l'échelle originale
            plt.scatter(pic_time, y_data[np.argmin(np.abs(x_data - pic_time))], color='red', label='Pic')

            # Afficher la largeur du pic entre les bornes
            borne_avant = peak_info[1] * 150 - peak_info[2] / 2
            borne_apres = peak_info[1] * 150 + peak_info[2] / 2
            plt.axvline(borne_avant, color='green', linestyle='--', label='Borne avant')
            plt.axvline(borne_apres, color='green', linestyle='--', label='Borne après')

    # Ajouter des labels et titre
    plt.title("Segment de chromatogramme avec pic marqué")
    plt.xlabel("Temps (unités normalisées)")
    plt.ylabel("Intensité")
    plt.legend()

    # Afficher le graphique
    plt.show()

# Exemple d'utilisation pour un segment
plot_segment_with_peak(X[0], Y[0])


import tensorflow as tf

class CustomAUC(tf.keras.metrics.AUC):

    def update_state(self, y_true, y_pred, sample_weight=None):
        # Séparer les classes de y_true et y_pred (on se concentre sur la classe des pics)
        y_true = tf.split(y_true, 3, axis=-1)[0]  # Classe 0 (absence de pic)
        y_pred = tf.split(y_pred, 3, axis=-1)[0]

        # Si y_true est de la classe 1 (pic), augmenter le poids
        if sample_weight is None:
            sample_weight = tf.cast(tf.equal(y_true, 1), tf.float32)
        
        # Optionnel : vous pouvez multiplier le poids par un facteur pour renforcer l'importance des pics
        # Par exemple, si on veut multiplier le poids des pics par 10
        sample_weight = sample_weight * 10.0  # Ajustez ce facteur selon vos besoins

        # Calculer l'AUC avec les poids ajustés
        super().update_state(y_true, y_pred, sample_weight)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Charger les chromatogrammes
file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

# Filtrer les chromatogrammes
df = df.dropna(subset=['pics'])
df = df[df["x"].apply(len) == 71840]

# Troncature des chromatogrammes
def truncate(row):
    mask = np.array(row["x"]) <= 150
    row["x"] = np.array(row["x"])[mask].tolist()
    row["y"] = np.array(row["y"])[mask].tolist()
    return row

df = df.apply(truncate, axis=1)

sequence_length = 512  # Taille du segment
stride = 256  # Chevauchement

# Fonction de segmentation avec padding du dernier segment
def segment_data(x, y, pics, segment_size=512, stride=256):
    segments_X, segments_Y = [], []
    num_segments = (len(x) - segment_size) // stride + 1  # Nombre de segments

    for i in range(num_segments):
        start_idx = i * stride
        end_idx = start_idx + segment_size

        segment_x = np.column_stack((x[start_idx:end_idx], y[start_idx:end_idx]))
        segment_y = np.zeros((segment_size, 3), dtype=np.float32)

        x_time = np.array(x[start_idx:end_idx])
        
        for pic_time, data in pics.items():
            pic_time = float(pic_time)
            borne_avant_time, borne_apres_time = data[1], data[2]

            if start_idx <= np.argmin(np.abs(np.array(x) - pic_time)) < end_idx:
                pic_idx = np.argmin(np.abs(x_time - pic_time))
                borne_avant_idx = np.argmin(np.abs(x_time - borne_avant_time))
                borne_apres_idx = np.argmin(np.abs(x_time - borne_apres_time))

                segment_y[pic_idx, 0] = 1  # Marquer la présence d'un pic
                segment_y[pic_idx, 1] = pic_time / 150.0  # Normalisation
                segment_y[pic_idx, 2] = borne_apres_time - borne_avant_time  # Largeur du pic
            
        segments_X.append(segment_x)
        segments_Y.append(segment_y)

    # Gestion du dernier segment (padding si nécessaire)
    if (len(x) - segment_size) % stride != 0:
        start_idx = len(x) - segment_size
        segment_x = np.column_stack((x[start_idx:], y[start_idx:]))

        # Compléter avec des zéros si la taille est inférieure
        padding_needed = segment_size - len(segment_x)
        if padding_needed > 0:
            pad_x = np.zeros((padding_needed, 2))
            pad_y = np.zeros((padding_needed, 3))
            segment_x = np.vstack((segment_x, pad_x))
            segment_y = np.vstack((segment_y, pad_y))

        segments_X.append(segment_x)
        segments_Y.append(segment_y)

    return segments_X, segments_Y

X, Y = [], []

for _, row in df.iterrows():
    segments_X, segments_Y = segment_data(np.array(row["x"]), np.array(row["y"]), row["pics"], sequence_length, stride)
    X.extend(segments_X)
    Y.extend(segments_Y)

X = np.array(X)
Y = np.array(Y)

# Normalisation des intensités (sur l'ensemble du chromatogramme)
for i in range(len(df)):
    max_intensity = np.max(df.iloc[i]["y"])
    if max_intensity > 0:
        df.iloc[i]["y"] = (np.array(df.iloc[i]["y"]) / max_intensity).tolist()

# Séparer les données en train/test
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)


import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense, Softmax

class SpatioTemporalAttention(Layer):
    def __init__(self, units):
        super(SpatioTemporalAttention, self).__init__()
        self.query_dense = Dense(units)  # Transformation de la requête
        self.key_dense = Dense(units)    # Transformation de la clé
        self.value_dense = Dense(units)  # Transformation de la valeur
        self.softmax = Softmax(axis=1)   # Softmax pour pondérer

    def call(self, inputs, mask=None):
        """
        inputs : (batch, seq_length, features)
        On veut calculer l'attention sur les régions où il y a des pics.
        """

        # Transformer les entrées
        query = self.query_dense(inputs)  
        key = self.key_dense(inputs)  
        value = self.value_dense(inputs)  

        # Calcul du score d'attention
        scores = tf.matmul(query, key, transpose_b=True)  # (batch, seq_length, seq_length)
        scores = scores / tf.sqrt(tf.cast(tf.shape(key)[-1], tf.float32))  # Normalisation

        # Application du softmax pour obtenir les poids d'attention
        attention_weights = self.softmax(scores)  # (batch, seq_length, seq_length)

        # Appliquer l'attention sur les valeurs
        weighted_output = tf.matmul(attention_weights, value)  # (batch, seq_length, features)

        return weighted_output, attention_weights  # On retourne aussi les poids pour analyse
class IPA(tf.keras.Model):
    def __init__(self, seed_value, regularization_factor, dropout_rate=0.2, lstm_units=128):
        super(IPA, self).__init__()

        self.stem = tf.keras.Sequential([
            BasicConv1D(filters=64, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=64, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.module_35x35 = Module_35x35(in_channels=64, regularization_factor=regularization_factor, seed_value=seed_value)

        # Deux LSTM + Deux Attention Spatio-Temporelle
        self.lstm1 = Bidirectional(LSTM(lstm_units, return_sequences=True))
        self.attention1 = SpatioTemporalAttention(units=lstm_units)

        self.lstm2 = Bidirectional(LSTM(lstm_units, return_sequences=True))
        self.attention2 = SpatioTemporalAttention(units=lstm_units)

        self.dropout = Dropout(rate=dropout_rate)
        self.classifier = Dense(3)

    def call(self, x):
        out = self.stem(x)
        out = self.module_35x35(out)

        out = self.lstm1(out)
        out, _ = self.attention1(out)  

        out = self.lstm2(out)
        out, _ = self.attention2(out)  

        out = self.dropout(out)
        out = self.classifier(out)

        pred, loc, area = tf.split(out, 3, axis=-1)
        pred = tf.nn.sigmoid(pred)
        loc = tf.nn.sigmoid(loc)
        out = tf.concat([pred, loc, area], axis=-1)

        return out

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv1D, LSTM, Dense, Dropout, LayerNormalization, Add
from tensorflow.keras.models import Model
import math

# Définition de la Locality Self-Attention (LSA)
class MultiHeadAttention_LSA(tf.keras.layers.MultiHeadAttention):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)

    def _compute_attention(self, query, key, value, attention_mask=None, training=None):
        query = tf.multiply(query, 1.0 / self.tau)
        attention_scores = tf.einsum(self._dot_product_equation, key, query)
        attention_scores = self._masked_softmax(attention_scores, attention_mask)
        attention_scores_dropout = self._dropout_layer(attention_scores, training=training)
        attention_output = tf.einsum(self._combine_equation, attention_scores_dropout, value)
        return attention_output, attention_scores

# Fonction pour ajouter un bloc MHA+LSA
def mha_lsa_block(x, key_dim=8, num_heads=2, dropout=0.5):
    x_norm = LayerNormalization(epsilon=1e-6)(x)
    num_patches = x.shape[1]

    # Masque diagonal pour Locality Self-Attention
    diag_attn_mask = 1 - tf.eye(num_patches)
    diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)

    # Application de l'attention
    x_attn = MultiHeadAttention_LSA(key_dim=key_dim, num_heads=num_heads, dropout=dropout)(
        x_norm, x_norm, attention_mask=diag_attn_mask
    )

    x_drop = Dropout(0.3)(x_attn)
    x_out = Add()([x, x_drop])  # Skip connection
    return x_out

# Définition du modèle IPA avec CNN+LSTM + MHA+LSA
def build_model(input_shape=(1000, 2)):
    inputs = Input(shape=input_shape)

    # Extraction de caractéristiques avec CNN
    x = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(inputs)
    x = Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')(x)
    x = Dropout(0.3)(x)

    # Passage par LSTM pour capturer les dépendances temporelles
    x = LSTM(128, return_sequences=True)(x)
    x = LSTM(64, return_sequences=True)(x)

    # Application de l'attention LSA
    x = mha_lsa_block(x, key_dim=8, num_heads=4, dropout=0.5)

    # Prédictions finales
    outputs = Dense(3, activation='sigmoid')(x)  # 3 sorties : probabilité du pic, localisation, largeur

    model = Model(inputs, outputs)
    return model

# Instanciation du modèle
model = build_model()
model.summary()
import tensorflow as tf
import math
from tensorflow.keras.layers import Layer, Dense, Dropout, LSTM, Bidirectional, LayerNormalization, Add
from tensorflow.keras.regularizers import L2
from tensorflow.keras.initializers import HeNormal

# Assurez-vous d'avoir ces modules définis ailleurs
# from your_module import BasicConv1D, Module_35x35  

# Multi-Head Self Attention avec Locality Self Attention (LSA)
class MultiHeadAttention_LSA(tf.keras.layers.MultiHeadAttention):
    """Local Multi-Head Self Attention (LSA) basé sur https://arxiv.org/abs/2112.13492v1"""
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # Paramètre d'ajustement de la température pour améliorer la localité
        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)

    def _compute_attention(self, query, key, value, attention_mask=None, training=None):
        query = tf.multiply(query, 1.0 / self.tau)  # Ajustement de la température
        attention_scores = tf.einsum(self._dot_product_equation, key, query)
        attention_scores = self._masked_softmax(attention_scores, attention_mask)
        attention_scores_dropout = self._dropout_layer(attention_scores, training=training)
        attention_output = tf.einsum(self._combine_equation, attention_scores_dropout, value)
        return attention_output, attention_scores

# Bloc Multi-Head Self Attention avec Locality Self Attention
def mha_lsa_block(input_feature, key_dim=8, num_heads=2, dropout=0.3, vanilla=False):
    """Bloc Multi-Head Self Attention avec LSA ou MHA standard"""
    x = LayerNormalization(epsilon=1e-6)(input_feature)

    if vanilla:
        # MHA classique (Attention is All You Need)
        x = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, dropout=dropout)(x, x)
    else:
        # LSA (Vision Transformer for Small Datasets)
        num_patches = tf.shape(input_feature)[1]
        diag_attn_mask = 1 - tf.eye(num_patches)
        diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)

        x = MultiHeadAttention_LSA(num_heads=num_heads, key_dim=key_dim, dropout=dropout)(x, x, attention_mask=diag_attn_mask)

    x = Dropout(dropout)(x)
    return Add()([input_feature, x])  # Skip connection

# Modèle IPA avec MHA-LSA
class IPA(tf.keras.Model):
    def __init__(self, seed_value, regularization_factor, dropout_rate=0.2, lstm_units=128, num_heads=8, key_dim=64):
        super(IPA, self).__init__()

        self.stem = tf.keras.Sequential([
            BasicConv1D(filters=64, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=HeNormal(seed_value)),
            BasicConv1D(filters=64, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=HeNormal(seed_value)),
        ])
        self.module_35x35 = Module_35x35(in_channels=64, regularization_factor=regularization_factor, seed_value=seed_value)

        # Deux LSTM + Deux Multi-Head Self Attention LSA
        self.lstm1 = Bidirectional(LSTM(lstm_units, return_sequences=True))
        self.attention1 = mha_lsa_block

        self.lstm2 = Bidirectional(LSTM(lstm_units, return_sequences=True))
        self.attention2 = mha_lsa_block

        self.dropout = Dropout(rate=dropout_rate)
        self.classifier = Dense(3)

    def call(self, x):
        out = self.stem(x)
        out = self.module_35x35(out)

        out = self.lstm1(out)
        out = self.attention1(out, key_dim=64, num_heads=8, dropout=0.3, vanilla=False)

        out = self.lstm2(out)
        out = self.attention2(out, key_dim=64, num_heads=8, dropout=0.3, vanilla=False)

        out = self.dropout(out)
        out = self.classifier(out)

        # Découpage des sorties
        pred, loc, area = tf.split(out, 3, axis=-1)
        pred = tf.nn.sigmoid(pred)
        loc = tf.nn.sigmoid(loc)
        area = tf.nn.relu(area)  # S'assure que l'aire est positive

        return tf.concat([pred, loc, area], axis=-1)

# Instanciation du modèle (exemple)
seed_value = 42
regularization_factor = 0.0001
model = IPA(seed_value, regularization_factor)

import os
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras import optimizers

if not os.path.exists(os.path.join(path_model, MODEL_NAME)):
    print('Model does not exist, training in progress...')

    model = IPA(seed_value=SEED_VALUE,
                regularization_factor=REGULARIZATION_COEF,
                dropout_rate=DROPOUT_RATE)

    # Schedule du learning rate
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=LEARNING_RATE,
        decay_steps=10000, decay_rate=0.001
    )

    optimizer = optimizers.Adam(learning_rate=lr_schedule)
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    # Callbacks
    stop_early = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)

    checkpoint_path = os.path.join(path_model, MODEL_NAME, "best_weights.tf")
    checkpoint_callback = ModelCheckpoint(
        filepath=checkpoint_path,
        monitor='val_loss',
        save_best_only=True,  # Sauvegarde uniquement les meilleurs poids
        save_weights_only=True,  # Ne sauvegarde que les poids, pas l'architecture
        mode='min',
        verbose=1
    )

    history = model.fit(
        X_train, Y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_test, Y_test),
        callbacks=[stop_early, checkpoint_callback, TqdmCallback(verbose=1)],
        verbose=0
    )

    print('Saving model architecture...')
    os.makedirs(os.path.join(path_model, MODEL_NAME), exist_ok=True)
    model.save(os.path.join(path_model, MODEL_NAME), overwrite=True, save_format='tf')

else:
    print('Model does exist, loading...')
    model = tf.keras.models.load_model(os.path.join(path_model, MODEL_NAME))

    # Charger les meilleurs poids s'ils existent
    checkpoint_path = os.path.join(path_model, MODEL_NAME, "best_weights.tf")
    if os.path.exists(checkpoint_path + ".index"):  # Vérifie si les poids existent
        print('Loading best weights...')
        model.load_weights(checkpoint_path)
