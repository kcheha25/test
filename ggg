import tensorflow as tf
from tensorflow.keras import Model, initializers
from tensorflow.keras.regularizers import L2
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, Dense, LeakyReLU, Input, Bidirectional, LSTM

class BasicConv1D(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_size, strides=1, **kwargs):
        super(BasicConv1D, self).__init__()
        self.conv = Conv1D(filters, kernel_size=kernel_size, strides=strides, padding="same", **kwargs)
        self.activation = LeakyReLU()

    def call(self, x):
        x = self.conv(x)
        x = self.activation(x)
        return x

class Module_35x35(tf.keras.layers.Layer):
    def __init__(self, in_channels: int, regularization_factor: float, seed_value: int):
        super(Module_35x35, self).__init__()
        self.branch1 = tf.keras.Sequential([
            AveragePooling1D(pool_size=3, strides=1, padding='same'),
            BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value))
        ])
        self.branch2 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels, kernel_size=1, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch3 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels, kernel_size=1, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch4 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])

    def call(self, x):
        branch1 = self.branch1(x)
        branch2 = self.branch2(x)
        branch3 = self.branch3(x)
        branch4 = self.branch4(x)
        out = tf.concat([branch1, branch2, branch3, branch4], axis=-1)
        return out

class IPA(tf.keras.Model):
    def __init__(self, seed_value, regularization_factor, dropout_rate=0.2):
        super(IPA, self).__init__()
        self.stem = tf.keras.Sequential([
            BasicConv1D(filters=32, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=32, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.module_35x35 = Module_35x35(in_channels=32, regularization_factor=regularization_factor, seed_value=seed_value)
        
        # LSTM pour capturer la dépendance temporelle
        self.lstm = Bidirectional(LSTM(64, return_sequences=True))

        self.dropout = Dropout(rate=dropout_rate)

        # Prédiction pour chaque point
        self.classifier = Dense(3)

    def call(self, x):
        out = self.stem(x)
        out = self.module_35x35(out)
        out = self.lstm(out)
        out = self.dropout(out)
        out = self.classifier(out)  # (batch, seq_len, 3)

        # Séparer la sortie en 3 parties
        pred, loc, area = tf.split(out, 3, axis=-1)

        # Appliquer sigmoid sur pred et loc
        pred = tf.nn.sigmoid(pred)
        loc = tf.nn.sigmoid(loc)

        # Concatenate les résultats
        out = tf.concat([pred, loc, area], axis=-1)
        return out

self.branch1 = tf.keras.Sequential([
    AveragePooling1D(pool_size=3, strides=1, padding='same'),
    BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=1,
                kernel_regularizer=L2(regularization_factor),
                kernel_initializer=initializers.HeNormal(seed_value))
])

if __name__ == '__main__':
    model = IPA(seed_value=1, regularization_factor=0.0095)
    model.build((None, 3890, 2))  # (batch, seq_len, 2)
    model.summary()



import os
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import EarlyStopping
from tqdm.keras import TqdmCallback
from sklearn.model_selection import train_test_split

# =========================
# Chargement et préparation
# =========================

file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

# On ne garde que les chromatogrammes avec pics
df = df.dropna(subset=['pics'])
df = df[df["x"].apply(len) == 71840]

# On coupe les chromatogrammes à x <= 150
def truncate(row):
    mask = np.array(row["x"]) <= 150
    row["x"] = np.array(row["x"])[mask].tolist()
    row["y"] = np.array(row["y"])[mask].tolist()
    return row

df = df.apply(truncate, axis=1)

sequence_length = df.iloc[0]["x"].__len__()

# Input X : (n, sequence_length, 2)
X = np.array([np.column_stack((row["x"], row["y"])) for _, row in df.iterrows()])

# Normalisation du temps
X[:, :, 0] = X[:, :, 0] / 150.0

# Normalisation de l'intensité
for i in range(X.shape[0]):
    max_intensity = np.max(X[i, :, 1])
    if max_intensity > 0:
        X[i, :, 1] /= max_intensity

Y = np.zeros((len(df), sequence_length, 3), dtype=np.float32)

for i, (_, row) in enumerate(df.iterrows()):
    x_time = np.array(row["x"])
    
    for pic_time, data in row["pics"].items():
        borne_avant_time = data[1]
        pic_time = float(pic_time)
        borne_apres_time = data[2]

        if pic_time > 150:
            continue

        # Trouver les indices les plus proches des bornes et du pic
        borne_avant_idx = np.argmin(np.abs(x_time - borne_avant_time))
        pic_idx = np.argmin(np.abs(x_time - pic_time))
z
        # Colonne 1 : Pic (mettre 1 à l'indice du pic)
        Y[i, pic_idx, 0] = 1

        # Colonne 2 : Temps normalisé du pic (mettre le temps normalisé à l'indice du pic)
        Y[i, pic_idx, 1] = pic_time / 150.0

        # Colonne 3 : Différence entre le temps à la borne après et la borne avant (mettre la différence dans la troisième colonne)
        time_diff = borne_apres_time - borne_avant_time
        Y[i, pic_idx, 2] = time_diff
# ======================
# Split train/test
# ======================

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ======================
# Hyperparamètres
# ======================

MODEL_NAME = 'IPA'
path_model = 'model/'
SEED_VALUE = 1

LEARNING_RATE = .001
BATCH_SIZE = 16
EPOCHS = 500

REGULARIZATION_COEF = .0095
DROPOUT_RATE = .2

# ======================
# Entraînement
# ======================

if not os.path.exists(path_model + MODEL_NAME):
    print('Model does not exist, training in progress...')

    model = IPA(seed_value=SEED_VALUE,
                regularization_factor=REGULARIZATION_COEF,
                dropout_rate=DROPOUT_RATE)

    # Schedule
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=LEARNING_RATE,
        decay_steps=10000, decay_rate=.001)

    optimizer = optimizers.Adam(learning_rate=lr_schedule)
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    stop_early = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)

    history = model.fit(
        X_train, Y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_test, Y_test),
        callbacks=[stop_early, TqdmCallback(verbose=1)],
        verbose=0
    )

    print('Saving model')
    os.makedirs(path_model + MODEL_NAME, exist_ok=True)
    model.save(path_model + MODEL_NAME, overwrite=True, save_format='tf')

else:
    print('Model does exist, loading...')
    model = tf.keras.models.load_model(path_model + MODEL_NAME)

# Évaluation globale
loss, acc = model.evaluate(X_test, Y_test)
print(f"Test Loss = {loss:.4f} | Test Accuracy = {acc:.4f}")
# Prédiction sur le premier chromatogramme du test
sample_idx = 0
X_sample = X_test[sample_idx:sample_idx+1]  # (1, seq_len, 2)
Y_sample_true = Y_test[sample_idx]          # (seq_len, 4)

# Prédiction
Y_sample_pred = model.predict(X_sample)[0]  # (seq_len, 4)

# Décodage des classes point par point
predicted_classes = np.argmax(Y_sample_pred, axis=-1)
true_classes = np.argmax(Y_sample_true, axis=-1)

print("Classes prédictes :", predicted_classes)
print("Classes réelles   :", true_classes)
import matplotlib.pyplot as plt

plt.figure(figsize=(15,4))
plt.plot(X_sample[0, :, 0], X_sample[0, :, 1], label="Signal chromatogramme")
plt.scatter(X_sample[0, :, 0], predicted_classes * 0.1, label="Classes prédictes", marker='x')
plt.scatter(X_sample[0, :, 0], true_classes * 0.1, label="Classes vraies", marker='o', alpha=0.5)
plt.legend()
plt.title("Résultat sur un échantillon de test")
plt.show()


from sklearn.utils.class_weight import compute_class_weight

# Calcul des poids de classe
y_flat = np.argmax(Y_train, axis=-1).flatten()
class_weights = compute_class_weight('balanced', classes=[0, 1, 2, 3], y=y_flat)
class_weights_dict = {i: w for i, w in enumerate(class_weights)}

print("Poids des classes :", class_weights_dict)

# Compilation du modèle avec la fonction de perte pondérée
model.compile(optimizer=optimizer, 
              loss=tf.keras.losses.CategoricalCrossentropy(),
              metrics=['accuracy', Precision(), Recall()])
import numpy as np
from collections import Counter

# Conversion de Y_train en labels de classe (0, 1, 2 ou 3)
y_flat = np.argmax(Y_train, axis=-1).flatten()

# Comptage des occurrences des classes
class_counts = Counter(y_flat)

# Affichage des résultats
for class_label in sorted(class_counts.keys()):
    print(f"Classe {class_label} : {class_counts[class_label]} occurrences")


import os
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split

# =========================
# Chargement et préparation
# =========================

file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

# On garde seulement les chromatogrammes contenant des pics
df = df.dropna(subset=['pics'])

# === Extraction des temps de pic et des noms de composants ===

pic_times = []
component_names = []

for _, row in df.iterrows():
    for pic_time_str, data in row["pics"].items():
        pic_time = float(pic_time_str)
        component_name = data[0]  # Nom du composant
        if pic_time <= 150:
            pic_times.append(pic_time)
            component_names.append(component_name)

# ==========================
# Normalisation des inputs X
# ==========================

X = np.array(pic_times, dtype=np.float32).reshape(-1, 1)
X /= 150.0  # Normalisation

# ========================================
# Encodage des labels (one-hot vector Y)
# ========================================

# Étape 1 : encoder les labels en entiers
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(component_names)

# Étape 2 : transformer en one-hot
onehot_encoder = OneHotEncoder(sparse=False)
Y = onehot_encoder.fit_transform(integer_encoded.reshape(-1, 1))

# ======================
# Split train/test
# ======================

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ======================
# Infos pour le modèle
# ======================

num_classes = Y.shape[1]
input_shape = X.shape[1:]  # (1,)
print(f"Nombre total d’échantillons : {X.shape[0]}")
print(f"Nombre de classes : {num_classes}")
print(f"Noms des classes : {label_encoder.classes_}")
import os
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.metrics import classification_report
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam

# =========================
# Chargement et préparation
# =========================

file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

df = df.dropna(subset=['pics'])

pic_times = []
component_names = []

for _, row in df.iterrows():
    for pic_time_str, data in row["pics"].items():
        pic_time = float(pic_time_str)
        if pic_time <= 150:
            pic_times.append(pic_time)
            component_names.append(data[0])

# ====================
# Normalisation et Encodage
# ====================

X = np.array(pic_times, dtype=np.float32).reshape(-1, 1) / 150.0  # Normalisation

# Label encoding
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(component_names)

# One-hot encoding
onehot_encoder = OneHotEncoder(sparse=False)
Y = onehot_encoder.fit_transform(integer_encoded.reshape(-1, 1))

# ===============
# Split des données
# ===============

X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)
X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42, stratify=Y_temp)

print(f"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")
print(f"Nombre de classes : {Y.shape[1]}")
print(f"Classes : {label_encoder.classes_}")

# ======================
# Définition du modèle IPA
# ======================

from tensorflow.keras import Model, initializers
from tensorflow.keras.regularizers import L2
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten
from tensorflow.keras.layers import Dropout, Dense, LeakyReLU, Input, Concatenate

class BasicConv1D(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_size, strides=1, **kwargs):
        super(BasicConv1D, self).__init__()
        self.conv = Conv1D(filters, kernel_size=kernel_size, strides=strides, **kwargs)
        self.activation = LeakyReLU()

    def call(self, x):
        x = self.conv(x)
        x = self.activation(x)
        return x

class Module_35x35(tf.keras.layers.Layer):
    def __init__(self, in_channels, regularization_factor, seed_value):
        super(Module_35x35, self).__init__()
        self.branch1 = tf.keras.Sequential([
            MaxPooling1D(pool_size=2),
            BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value))
        ])
        self.branch2 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch3 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch4 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])

    def call(self, x):
        return Concatenate(axis=1)([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)])

class IPA(tf.keras.Model):
    def __init__(self, seed_value, regularization_factor, num_classes, dropout_rate=0.2):
        super(IPA, self).__init__()
        self.stem = tf.keras.Sequential([
            BasicConv1D(16, kernel_size=3, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(16, kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(16, kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.module_35x35 = Module_35x35(32, regularization_factor, seed_value)
        self.flatten = Flatten()
        self.dropout = Dropout(rate=dropout_rate)
        self.classifier = Dense(num_classes, activation="softmax")

    def call(self, x):
        x = tf.expand_dims(x, axis=2)  # (batch, length, 1)
        x = self.stem(x)
        x = self.module_35x35(x)
        x = self.flatten(x)
        x = self.dropout(x)
        return self.classifier(x)

# ================
# Compilation & Entraînement
# ================

model = IPA(seed_value=42, regularization_factor=0.001, num_classes=Y.shape[1])
model.compile(optimizer=Adam(learning_rate=0.001),
              loss="categorical_crossentropy",
              metrics=["accuracy"])

early_stop = EarlyStopping(patience=10, restore_best_weights=True)

model.fit(X_train, Y_train,
          validation_data=(X_val, Y_val),
          epochs=100,
          batch_size=32,
          callbacks=[early_stop],
          verbose=1)

# ==========
# Évaluation
# ==========

Y_pred = model.predict(X_test)
y_pred_labels = np.argmax(Y_pred, axis=1)
y_true_labels = np.argmax(Y_test, axis=1)

report = classification_report(y_true_labels, y_pred_labels, target_names=label_encoder.classes_)
print("\nClassification Report:\n", report)
