import tensorflow as tf
from tensorflow.keras import Model, initializers
from tensorflow.keras.regularizers import L2
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, Dense, LeakyReLU, Input, Bidirectional, LSTM

class BasicConv1D(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_size, strides=1, **kwargs):
        super(BasicConv1D, self).__init__()
        self.conv = Conv1D(filters, kernel_size=kernel_size, strides=strides, padding="same", **kwargs)
        self.activation = LeakyReLU()

    def call(self, x):
        x = self.conv(x)
        x = self.activation(x)
        return x

class Module_35x35(tf.keras.layers.Layer):
    def __init__(self, in_channels: int, regularization_factor: float, seed_value: int):
        super(Module_35x35, self).__init__()
        self.branch1 = tf.keras.Sequential([
            AveragePooling1D(pool_size=3, strides=1, padding='same'),
            BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value))
        ])
        self.branch2 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels, kernel_size=1, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch3 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels, kernel_size=1, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch4 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])

    def call(self, x):
        branch1 = self.branch1(x)
        branch2 = self.branch2(x)
        branch3 = self.branch3(x)
        branch4 = self.branch4(x)
        out = tf.concat([branch1, branch2, branch3, branch4], axis=-1)
        return out

class IPA(tf.keras.Model):
    def __init__(self, seed_value, regularization_factor, dropout_rate=0.2):
        super(IPA, self).__init__()
        self.stem = tf.keras.Sequential([
            BasicConv1D(filters=32, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=32, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.module_35x35 = Module_35x35(in_channels=32, regularization_factor=regularization_factor, seed_value=seed_value)
        
        # LSTM pour capturer la dépendance temporelle
        self.lstm = Bidirectional(LSTM(64, return_sequences=True))

        self.dropout = Dropout(rate=dropout_rate)

        # Prédiction pour chaque point
        self.classifier = Dense(3)

    def call(self, x):
        out = self.stem(x)
        out = self.module_35x35(out)
        out = self.lstm(out)
        out = self.dropout(out)
        out = self.classifier(out)  # (batch, seq_len, 3)

        # Séparer la sortie en 3 parties
        pred, loc, area = tf.split(out, 3, axis=-1)

        # Appliquer sigmoid sur pred et loc
        pred = tf.nn.sigmoid(pred)
        loc = tf.nn.sigmoid(loc)

        # Concatenate les résultats
        out = tf.concat([pred, loc, area], axis=-1)
        return out

self.branch1 = tf.keras.Sequential([
    AveragePooling1D(pool_size=3, strides=1, padding='same'),
    BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=1,
                kernel_regularizer=L2(regularization_factor),
                kernel_initializer=initializers.HeNormal(seed_value))
])

if __name__ == '__main__':
    model = IPA(seed_value=1, regularization_factor=0.0095)
    model.build((None, 3890, 2))  # (batch, seq_len, 2)
    model.summary()



import os
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import EarlyStopping
from tqdm.keras import TqdmCallback
from sklearn.model_selection import train_test_split

# =========================
# Chargement et préparation
# =========================

file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

# On ne garde que les chromatogrammes avec pics
df = df.dropna(subset=['pics'])
df = df[df["x"].apply(len) == 71840]

# On coupe les chromatogrammes à x <= 150
def truncate(row):
    mask = np.array(row["x"]) <= 150
    row["x"] = np.array(row["x"])[mask].tolist()
    row["y"] = np.array(row["y"])[mask].tolist()
    return row

df = df.apply(truncate, axis=1)

sequence_length = df.iloc[0]["x"].__len__()

# Input X : (n, sequence_length, 2)
X = np.array([np.column_stack((row["x"], row["y"])) for _, row in df.iterrows()])

# Normalisation du temps
X[:, :, 0] = X[:, :, 0] / 150.0

# Normalisation de l'intensité
for i in range(X.shape[0]):
    max_intensity = np.max(X[i, :, 1])
    if max_intensity > 0:
        X[i, :, 1] /= max_intensity

Y = np.zeros((len(df), sequence_length, 3), dtype=np.float32)

for i, (_, row) in enumerate(df.iterrows()):
    x_time = np.array(row["x"])
    
    for pic_time, data in row["pics"].items():
        borne_avant_time = data[1]
        pic_time = float(pic_time)
        borne_apres_time = data[2]

        if pic_time > 150:
            continue

        # Trouver les indices les plus proches des bornes et du pic
        borne_avant_idx = np.argmin(np.abs(x_time - borne_avant_time))
        pic_idx = np.argmin(np.abs(x_time - pic_time))
z
        # Colonne 1 : Pic (mettre 1 à l'indice du pic)
        Y[i, pic_idx, 0] = 1

        # Colonne 2 : Temps normalisé du pic (mettre le temps normalisé à l'indice du pic)
        Y[i, pic_idx, 1] = pic_time / 150.0

        # Colonne 3 : Différence entre le temps à la borne après et la borne avant (mettre la différence dans la troisième colonne)
        time_diff = borne_apres_time - borne_avant_time
        Y[i, pic_idx, 2] = time_diff
# ======================
# Split train/test
# ======================

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ======================
# Hyperparamètres
# ======================

MODEL_NAME = 'IPA'
path_model = 'model/'
SEED_VALUE = 1

LEARNING_RATE = .001
BATCH_SIZE = 16
EPOCHS = 500

REGULARIZATION_COEF = .0095
DROPOUT_RATE = .2

# ======================
# Entraînement
# ======================

if not os.path.exists(path_model + MODEL_NAME):
    print('Model does not exist, training in progress...')

    model = IPA(seed_value=SEED_VALUE,
                regularization_factor=REGULARIZATION_COEF,
                dropout_rate=DROPOUT_RATE)

    # Schedule
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=LEARNING_RATE,
        decay_steps=10000, decay_rate=.001)

    optimizer = optimizers.Adam(learning_rate=lr_schedule)
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    stop_early = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)

    history = model.fit(
        X_train, Y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_test, Y_test),
        callbacks=[stop_early, TqdmCallback(verbose=1)],
        verbose=0
    )

    print('Saving model')
    os.makedirs(path_model + MODEL_NAME, exist_ok=True)
    model.save(path_model + MODEL_NAME, overwrite=True, save_format='tf')

else:
    print('Model does exist, loading...')
    model = tf.keras.models.load_model(path_model + MODEL_NAME)

# Évaluation globale
loss, acc = model.evaluate(X_test, Y_test)
print(f"Test Loss = {loss:.4f} | Test Accuracy = {acc:.4f}")
# Prédiction sur le premier chromatogramme du test
sample_idx = 0
X_sample = X_test[sample_idx:sample_idx+1]  # (1, seq_len, 2)
Y_sample_true = Y_test[sample_idx]          # (seq_len, 4)

# Prédiction
Y_sample_pred = model.predict(X_sample)[0]  # (seq_len, 4)

# Décodage des classes point par point
predicted_classes = np.argmax(Y_sample_pred, axis=-1)
true_classes = np.argmax(Y_sample_true, axis=-1)

print("Classes prédictes :", predicted_classes)
print("Classes réelles   :", true_classes)
import matplotlib.pyplot as plt

plt.figure(figsize=(15,4))
plt.plot(X_sample[0, :, 0], X_sample[0, :, 1], label="Signal chromatogramme")
plt.scatter(X_sample[0, :, 0], predicted_classes * 0.1, label="Classes prédictes", marker='x')
plt.scatter(X_sample[0, :, 0], true_classes * 0.1, label="Classes vraies", marker='o', alpha=0.5)
plt.legend()
plt.title("Résultat sur un échantillon de test")
plt.show()


from sklearn.utils.class_weight import compute_class_weight

# Calcul des poids de classe
y_flat = np.argmax(Y_train, axis=-1).flatten()
class_weights = compute_class_weight('balanced', classes=[0, 1, 2, 3], y=y_flat)
class_weights_dict = {i: w for i, w in enumerate(class_weights)}

print("Poids des classes :", class_weights_dict)

# Compilation du modèle avec la fonction de perte pondérée
model.compile(optimizer=optimizer, 
              loss=tf.keras.losses.CategoricalCrossentropy(),
              metrics=['accuracy', Precision(), Recall()])
import numpy as np
from collections import Counter

# Conversion de Y_train en labels de classe (0, 1, 2 ou 3)
y_flat = np.argmax(Y_train, axis=-1).flatten()

# Comptage des occurrences des classes
class_counts = Counter(y_flat)

# Affichage des résultats
for class_label in sorted(class_counts.keys()):
    print(f"Classe {class_label} : {class_counts[class_label]} occurrences")


import os
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split

# =========================
# Chargement et préparation
# =========================

file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

# On garde seulement les chromatogrammes contenant des pics
df = df.dropna(subset=['pics'])

# === Extraction des temps de pic et des noms de composants ===

pic_times = []
component_names = []

for _, row in df.iterrows():
    for pic_time_str, data in row["pics"].items():
        pic_time = float(pic_time_str)
        component_name = data[0]  # Nom du composant
        if pic_time <= 150:
            pic_times.append(pic_time)
            component_names.append(component_name)

# ==========================
# Normalisation des inputs X
# ==========================

X = np.array(pic_times, dtype=np.float32).reshape(-1, 1)
X /= 150.0  # Normalisation

# ========================================
# Encodage des labels (one-hot vector Y)
# ========================================

# Étape 1 : encoder les labels en entiers
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(component_names)

# Étape 2 : transformer en one-hot
onehot_encoder = OneHotEncoder(sparse=False)
Y = onehot_encoder.fit_transform(integer_encoded.reshape(-1, 1))

# ======================
# Split train/test
# ======================

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ======================
# Infos pour le modèle
# ======================

num_classes = Y.shape[1]
input_shape = X.shape[1:]  # (1,)
print(f"Nombre total d’échantillons : {X.shape[0]}")
print(f"Nombre de classes : {num_classes}")
print(f"Noms des classes : {label_encoder.classes_}")
import os
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.metrics import classification_report
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam

# =========================
# Chargement et préparation
# =========================

file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

df = df.dropna(subset=['pics'])

pic_times = []
component_names = []

for _, row in df.iterrows():
    for pic_time_str, data in row["pics"].items():
        pic_time = float(pic_time_str)
        if pic_time <= 150:
            pic_times.append(pic_time)
            component_names.append(data[0])

# ====================
# Normalisation et Encodage
# ====================

X = np.array(pic_times, dtype=np.float32).reshape(-1, 1) / 150.0  # Normalisation

# Label encoding
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(component_names)

# One-hot encoding
onehot_encoder = OneHotEncoder(sparse=False)
Y = onehot_encoder.fit_transform(integer_encoded.reshape(-1, 1))

# ===============
# Split des données
# ===============

X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)
X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42, stratify=Y_temp)

print(f"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")
print(f"Nombre de classes : {Y.shape[1]}")
print(f"Classes : {label_encoder.classes_}")

# ======================
# Définition du modèle IPA
# ======================

from tensorflow.keras import Model, initializers
from tensorflow.keras.regularizers import L2
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten
from tensorflow.keras.layers import Dropout, Dense, LeakyReLU, Input, Concatenate

class BasicConv1D(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_size, strides=1, **kwargs):
        super(BasicConv1D, self).__init__()
        self.conv = Conv1D(filters, kernel_size=kernel_size, strides=strides, **kwargs)
        self.activation = LeakyReLU()

    def call(self, x):
        x = self.conv(x)
        x = self.activation(x)
        return x

class Module_35x35(tf.keras.layers.Layer):
    def __init__(self, in_channels, regularization_factor, seed_value):
        super(Module_35x35, self).__init__()
        self.branch1 = tf.keras.Sequential([
            MaxPooling1D(pool_size=2),
            BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value))
        ])
        self.branch2 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch3 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch4 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])

    def call(self, x):
        return Concatenate(axis=1)([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)])

class IPA(tf.keras.Model):
    def __init__(self, seed_value, regularization_factor, num_classes, dropout_rate=0.2):
        super(IPA, self).__init__()
        self.stem = tf.keras.Sequential([
            BasicConv1D(16, kernel_size=3, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(16, kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(16, kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.module_35x35 = Module_35x35(32, regularization_factor, seed_value)
        self.flatten = Flatten()
        self.dropout = Dropout(rate=dropout_rate)
        self.classifier = Dense(num_classes, activation="softmax")

    def call(self, x):
        x = tf.expand_dims(x, axis=2)  # (batch, length, 1)
        x = self.stem(x)
        x = self.module_35x35(x)
        x = self.flatten(x)
        x = self.dropout(x)
        return self.classifier(x)

# ================
# Compilation & Entraînement
# ================

model = IPA(seed_value=42, regularization_factor=0.001, num_classes=Y.shape[1])
model.compile(optimizer=Adam(learning_rate=0.001),
              loss="categorical_crossentropy",
              metrics=["accuracy"])

early_stop = EarlyStopping(patience=10, restore_best_weights=True)

model.fit(X_train, Y_train,
          validation_data=(X_val, Y_val),
          epochs=100,
          batch_size=32,
          callbacks=[early_stop],
          verbose=1)

# ==========
# Évaluation
# ==========

Y_pred = model.predict(X_test)
y_pred_labels = np.argmax(Y_pred, axis=1)
y_true_labels = np.argmax(Y_test, axis=1)

report = classification_report(y_true_labels, y_pred_labels, target_names=label_encoder.classes_)
print("\nClassification Report:\n", report)

import tensorflow as tf
import tensorflow.keras.backend as K

class CustomLoss(tf.keras.losses.Loss):
    def __init__(self, n_splits, weight_prob=1.0, weight_hinge=1.0, gamma=2.0, alpha=0.25, **kwargs):
        super().__init__(**kwargs)
        self.n_splits = n_splits
        self.weight_prob = weight_prob
        self.weight_hinge = weight_hinge
        self.gamma = gamma
        self.alpha = alpha

    def categorical_focal_loss(self, y_true, y_pred):
        epsilon = K.epsilon()
        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)
        
        # Normalisation
        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)
        
        # Calcul de la perte
        cross_entropy = -y_true * K.log(y_pred)
        focal_loss = self.alpha * K.pow(1 - y_pred, self.gamma) * cross_entropy
        return K.sum(focal_loss, axis=-1)

    def call(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)

        prob_loss = self.categorical_focal_loss(y_true, y_pred)
        return self.weight_prob * tf.reduce_mean(prob_loss)
model.compile(
    optimizer='adam',
    loss=CustomLoss(n_splits=1, weight_prob=1.0, gamma=2.0, alpha=0.25),
    metrics=['accuracy']
)


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
import xgboost as xgb

# ================
# Chargement des données
# ================

file_path = "chromatogrammes.json"
df = pd.read_json(file_path)
df = df.dropna(subset=['pics'])

pic_times = []
component_names = []

for _, row in df.iterrows():
    for pic_time_str, data in row["pics"].items():
        pic_time = float(pic_time_str)
        if pic_time <= 150:
            pic_times.append(pic_time)
            component_names.append(data[0])

# ================
# Normalisation et Label Encoding
# ================

X = np.array(pic_times, dtype=np.float32).reshape(-1, 1) / 150.0  # Normalisation entre 0 et 1

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(component_names)  # Pas de one-hot ici

# ================
# Split des données
# ================

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

print(f"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")
print(f"Nombre de classes : {len(label_encoder.classes_)}")
print(f"Classes : {label_encoder.classes_}")

# ================
# Entraînement du modèle XGBoost
# ================

model = xgb.XGBClassifier(
    objective='multi:softmax',
    num_class=len(label_encoder.classes_),
    eval_metric='mlogloss',
    use_label_encoder=False,
    max_depth=4,
    learning_rate=0.1,
    n_estimators=100,
    random_state=42
)

model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=10,
    verbose=True
)

# ================
# Évaluation
# ================

y_pred = model.predict(X_test)

print("\nAccuracy :", accuracy_score(y_test, y_pred))
print("\nClassification Report :\n", classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# (Optionnel) prédire le nom du composant
predicted_class_names = label_encoder.inverse_transform(y_pred)
from sklearn.utils.class_weight import compute_sample_weight

# Poids inverses pour équilibrer les classes
weights = compute_sample_weight(class_weight='balanced', y=y_train)

model.fit(
    X_train, y_train,
    sample_weight=weights,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=10,
    verbose=True
)
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from xgboost import XGBClassifier
from sklearn.utils.class_weight import compute_sample_weight

# Poids des classes pour compenser le déséquilibre
weights = compute_sample_weight(class_weight='balanced', y=y_train)

# Définition du modèle de base
xgb_model = XGBClassifier(
    objective='multi:softmax',
    num_class=len(np.unique(y_train)),
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42
)

# Grille des hyperparamètres à tester
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'n_estimators': [50, 100, 200],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# GridSearchCV avec validation croisée
grid_search = GridSearchCV(
    estimator=xgb_model,
    param_grid=param_grid,
    scoring='f1_macro',  # ou 'accuracy', 'balanced_accuracy', 'f1_weighted'
    cv=3,
    verbose=1,
    n_jobs=-1
)

# Entraînement
grid_search.fit(X_train, y_train, sample_weight=weights)

# Résultats
print("Meilleurs hyperparamètres :", grid_search.best_params_)

# Évaluation sur le test
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))
import numpy as np
import xgboost as xgb

# Définir la fonction de perte personnalisée pour XGBoost (Focal Loss)
def focal_loss(gamma=2.0, alpha=0.25):
    def focal_loss_inner(y_true, y_pred):
        y_true = np.array(y_true)
        y_pred = np.clip(np.array(y_pred), 1e-7, 1 - 1e-7)
        
        # Calcul du cross entropy classique
        cross_entropy = -y_true * np.log(y_pred)
        
        # Calcul du modulateur de focal loss
        modulating_factor = np.power(1 - y_pred, gamma)
        loss = alpha * modulating_factor * cross_entropy
        grad = loss - y_true
        hess = grad * (1 - y_pred)  # Calcul du hessien
        
        return grad, hess
    return focal_loss_inner

# Paramètres de base pour XGBoost
params = {
    'objective': 'multi:softmax',  # pour la classification multi-classes
    'num_class': 5,  # Remplace par le nombre de classes de ton dataset
    'eval_metric': 'mlogloss',
    'eta': 0.1,
    'max_depth': 6,
    'alpha': 0.25,  # alpha de la focal loss
    'gamma': 2.0    # gamma de la focal loss
}

# Charger les données et créer un DMatrix
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Appliquer la fonction de perte personnalisée dans l'entraînement
bst = xgb.train(
    params,
    dtrain,
    num_boost_round=100,
    obj=focal_loss(gamma=2.0, alpha=0.25),  # Fonction de perte focalisée
    evals=[(dtest, 'test')],
    early_stopping_rounds=10
)

# Prédiction
y_pred = bst.predict(dtest)
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Initialisation du modèle SVM
model = SVC(class_weight='balanced', kernel='linear', random_state=42)

# Entraînement
model.fit(X_train, np.argmax(Y_train, axis=1))

# Prédictions
y_pred = model.predict(X_test)

# Évaluation
accuracy = accuracy_score(np.argmax(Y_test, axis=1), y_pred)
print(f"Accuracy : {accuracy}")


import tensorflow as tf
import tensorflow.keras.backend as K

class CustomLoss(tf.keras.losses.Loss):
    def __init__(self, n_splits, alpha_per_class, weight_prob=1.0, weight_hinge=1.0, gamma=2.0, **kwargs):
        super().__init__(**kwargs)
        self.n_splits = n_splits
        self.weight_prob = weight_prob
        self.weight_hinge = weight_hinge
        self.gamma = gamma
        self.alpha_per_class = tf.constant(alpha_per_class, dtype=tf.float32)  # Tensor de taille (num_classes,)

    def categorical_focal_loss(self, y_true, y_pred):
        epsilon = K.epsilon()
        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)

        # Normalisation
        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)

        # focal_weight shape : (batch, num_classes)
        cross_entropy = -y_true * K.log(y_pred)
        alpha_factor = y_true * self.alpha_per_class  # broadcasting (batch, num_classes)
        focal_weight = K.pow(1 - y_pred, self.gamma)

        loss = alpha_factor * focal_weight * cross_entropy
        return K.sum(loss, axis=-1)

    def call(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)

        prob_loss = self.categorical_focal_loss(y_true, y_pred)
        return self.weight_prob * tf.reduce_mean(prob_loss)
from sklearn.utils.class_weight import compute_class_weight

# Encode labels
integer_encoded = label_encoder.transform(component_names)
class_weights = compute_class_weight(class_weight='balanced',
                                     classes=np.unique(integer_encoded),
                                     y=integer_encoded)
alpha_per_class = class_weights / np.sum(class_weights)
loss_fn = CustomLoss(n_splits=1, alpha_per_class=alpha_per_class, gamma=2.0)
model.compile(optimizer=Adam(learning_rate=0.001),
              loss=loss_fn,
              metrics=["accuracy"])
