import tensorflow as tf
from tensorflow.keras import Model, initializers
from tensorflow.keras.regularizers import L2
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, Dense, LeakyReLU, Input, Bidirectional, LSTM

class BasicConv1D(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_size, strides=1, **kwargs):
        super(BasicConv1D, self).__init__()
        self.conv = Conv1D(filters, kernel_size=kernel_size, strides=strides, padding="same", **kwargs)
        self.activation = LeakyReLU()

    def call(self, x):
        x = self.conv(x)
        x = self.activation(x)
        return x

class Module_35x35(tf.keras.layers.Layer):
    def __init__(self, in_channels: int, regularization_factor: float, seed_value: int):
        super(Module_35x35, self).__init__()
        self.branch1 = tf.keras.Sequential([
            AveragePooling1D(pool_size=3, strides=1, padding='same'),
            BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value))
        ])
        self.branch2 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels, kernel_size=1, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch3 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels, kernel_size=1, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch4 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])

    def call(self, x):
        branch1 = self.branch1(x)
        branch2 = self.branch2(x)
        branch3 = self.branch3(x)
        branch4 = self.branch4(x)
        out = tf.concat([branch1, branch2, branch3, branch4], axis=-1)
        return out

class IPA(tf.keras.Model):
    def __init__(self, seed_value, regularization_factor, dropout_rate=0.2):
        super(IPA, self).__init__()
        self.stem = tf.keras.Sequential([
            BasicConv1D(filters=32, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=32, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.module_35x35 = Module_35x35(in_channels=32, regularization_factor=regularization_factor, seed_value=seed_value)
        
        # LSTM pour capturer la dépendance temporelle
        self.lstm = Bidirectional(LSTM(64, return_sequences=True))

        self.dropout = Dropout(rate=dropout_rate)

        # Prédiction pour chaque point
        self.classifier = Dense(3)

    def call(self, x):
        out = self.stem(x)
        out = self.module_35x35(out)
        out = self.lstm(out)
        out = self.dropout(out)
        out = self.classifier(out)  # (batch, seq_len, 3)

        # Séparer la sortie en 3 parties
        pred, loc, area = tf.split(out, 3, axis=-1)

        # Appliquer sigmoid sur pred et loc
        pred = tf.nn.sigmoid(pred)
        loc = tf.nn.sigmoid(loc)

        # Concatenate les résultats
        out = tf.concat([pred, loc, area], axis=-1)
        return out

self.branch1 = tf.keras.Sequential([
    AveragePooling1D(pool_size=3, strides=1, padding='same'),
    BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=1,
                kernel_regularizer=L2(regularization_factor),
                kernel_initializer=initializers.HeNormal(seed_value))
])

if __name__ == '__main__':
    model = IPA(seed_value=1, regularization_factor=0.0095)
    model.build((None, 3890, 2))  # (batch, seq_len, 2)
    model.summary()



import os
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import EarlyStopping
from tqdm.keras import TqdmCallback
from sklearn.model_selection import train_test_split

# =========================
# Chargement et préparation
# =========================

file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

# On ne garde que les chromatogrammes avec pics
df = df.dropna(subset=['pics'])
df = df[df["x"].apply(len) == 71840]

# On coupe les chromatogrammes à x <= 150
def truncate(row):
    mask = np.array(row["x"]) <= 150
    row["x"] = np.array(row["x"])[mask].tolist()
    row["y"] = np.array(row["y"])[mask].tolist()
    return row

df = df.apply(truncate, axis=1)

sequence_length = df.iloc[0]["x"].__len__()

# Input X : (n, sequence_length, 2)
X = np.array([np.column_stack((row["x"], row["y"])) for _, row in df.iterrows()])

# Normalisation du temps
X[:, :, 0] = X[:, :, 0] / 150.0

# Normalisation de l'intensité
for i in range(X.shape[0]):
    max_intensity = np.max(X[i, :, 1])
    if max_intensity > 0:
        X[i, :, 1] /= max_intensity

Y = np.zeros((len(df), sequence_length, 3), dtype=np.float32)

for i, (_, row) in enumerate(df.iterrows()):
    x_time = np.array(row["x"])
    
    for pic_time, data in row["pics"].items():
        borne_avant_time = data[1]
        pic_time = float(pic_time)
        borne_apres_time = data[2]

        if pic_time > 150:
            continue

        # Trouver les indices les plus proches des bornes et du pic
        borne_avant_idx = np.argmin(np.abs(x_time - borne_avant_time))
        pic_idx = np.argmin(np.abs(x_time - pic_time))
z
        # Colonne 1 : Pic (mettre 1 à l'indice du pic)
        Y[i, pic_idx, 0] = 1

        # Colonne 2 : Temps normalisé du pic (mettre le temps normalisé à l'indice du pic)
        Y[i, pic_idx, 1] = pic_time / 150.0

        # Colonne 3 : Différence entre le temps à la borne après et la borne avant (mettre la différence dans la troisième colonne)
        time_diff = borne_apres_time - borne_avant_time
        Y[i, pic_idx, 2] = time_diff
# ======================
# Split train/test
# ======================

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ======================
# Hyperparamètres
# ======================

MODEL_NAME = 'IPA'
path_model = 'model/'
SEED_VALUE = 1

LEARNING_RATE = .001
BATCH_SIZE = 16
EPOCHS = 500

REGULARIZATION_COEF = .0095
DROPOUT_RATE = .2

# ======================
# Entraînement
# ======================

if not os.path.exists(path_model + MODEL_NAME):
    print('Model does not exist, training in progress...')

    model = IPA(seed_value=SEED_VALUE,
                regularization_factor=REGULARIZATION_COEF,
                dropout_rate=DROPOUT_RATE)

    # Schedule
    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
        initial_learning_rate=LEARNING_RATE,
        decay_steps=10000, decay_rate=.001)

    optimizer = optimizers.Adam(learning_rate=lr_schedule)
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

    stop_early = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)

    history = model.fit(
        X_train, Y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_test, Y_test),
        callbacks=[stop_early, TqdmCallback(verbose=1)],
        verbose=0
    )

    print('Saving model')
    os.makedirs(path_model + MODEL_NAME, exist_ok=True)
    model.save(path_model + MODEL_NAME, overwrite=True, save_format='tf')

else:
    print('Model does exist, loading...')
    model = tf.keras.models.load_model(path_model + MODEL_NAME)

# Évaluation globale
loss, acc = model.evaluate(X_test, Y_test)
print(f"Test Loss = {loss:.4f} | Test Accuracy = {acc:.4f}")
# Prédiction sur le premier chromatogramme du test
sample_idx = 0
X_sample = X_test[sample_idx:sample_idx+1]  # (1, seq_len, 2)
Y_sample_true = Y_test[sample_idx]          # (seq_len, 4)

# Prédiction
Y_sample_pred = model.predict(X_sample)[0]  # (seq_len, 4)

# Décodage des classes point par point
predicted_classes = np.argmax(Y_sample_pred, axis=-1)
true_classes = np.argmax(Y_sample_true, axis=-1)

print("Classes prédictes :", predicted_classes)
print("Classes réelles   :", true_classes)
import matplotlib.pyplot as plt

plt.figure(figsize=(15,4))
plt.plot(X_sample[0, :, 0], X_sample[0, :, 1], label="Signal chromatogramme")
plt.scatter(X_sample[0, :, 0], predicted_classes * 0.1, label="Classes prédictes", marker='x')
plt.scatter(X_sample[0, :, 0], true_classes * 0.1, label="Classes vraies", marker='o', alpha=0.5)
plt.legend()
plt.title("Résultat sur un échantillon de test")
plt.show()


from sklearn.utils.class_weight import compute_class_weight

# Calcul des poids de classe
y_flat = np.argmax(Y_train, axis=-1).flatten()
class_weights = compute_class_weight('balanced', classes=[0, 1, 2, 3], y=y_flat)
class_weights_dict = {i: w for i, w in enumerate(class_weights)}

print("Poids des classes :", class_weights_dict)

# Compilation du modèle avec la fonction de perte pondérée
model.compile(optimizer=optimizer, 
              loss=tf.keras.losses.CategoricalCrossentropy(),
              metrics=['accuracy', Precision(), Recall()])
import numpy as np
from collections import Counter

# Conversion de Y_train en labels de classe (0, 1, 2 ou 3)
y_flat = np.argmax(Y_train, axis=-1).flatten()

# Comptage des occurrences des classes
class_counts = Counter(y_flat)

# Affichage des résultats
for class_label in sorted(class_counts.keys()):
    print(f"Classe {class_label} : {class_counts[class_label]} occurrences")


import os
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split

# =========================
# Chargement et préparation
# =========================

file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

# On garde seulement les chromatogrammes contenant des pics
df = df.dropna(subset=['pics'])

# === Extraction des temps de pic et des noms de composants ===

pic_times = []
component_names = []

for _, row in df.iterrows():
    for pic_time_str, data in row["pics"].items():
        pic_time = float(pic_time_str)
        component_name = data[0]  # Nom du composant
        if pic_time <= 150:
            pic_times.append(pic_time)
            component_names.append(component_name)

# ==========================
# Normalisation des inputs X
# ==========================

X = np.array(pic_times, dtype=np.float32).reshape(-1, 1)
X /= 150.0  # Normalisation

# ========================================
# Encodage des labels (one-hot vector Y)
# ========================================

# Étape 1 : encoder les labels en entiers
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(component_names)

# Étape 2 : transformer en one-hot
onehot_encoder = OneHotEncoder(sparse=False)
Y = onehot_encoder.fit_transform(integer_encoded.reshape(-1, 1))

# ======================
# Split train/test
# ======================

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# ======================
# Infos pour le modèle
# ======================

num_classes = Y.shape[1]
input_shape = X.shape[1:]  # (1,)
print(f"Nombre total d’échantillons : {X.shape[0]}")
print(f"Nombre de classes : {num_classes}")
print(f"Noms des classes : {label_encoder.classes_}")
import os
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.metrics import classification_report
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam

# =========================
# Chargement et préparation
# =========================

file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

df = df.dropna(subset=['pics'])

pic_times = []
component_names = []

for _, row in df.iterrows():
    for pic_time_str, data in row["pics"].items():
        pic_time = float(pic_time_str)
        if pic_time <= 150:
            pic_times.append(pic_time)
            component_names.append(data[0])

# ====================
# Normalisation et Encodage
# ====================

X = np.array(pic_times, dtype=np.float32).reshape(-1, 1) / 150.0  # Normalisation
X = np.pad(X, ((0, 0), (0, 2)), mode='constant') 
# Label encoding
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(component_names)

# One-hot encoding
onehot_encoder = OneHotEncoder(sparse=False)
Y = onehot_encoder.fit_transform(integer_encoded.reshape(-1, 1))

# ===============
# Split des données
# ===============

X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)
X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42, stratify=Y_temp)

print(f"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")
print(f"Nombre de classes : {Y.shape[1]}")
print(f"Classes : {label_encoder.classes_}")

# ======================
# Définition du modèle IPA
# ======================

from tensorflow.keras import Model, initializers
from tensorflow.keras.regularizers import L2
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten
from tensorflow.keras.layers import Dropout, Dense, LeakyReLU, Input, Concatenate

class BasicConv1D(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_size, strides=1, **kwargs):
        super(BasicConv1D, self).__init__()
        self.conv = Conv1D(filters, kernel_size=kernel_size, strides=strides, **kwargs)
        self.activation = LeakyReLU()

    def call(self, x):
        x = self.conv(x)
        x = self.activation(x)
        return x

class Module_35x35(tf.keras.layers.Layer):
    def __init__(self, in_channels, regularization_factor, seed_value):
        super(Module_35x35, self).__init__()
        self.branch1 = tf.keras.Sequential([
            MaxPooling1D(pool_size=2),
            BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value))
        ])
        self.branch2 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=1,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch3 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(filters=in_channels * 2, kernel_size=3, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.branch4 = tf.keras.Sequential([
            BasicConv1D(filters=in_channels * 2, kernel_size=1, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])

    def call(self, x):
        return Concatenate(axis=1)([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)])

class IPA(tf.keras.Model):
    def __init__(self, seed_value, regularization_factor, num_classes, dropout_rate=0.2):
        super(IPA, self).__init__()
        self.stem = tf.keras.Sequential([
            BasicConv1D(16, kernel_size=3, strides=2,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(16, kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
            BasicConv1D(16, kernel_size=3,
                        kernel_regularizer=L2(regularization_factor),
                        kernel_initializer=initializers.HeNormal(seed_value)),
        ])
        self.module_35x35 = Module_35x35(32, regularization_factor, seed_value)
        self.flatten = Flatten()
        self.dropout = Dropout(rate=dropout_rate)
        self.classifier = Dense(num_classes, activation="softmax")

    def call(self, x):
        x = tf.expand_dims(x, axis=2)  # (batch, length, 1)
        x = self.stem(x)
        x = self.module_35x35(x)
        x = self.flatten(x)
        x = self.dropout(x)
        return self.classifier(x)

# ================
# Compilation & Entraînement
# ================

model = IPA(seed_value=42, regularization_factor=0.001, num_classes=Y.shape[1])
model.compile(optimizer=Adam(learning_rate=0.001),
              loss="categorical_crossentropy",
              metrics=["accuracy"])

early_stop = EarlyStopping(patience=10, restore_best_weights=True)

model.fit(X_train, Y_train,
          validation_data=(X_val, Y_val),
          epochs=100,
          batch_size=32,
          callbacks=[early_stop],
          verbose=1)

# ==========
# Évaluation
# ==========

Y_pred = model.predict(X_test)
y_pred_labels = np.argmax(Y_pred, axis=1)
y_true_labels = np.argmax(Y_test, axis=1)

report = classification_report(y_true_labels, y_pred_labels, target_names=label_encoder.classes_)
print("\nClassification Report:\n", report)

import tensorflow as tf
import tensorflow.keras.backend as K

class CustomLoss(tf.keras.losses.Loss):
    def __init__(self, n_splits, weight_prob=1.0, weight_hinge=1.0, gamma=2.0, alpha=0.25, **kwargs):
        super().__init__(**kwargs)
        self.n_splits = n_splits
        self.weight_prob = weight_prob
        self.weight_hinge = weight_hinge
        self.gamma = gamma
        self.alpha = alpha

    def categorical_focal_loss(self, y_true, y_pred):
        epsilon = K.epsilon()
        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)
        
        # Normalisation
        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)
        
        # Calcul de la perte
        cross_entropy = -y_true * K.log(y_pred)
        focal_loss = self.alpha * K.pow(1 - y_pred, self.gamma) * cross_entropy
        return K.sum(focal_loss, axis=-1)

    def call(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)

        prob_loss = self.categorical_focal_loss(y_true, y_pred)
        return self.weight_prob * tf.reduce_mean(prob_loss)
model.compile(
    optimizer='adam',
    loss=CustomLoss(n_splits=1, weight_prob=1.0, gamma=2.0, alpha=0.25),
    metrics=['accuracy']
)


import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
import xgboost as xgb

# ================
# Chargement des données
# ================

file_path = "chromatogrammes.json"
df = pd.read_json(file_path)
df = df.dropna(subset=['pics'])

pic_times = []
component_names = []

for _, row in df.iterrows():
    for pic_time_str, data in row["pics"].items():
        pic_time = float(pic_time_str)
        if pic_time <= 150:
            pic_times.append(pic_time)
            component_names.append(data[0])

# ================
# Normalisation et Label Encoding
# ================

X = np.array(pic_times, dtype=np.float32).reshape(-1, 1) / 150.0  # Normalisation entre 0 et 1

label_encoder = LabelEncoder()
y = label_encoder.fit_transform(component_names)  # Pas de one-hot ici

# ================
# Split des données
# ================

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

print(f"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}")
print(f"Nombre de classes : {len(label_encoder.classes_)}")
print(f"Classes : {label_encoder.classes_}")

# ================
# Entraînement du modèle XGBoost
# ================

model = xgb.XGBClassifier(
    objective='multi:softmax',
    num_class=len(label_encoder.classes_),
    eval_metric='mlogloss',
    use_label_encoder=False,
    max_depth=4,
    learning_rate=0.1,
    n_estimators=100,
    random_state=42
)

model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=10,
    verbose=True
)

# ================
# Évaluation
# ================

y_pred = model.predict(X_test)

print("\nAccuracy :", accuracy_score(y_test, y_pred))
print("\nClassification Report :\n", classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# (Optionnel) prédire le nom du composant
predicted_class_names = label_encoder.inverse_transform(y_pred)
from sklearn.utils.class_weight import compute_sample_weight

# Poids inverses pour équilibrer les classes
weights = compute_sample_weight(class_weight='balanced', y=y_train)

model.fit(
    X_train, y_train,
    sample_weight=weights,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=10,
    verbose=True
)
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from xgboost import XGBClassifier
from sklearn.utils.class_weight import compute_sample_weight

# Poids des classes pour compenser le déséquilibre
weights = compute_sample_weight(class_weight='balanced', y=y_train)

# Définition du modèle de base
xgb_model = XGBClassifier(
    objective='multi:softmax',
    num_class=len(np.unique(y_train)),
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42
)

# Grille des hyperparamètres à tester
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.3],
    'n_estimators': [50, 100, 200],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# GridSearchCV avec validation croisée
grid_search = GridSearchCV(
    estimator=xgb_model,
    param_grid=param_grid,
    scoring='f1_macro',  # ou 'accuracy', 'balanced_accuracy', 'f1_weighted'
    cv=3,
    verbose=1,
    n_jobs=-1
)

# Entraînement
grid_search.fit(X_train, y_train, sample_weight=weights)

# Résultats
print("Meilleurs hyperparamètres :", grid_search.best_params_)

# Évaluation sur le test
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))
import numpy as np
import xgboost as xgb

# Définir la fonction de perte personnalisée pour XGBoost (Focal Loss)
def focal_loss(gamma=2.0, alpha=0.25):
    def focal_loss_inner(y_true, y_pred):
        y_true = np.array(y_true)
        y_pred = np.clip(np.array(y_pred), 1e-7, 1 - 1e-7)
        
        # Calcul du cross entropy classique
        cross_entropy = -y_true * np.log(y_pred)
        
        # Calcul du modulateur de focal loss
        modulating_factor = np.power(1 - y_pred, gamma)
        loss = alpha * modulating_factor * cross_entropy
        grad = loss - y_true
        hess = grad * (1 - y_pred)  # Calcul du hessien
        
        return grad, hess
    return focal_loss_inner

# Paramètres de base pour XGBoost
params = {
    'objective': 'multi:softmax',  # pour la classification multi-classes
    'num_class': 5,  # Remplace par le nombre de classes de ton dataset
    'eval_metric': 'mlogloss',
    'eta': 0.1,
    'max_depth': 6,
    'alpha': 0.25,  # alpha de la focal loss
    'gamma': 2.0    # gamma de la focal loss
}

# Charger les données et créer un DMatrix
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Appliquer la fonction de perte personnalisée dans l'entraînement
bst = xgb.train(
    params,
    dtrain,
    num_boost_round=100,
    obj=focal_loss(gamma=2.0, alpha=0.25),  # Fonction de perte focalisée
    evals=[(dtest, 'test')],
    early_stopping_rounds=10
)

# Prédiction
y_pred = bst.predict(dtest)
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Initialisation du modèle SVM
model = SVC(class_weight='balanced', kernel='linear', random_state=42)

# Entraînement
model.fit(X_train, np.argmax(Y_train, axis=1))

# Prédictions
y_pred = model.predict(X_test)

# Évaluation
accuracy = accuracy_score(np.argmax(Y_test, axis=1), y_pred)
print(f"Accuracy : {accuracy}")


import tensorflow as tf
import tensorflow.keras.backend as K

class CustomLoss(tf.keras.losses.Loss):
    def __init__(self, n_splits, alpha_per_class, weight_prob=1.0, weight_hinge=1.0, gamma=2.0, **kwargs):
        super().__init__(**kwargs)
        self.n_splits = n_splits
        self.weight_prob = weight_prob
        self.weight_hinge = weight_hinge
        self.gamma = gamma
        self.alpha_per_class = tf.constant(alpha_per_class, dtype=tf.float32)  # Tensor de taille (num_classes,)

    def categorical_focal_loss(self, y_true, y_pred):
        epsilon = K.epsilon()
        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)

        # Normalisation
        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)

        # focal_weight shape : (batch, num_classes)
        cross_entropy = -y_true * K.log(y_pred)
        alpha_factor = y_true * self.alpha_per_class  # broadcasting (batch, num_classes)
        focal_weight = K.pow(1 - y_pred, self.gamma)

        loss = alpha_factor * focal_weight * cross_entropy
        return K.sum(loss, axis=-1)

    def call(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.float32)
        y_pred = tf.cast(y_pred, tf.float32)

        prob_loss = self.categorical_focal_loss(y_true, y_pred)
        return self.weight_prob * tf.reduce_mean(prob_loss)
from sklearn.utils.class_weight import compute_class_weight

# Encode labels
integer_encoded = label_encoder.transform(component_names)
class_weights = compute_class_weight(class_weight='balanced',
                                     classes=np.unique(integer_encoded),
                                     y=integer_encoded)
alpha_per_class = class_weights / np.sum(class_weights)
loss_fn = CustomLoss(n_splits=1, alpha_per_class=alpha_per_class, gamma=2.0)
model.compile(optimizer=Adam(learning_rate=0.001),
              loss=loss_fn,
              metrics=["accuracy"])

self.masking = Masking(mask_value=0.0)  # Masquer les valeurs égales à 0
        
        self.module_35x35 = Module_35x35(32, regularization_factor, seed_value)
        self.flatten = Flatten()
        self.dropout = Dropout(rate=dropout_rate)
        self.classifier = Dense(num_classes, activation="softmax")

    def call(self, x):
        x = self.masking(x)  # Appliquer le masking sur l'entrée
        x = tf.expand_dims(x, axis=2)  # (batch, length, 1)
        x = self.stem(x)
        x = self.module_35x35(x)
        x = self.flatten(x)
        x = self.dropout(x)
        return self.classifier(x)

class_weights = compute_class_weight('balanced', classes=np.unique(integer_encoded), y=integer_encoded)
class_weight_dict = dict(zip(np.unique(integer_encoded), class_weights))

print(f"Class Weights: {class_weight_dict}")

# Optionnel: Si vous voulez voir les tailles des classes
print(f"Distribution des classes: \n{counts}")

# Utilisation des poids de classe dans le modèle Keras (exemple)
# Par exemple, vous pouvez entraîner votre modèle avec ces poids de classe :
model.fit(X, Y, epochs=10, batch_size=32, class_weight=class_weight_dict)

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# =========================
# Chargement et préparation
# =========================

file_path = "chromatogrammes.json"
df = pd.read_json(file_path)

df = df.dropna(subset=['pics'])

pic_times = []
component_names = []

# Collecte des données de pics
for _, row in df.iterrows():
    for pic_time_str, data in row["pics"].items():
        pic_time = float(pic_time_str)
        if pic_time <= 150:
            pic_times.append(pic_time)
            component_names.append(data[0])

# ====================
# Normalisation et Encodage
# ====================

# 1. Compter les occurrences des classes
class_counts = pd.Series(component_names).value_counts()

# 2. Identifier les classes à garder (au moins 50 exemples)
valid_classes = class_counts[class_counts >= 50].index.tolist()

# 3. Filtrer les composants et les temps de rétention en fonction des classes valides
filtered_pic_times = []
filtered_component_names = []

for pic_time, component in zip(pic_times, component_names):
    if component in valid_classes:
        filtered_pic_times.append(pic_time)
        filtered_component_names.append(component)

# 4. Limiter à 1000 exemples pour les classes ayant plus de 1000 occurrences
final_pic_times = []
final_component_names = []

for component in valid_classes:
    # Trouver les indices des échantillons de chaque classe
    indices = [i for i, x in enumerate(filtered_component_names) if x == component]
    if len(indices) > 1000:
        # Limiter à 1000 exemples
        indices = indices[:1000]
    
    # Ajouter les échantillons filtrés
    final_pic_times.extend([filtered_pic_times[i] for i in indices])
    final_component_names.extend([filtered_component_names[i] for i in indices])

# 5. Appliquer l'encodage après filtrage
X = np.array(final_pic_times, dtype=np.float32).reshape(-1, 1) / 150.0  # Normalisation
X = np.pad(X, ((0, 0), (0, 2)), mode='constant')  # Ajout de colonnes de zéros

# Label encoding
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(final_component_names)

# One-hot encoding
onehot_encoder = OneHotEncoder(sparse=False)
Y = onehot_encoder.fit_transform(integer_encoded.reshape(-1, 1))

# ============================
# Vérification de la suppression et du rééchantillonnage
# ============================

# Vérifier la distribution des classes après filtrage
remaining_classes = label_encoder.inverse_transform(np.argmax(Y, axis=1))
remaining_class_counts = pd.Series(remaining_classes).value_counts()

print(f"Classes restantes après filtrage et rééchantillonnage : \n{remaining_class_counts}")


import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from collections import Counter
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.utils.class_weight import compute_class_weight

# ========== Chargement ==========

file_path = "chromatogrammes.json"
df = pd.read_json(file_path)
df = df.dropna(subset=['pics'])

pic_times = []
component_names = []

for _, row in df.iterrows():
    for pic_time_str, data in row["pics"].items():
        pic_time = float(pic_time_str)
        if pic_time <= 150:
            pic_times.append(pic_time)
            component_names.append(data[0])

# ========== Équilibrage manuel des classes ==========

counts = Counter(component_names)
min_count = 10
max_count = 1000

filtered_times = []
filtered_names = []

temp_class_counter = {}

for time, name in zip(pic_times, component_names):
    if counts[name] >= min_count:
        temp_class_counter[name] = temp_class_counter.get(name, 0) + 1
        if temp_class_counter[name] <= max_count:
            filtered_times.append(time)
            filtered_names.append(name)

# ========== Encodage ==========

X = np.array(filtered_times, dtype=np.float32).reshape(-1, 1) / 150.0  # Normalisation entre 0 et 1

label_encoder = LabelEncoder()
y_int = label_encoder.fit_transform(filtered_names)
num_classes = len(label_encoder.classes_)

# One-hot encoding
onehot_encoder = OneHotEncoder(sparse=False)
Y = onehot_encoder.fit_transform(y_int.reshape(-1, 1))

# ========== Split & Poids de classe ==========

X_train, X_test, Y_train, Y_test, y_train_int, y_test_int = train_test_split(
    X, Y, y_int, test_size=0.2, stratify=y_int, random_state=42)

# Calcul des poids de classe
class_weights = compute_class_weight(class_weight='balanced',
                                     classes=np.unique(y_train_int),
                                     y=y_train_int)
class_weight_dict = {i: w for i, w in enumerate(class_weights)}

# ========== Modèle dense simple ==========

model = Sequential([
    Dense(128, activation='relu', input_shape=(1,)),
    Dense(64, activation='relu'),
    Dense(num_classes, activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# ========== Entraînement ==========

early_stop = EarlyStopping(patience=10, restore_best_weights=True)

model.fit(X_train, Y_train,
          validation_data=(X_test, Y_test),
          epochs=100,
          batch_size=32,
          callbacks=[early_stop],
          class_weight=class_weight_dict,
          verbose=1)

# ========== Prédiction ==========

def predict_component_name(retention_time):
    norm_time = np.array([[retention_time / 150.0]])
    prediction = model.predict(norm_time)
    predicted_index = np.argmax(prediction, axis=1)[0]
    return label_encoder.inverse_transform([predicted_index])[0]

# Exemple
print("Prédiction :", predict_component_name(73.4))
model = Sequential([
    Dense(512, activation='relu', input_shape=(1,)),
    Dropout(0.3),
    Dense(256, activation='relu'),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(num_classes, activation='softmax')
])

from collections import Counter
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import CategoricalNB, ComplementNB
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

# Supposons que tu aies déjà chargé les données et préparé les variables pic_times et component_names
# Exemple de données : 
# component_names = ["comp1", "comp2", "comp1", "comp3", ...]
# pic_times = [0.1, 0.2, 0.15, 0.25, ...]

# Encodage des composants (Label Encoding et One Hot Encoding)
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(component_names)

# Normalisation des données de temps de pic
X_full = np.array(pic_times, dtype=np.float32).reshape(-1, 1) / 150.0  # Normalisation
X_full = np.pad(X_full, ((0, 0), (0, 2)), mode='constant')  # Ajout du padding pour correspondre à la forme attendue

# One-hot encoding des labels
onehot_encoder = OneHotEncoder(sparse=False)
Y_full = onehot_encoder.fit_transform(integer_encoded.reshape(-1, 1))

# Compter les instances par classe
class_counts = Counter(integer_encoded)

# Listes pour stocker les données augmentées
X_augmented = []
Y_augmented = []

# Parcourir chaque classe pour appliquer l'augmentation ou la réduction
for class_idx in range(len(label_encoder.classes_)):
    indices = np.where(integer_encoded == class_idx)[0]  # Obtenir les indices des échantillons de cette classe
    count = len(indices)  # Nombre d'exemples pour cette classe

    if count < 400:  # Si moins de 400 exemples, dupliquer pour avoir 400
        repeat_factor = (400 + count - 1) // count  # Calcul du facteur de répétition
        repeated_indices = np.tile(indices, repeat_factor)  # Répéter les indices pour augmenter la classe
        selected_indices = repeated_indices[:400]  # Sélectionner uniquement les 400 premiers indices
    elif count > 500:  # Si plus de 500 exemples, réduire à 500 exemples aléatoires
        selected_indices = np.random.choice(indices, 500, replace=False)  # Sélectionner aléatoirement 500 indices
    else:  # Si entre 400 et 500 exemples, ne rien changer
        selected_indices = indices

    # Ajouter les exemples sélectionnés dans les listes d'augmentation
    X_augmented.append(X_full[selected_indices])
    Y_augmented.append(Y_full[selected_indices])

# Concaténer toutes les classes augmentées
X = np.vstack(X_augmented)
Y = np.vstack(Y_augmented)

# Séparer les données en ensembles d'entraînement, validation et test
X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)
X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42, stratify=Y_temp)

# Afficher la distribution après équilibrage
print("Distribution après équilibrage :")
for i, c in enumerate(label_encoder.classes_):
    print(f"{c}: {(Y[:, i] == 1).sum()}")

# Observer le nombre d'exemples par classe dans chaque partition
def print_class_distribution(Y, label_encoder):
    # Somme des occurrences de chaque classe
    class_counts = np.sum(Y, axis=0)
    print("Distribution des classes :")
    for i, c in enumerate(label_encoder.classes_):
        print(f"{c}: {class_counts[i]}")

# Distribution dans X_train, X_val, X_test
print("Distribution dans X_train :")
print_class_distribution(Y_train, label_encoder)

print("\nDistribution dans X_val :")
print_class_distribution(Y_val, label_encoder)

print("\nDistribution dans X_test :")
print_class_distribution(Y_test, label_encoder)

# Définir une fonction pour entraîner et évaluer un modèle
def evaluate_model(model, X_train, Y_train, X_val, Y_val, X_test, Y_test):
    # Entraîner le modèle
    model.fit(X_train, Y_train)
    
    # Prédictions sur les ensembles de validation et de test
    Y_val_pred = model.predict(X_val)
    Y_test_pred = model.predict(X_test)
    
    # Calculer les scores de précision
    val_accuracy = accuracy_score(np.argmax(Y_val, axis=1), np.argmax(Y_val_pred, axis=1))
    test_accuracy = accuracy_score(np.argmax(Y_test, axis=1), np.argmax(Y_test_pred, axis=1))
    
    print(f"Validation Accuracy: {val_accuracy * 100:.2f}%")
    print(f"Test Accuracy: {test_accuracy * 100:.2f}%")
    
# Définir les modèles
models = {
    "SVM": SVC(kernel='linear'),
    "XGBoost": XGBClassifier(),
    "KNN": KNeighborsClassifier(),
    "Categorical Naive Bayes": CategoricalNB(),
    "Complement Naive Bayes": ComplementNB()
}

# Évaluer tous les modèles
for model_name, model in models.items():
    print(f"\nÉvaluation de {model_name} :")
    evaluate_model(model, X_train, Y_train, X_val, Y_val, X_test, Y_test)


import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Exemple de données (temps de rétention et classes associées)
pic_times = [0.1, 0.2, 0.15, 0.25, 0.3, 0.4, 0.35]  # Les temps de rétention des pics
component_names = ["comp1", "comp2", "comp1", "comp3", "comp1", "comp2", "comp3"]  # Les classes des pics

# Encodage des classes
label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(component_names)

# Reshaping des données pour K-NN (temps de rétention en 2D)
X = np.array(pic_times).reshape(-1, 1)
y = integer_encoded

# Séparation des données en jeu d'entraînement et test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Initialisation du modèle K-NN
knn = KNeighborsClassifier(n_neighbors=1)  # Le plus proche voisin (k=1)
knn.fit(X_train, y_train)

# Prédictions sur l'ensemble de test
y_pred = knn.predict(X_test)

# Calcul de l'exactitude (accuracy)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy * 100:.2f}%")

# Afficher les prédictions et les classes réelles
print("\nPrédictions sur X_test :")
for i, time in enumerate(X_test):
    predicted_class = label_encoder.inverse_transform([y_pred[i]])[0]
    actual_class = label_encoder.inverse_transform([y_test[i]])[0]
    print(f"Temps de rétention: {time[0]}, Prédiction: {predicted_class}, Réel: {actual_class}")
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler

# Normalisation des temps de rétention
scaler = StandardScaler()
X_scaled = scaler.fit_transform(np.array(pic_times).reshape(-1, 1))

# Définir les paramètres à tester
param_grid = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'metric': ['minkowski', 'euclidean', 'manhattan'],
    'p': [1, 2]  # Manhattan (1) ou Euclidienne (2)
}

# Initialiser le modèle K-NN
knn = KNeighborsClassifier()

# Effectuer la recherche sur grille
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_scaled, Y_full)  # Y_full étant les labels one-hot

# Afficher les meilleurs hyperparamètres trouvés
print("Meilleurs hyperparamètres : ", grid_search.best_params_)
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV

# Normalisation des temps de rétention
scaler = StandardScaler()
X_scaled = scaler.fit_transform(np.array(pic_times).reshape(-1, 1))

# Définir les paramètres à tester
param_grid = {
    'hidden_layer_sizes': [(50,), (100,), (50, 50)],
    'activation': ['relu', 'tanh'],
    'learning_rate': ['constant', 'adaptive']
}

# Initialiser le modèle MLP
mlp = MLPClassifier()

# Effectuer la recherche sur grille
grid_search = GridSearchCV(mlp, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_scaled, Y_full)

# Afficher les meilleurs hyperparamètres trouvés
print("Meilleurs hyperparamètres : ", grid_search.best_params_)
