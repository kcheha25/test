import cv2
import numpy as np
import matplotlib.pyplot as plt
import pywt
from scipy.signal import find_peaks
from scipy.fftpack import fft

def rotate_and_crop(image, bbox):
    """
    Effectue une rotation et extrait la région d'intérêt (ROI) en tenant compte de l'angle.
    """
    x, y, w, h, angle_rad, _ = bbox
    x, y, w, h = map(int, [x, y, w, h])
    
    angle_deg = np.degrees(angle_rad)
    center = (x, y)
    M = cv2.getRotationMatrix2D(center, -angle_deg, 1.0)

    rotated_img = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]),
                                 flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)

    x1, y1 = int(x - w / 2), int(y - h / 2)
    x2, y2 = int(x + w / 2), int(y + h / 2)

    roi = rotated_img[max(y1, 0):min(y2, image.shape[0]), max(x1, 0):min(x2, image.shape[1])]
    
    return roi

def integrate_in_orientation(image, bbox):
    """
    Projette les intensités dans la direction de l'angle de la bbox.
    """
    x, y, w, h, angle_rad, _ = bbox
    roi = rotate_and_crop(image, bbox)

    gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)

    angle_deg = np.degrees(angle_rad)
    direction = np.array([np.cos(angle_rad), np.sin(angle_rad)])

    rows, cols = gray_roi.shape
    projection = np.zeros(rows)

    for i in range(rows):
        for j in range(cols):
            x_proj = int(i + direction[0] * j)
            y_proj = int(j + direction[1] * i)

            if 0 <= x_proj < rows and 0 <= y_proj < cols:
                projection[i] += gray_roi[x_proj, y_proj]
    
    return projection

def count_sheets_in_bbox(image, bbox):
    """
    Applique plusieurs méthodes d'analyse sur le profil d'intensité :
    1. Détection des pics
    2. Analyse spectrale FFT
    3. Décomposition multi-niveaux (wavedec)
    4. Décomposition simple (dwt)
    """
    intensity_profile = integrate_in_orientation(image, bbox)

    # 1. Détection des pics
    peaks, _ = find_peaks(intensity_profile, distance=max(5, int(len(intensity_profile) * 0.1)), 
                          height=np.max(intensity_profile) * 0.3)

    # 2. Spectre FFT
    fft_spectrum = np.abs(fft(intensity_profile))
    fft_freqs = np.fft.fftfreq(len(intensity_profile))

    # Détection de la fréquence dominante
    peak_idx = np.argmax(fft_spectrum[1:]) + 1  # On ignore la fréquence DC (0)
    dominant_freq = abs(fft_freqs[peak_idx])

    # 3. Décomposition multi-niveaux par ondelettes de Haar
    coeffs = pywt.wavedec(intensity_profile, 'haar', level=3)
    haar_spectrum = coeffs[0]  # Approximation de bas niveau

    # 4. Décomposition simple (DWT niveau 1)
    cA, cD = pywt.dwt(intensity_profile, 'haar')

    # Affichage des résultats
    fig, axes = plt.subplots(2, 3, figsize=(18, 8))

    # (a) Profil d'intensité avec pics
    axes[0, 0].plot(intensity_profile, label="Profil d'intensité")
    axes[0, 0].scatter(peaks, intensity_profile[peaks], color='r', label="Pics détectés")
    axes[0, 0].set_title(f"Find Peaks - {len(peaks)} feuillets détectés")
    axes[0, 0].legend()

    # (b) Spectre FFT brut
    axes[0, 1].plot(fft_freqs[:len(fft_freqs)//2], fft_spectrum[:len(fft_spectrum)//2])
    axes[0, 1].set_title("Spectre FFT - Brute")

    # (c) Spectre FFT avec fréquence dominante
    axes[0, 2].plot(fft_freqs[:len(fft_freqs)//2], fft_spectrum[:len(fft_spectrum)//2], label="FFT Spectrum")
    axes[0, 2].axvline(dominant_freq, color='r', linestyle='--', label=f"Fréquence dominante: {dominant_freq:.2f}")
    axes[0, 2].set_title("Spectre FFT avec pic marqué")
    axes[0, 2].legend()

    # (d) Décomposition multi-niveaux (wavedec)
    axes[1, 0].plot(haar_spectrum)
    axes[1, 0].set_title("Spectre Haar (wavedec)")

    # (e) Décomposition simple (DWT niveau 1)
    axes[1, 1].plot(cA, label="Approximation (cA)")
    axes[1, 1].plot(cD, label="Détails (cD)", linestyle="dashed")
    axes[1, 1].set_title("DWT Niveau 1 - Approximation & Détails")
    axes[1, 1].legend()

    # (f) ROI extraite
    roi = rotate_and_crop(image, bbox)
    axes[1, 2].imshow(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))
    axes[1, 2].set_title("ROI alignée")

    plt.tight_layout()
    plt.show()

    return len(peaks)

# Chargement de l'image et exécution de MMRotate
image_path = "path_to_your_image.jpg"
image = cv2.imread(image_path)

# Résultats de l'inférence MMRotate
result = inference_detector(model, image)

# Boucle sur chaque bbox détectée
for bbox in result[0]:  
    num_sheets = count_sheets_in_bbox(image, bbox)
    print(f"Nombre estimé de feuillets dans la bbox {bbox[:4]} : {num_sheets}")

import cv2
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import find_peaks
from sklearn.cluster import KMeans

def apply_kmeans_segmentation(roi):
    """
    Applique K-means clustering à 2 classes sur une ROI en niveaux de gris.
    """
    gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    
    # Mise en forme des données pour K-means (transforme en un vecteur de pixels)
    pixels = gray_roi.reshape(-1, 1)
    
    # Application du K-means clustering
    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
    kmeans.fit(pixels)
    labels = kmeans.labels_.reshape(gray_roi.shape)  # Remettre sous forme d'image
    
    # Création d'une image couleur pour la visualisation des clusters
    segmented_img = np.zeros_like(roi)
    segmented_img[labels == 0] = [255, 0, 0]  # Bleu pour une classe
    segmented_img[labels == 1] = [0, 255, 0]  # Vert pour l'autre
    
    return segmented_img

def count_sheets_with_kmeans(image, bbox):
    """
    Compte les feuillets dans une bbox et applique K-means clustering sur la ROI.
    """
    roi = rotate_and_crop(image, bbox)
    
    # Appliquer K-means clustering
    segmented_roi = apply_kmeans_segmentation(roi)
    
    # Calcul du profil d'intensité projeté
    intensity_profile = integrate_in_orientation(image, bbox)
    
    # Détection des pics dans le profil d'intensité projeté
    peaks, _ = find_peaks(intensity_profile, distance=max(5, int(bbox[3] * 0.1)),
                          height=np.max(intensity_profile) * 0.3)
    
    # Affichage des résultats
    plt.figure(figsize=(12, 6))
    
    # Affichage du profil d'intensité projeté avec pics détectés
    plt.subplot(1, 3, 1)
    plt.plot(intensity_profile)
    plt.scatter(peaks, intensity_profile[peaks], color='r')
    plt.title(f"Profil d'intensité - {len(peaks)} feuillets détectés")
    
    # Affichage de la ROI originale
    plt.subplot(1, 3, 2)
    plt.imshow(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))
    plt.title("ROI alignée")
    
    # Affichage de la segmentation K-means
    plt.subplot(1, 3, 3)
    plt.imshow(segmented_roi)
    plt.title("Segmentation K-means")
    
    plt.show()
    
    return len(peaks)

# Exemple d'utilisation avec une image et des bbox détectées
image_path = "path_to_your_image.jpg"
image = cv2.imread(image_path)

# Résultats de l'inférence du modèle de détection (ex: MMRotate)
result = inference_detector(model, image)

for bbox in result[0]:  
    num_sheets = count_sheets_with_kmeans(image, bbox)
    print(f"Nombre estimé de feuillets dans la bbox {bbox[:4]} : {num_sheets}")
def apply_kmeans_segmentation(roi, n_clusters=2):
    """
    Applique K-means clustering avec un nombre dynamique de classes sur une ROI en niveaux de gris.
    """
    gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
    
    # Mise en forme des données pour K-means (transforme en un vecteur de pixels)
    pixels = gray_roi.reshape(-1, 1)
    
    # Application du K-means clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    kmeans.fit(pixels)
    labels = kmeans.labels_.reshape(gray_roi.shape)  # Remettre sous forme d'image
    
    # Générer des couleurs dynamiques pour chaque cluster
    colors = np.random.randint(0, 255, (n_clusters, 3), dtype=np.uint8)
    
    # Création d'une image couleur pour visualisation des clusters
    segmented_img = np.zeros_like(roi)
    for k in range(n_clusters):
        segmented_img[labels == k] = colors[k]
    
    return segmented_img

    import cv2
import numpy as np
import matplotlib.pyplot as plt

import os

def yolo_to_dota(yolo_file, img_width, img_height, class_names, output_dir):
    """
    Convertit un fichier d'annotations YOLO en format DOTA.
    yolo_file : Chemin vers le fichier d'annotations YOLO (.txt).
    img_width, img_height : Largeur et hauteur de l'image en pixels.
    class_names : Liste des noms de classes pour l'indexation.
    output_dir : Répertoire où les fichiers DOTA seront enregistrés.
    """
    
    with open(yolo_file, 'r') as f:
        annotations = f.readlines()

    dota_annotations = []
    
    for annotation in annotations:
        parts = annotation.strip().split()
        
        # Convertir les coordonnées de YOLO en coordonnées réelles
        class_id = int(parts[0])
        x_center = float(parts[1]) * img_width
        y_center = float(parts[2]) * img_height
        width = float(parts[3]) * img_width
        height = float(parts[4]) * img_height
        
        # Calculer les coins de la bounding box (rectangle orienté)
        x1 = x_center - width / 2
        y1 = y_center - height / 2
        x2 = x_center + width / 2
        y2 = y_center - height / 2
        x3 = x_center + width / 2
        y3 = y_center + height / 2
        x4 = x_center - width / 2
        y4 = y_center + height / 2
        
        # Formater la ligne pour DOTA
        dota_annotation = f"{x1} {y1} {x2} {y2} {x3} {y3} {x4} {y4} {class_id} 0"
        dota_annotations.append(dota_annotation)
    
    # Enregistrer les annotations DOTA dans un fichier
    output_file = os.path.join(output_dir, os.path.basename(yolo_file).replace('.txt', '.txt'))
    with open(output_file, 'w') as f:
        f.write("\n".join(dota_annotations))
    
    print(f"Conversion terminée pour {yolo_file}. Les annotations DOTA ont été enregistrées dans {output_file}.")

def convert_yolo_to_dota(yolo_dir, img_dir, class_names, output_dir):
    """
    Convertit toutes les annotations YOLO d'un répertoire en format DOTA.
    yolo_dir : Répertoire contenant les fichiers d'annotations YOLO (.txt).
    img_dir : Répertoire contenant les images (utilisé pour obtenir la taille des images).
    class_names : Liste des noms de classes.
    output_dir : Répertoire où les fichiers DOTA seront enregistrés.
    """
    
    os.makedirs(output_dir, exist_ok=True)

    for yolo_file in os.listdir(yolo_dir):
        if yolo_file.endswith('.txt'):
            img_path = os.path.join(img_dir, yolo_file.replace('.txt', '.jpg'))  # Assumer que les images sont en .jpg
            img = cv2.imread(img_path)
            img_height, img_width = img.shape[:2]
            
            yolo_to_dota(os.path.join(yolo_dir, yolo_file), img_width, img_height, class_names, output_dir)

# Exemple d'utilisation
class_names = ["plane", "car", "bus", "train"]  # Liste des classes
yolo_dir = 'path/to/yolo/annotations'
img_dir = 'path/to/images'
output_dir = 'path/to/output/dota_annotations'

convert_yolo_to_dota(yolo_dir, img_dir, class_names, output_dir)


    projections = integrate_in_orientation(image, bbox)

    # Identifier la projection avec la plus grande valeur individuelle
    projections_values = [
        (max(projections[0]), 'Projection principale'),
        (max(projections[1]), 'Projection perpendiculaire'),
        (max(projections[2]), 'Projection diagonale 1'),
        (max(projections[3]), 'Projection diagonale 2')
    ]

    # Trouver la projection avec la plus grande valeur
    max_value_proj, max_proj_name = max(projections_values, key=lambda x: x[0])

    # Choisir la projection ayant la plus grande valeur
    if max_proj_name == 'Projection principale':
        max_value_proj = projections[0]
    elif max_proj_name == 'Projection perpendiculaire':
        max_value_proj = projections[1]
    elif max_proj_name == 'Projection diagonale 1':
        max_value_proj = projections[2]
    else:
        max_value_proj = projections[3]

    # Appliquer find_peaks sur la projection avec la plus grande valeur
    peaks, _ = find_peaks(max_value_proj, distance=max(5, int(len(max_value_proj) * 0.1)),
                          height=np.max(max_value_proj) * 0.3)

    # Affichage du résultat
    print(f"Projection choisie : {max_proj_name}")


import cv2
import numpy as np
import matplotlib.pyplot as plt

def read_dota_annotations(txt_file):
    """Lire les annotations DOTA depuis un fichier texte."""
    annotations = []
    
    with open(txt_file, 'r') as f:
        for line in f.readlines():
            data = line.strip().split()
            # Les 8 premiers éléments correspondent aux coordonnées des coins du polygone
            polygon = list(map(float, data[:8]))  # x1, y1, x2, y2, x3, y3, x4, y4
            annotations.append(polygon)
    
    return annotations

def draw_dota_annotations(image, annotations):
    """Dessiner les annotations DOTA (polygones) sur l'image."""
    for polygon in annotations:
        # Convertir les coordonnées du polygone en format numpy array pour pouvoir dessiner
        pts = np.array(polygon).reshape((-1, 1, 2)).astype(np.int32)
        # Dessiner le polygone en rouge
        cv2.polylines(image, [pts], isClosed=True, color=(0, 0, 255), thickness=2)

def visualize_image_with_annotations(image_path, txt_file):
    """Afficher l'image avec les annotations DOTA."""
    # Charger l'image
    image = cv2.imread(image_path)
    
    # Lire les annotations DOTA depuis le fichier texte
    annotations = read_dota_annotations(txt_file)
    
    # Dessiner les annotations sur l'image
    draw_dota_annotations(image, annotations)
    
    # Convertir l'image BGR en RGB pour l'affichage avec Matplotlib
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    # Afficher l'image avec les annotations
    plt.figure(figsize=(10, 10))
    plt.imshow(image_rgb)
    plt.axis('off')
    plt.show()

# Exemple d'utilisation :
image_path = 'path/to/your/image.jpg'  # Chemin vers l'image
txt_file = 'path/to/your/image.txt'    # Chemin vers le fichier d'annotations .txt

visualize_image_with_annotations(image_path, txt_file)

import os
import cv2
import numpy as np
from sklearn.svm import SVR  # Support Vector Regression (régression)
from sklearn.preprocessing import StandardScaler
from skimage.feature import hog
from sklearn.model_selection import train_test_split

# Fonction pour compter le nombre d'objets dans un fichier d'annotation DOTA
def count_objects_in_dota_annotation(txt_file):
    """Compter le nombre d'objets dans un fichier d'annotation DOTA."""
    with open(txt_file, 'r') as f:
        annotations = f.readlines()
    return len(annotations)  # Retourne le nombre d'objets (lignes dans le fichier)

# Fonction pour extraire des caractéristiques HOG d'une image
def extract_hog_features(image_path):
    """Extraire les caractéristiques HOG d'une image."""
    img = cv2.imread(image_path)
    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convertir en niveaux de gris
    
    # Extraire les caractéristiques HOG
    fd, hog_image = hog(gray_img, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)
    return fd  # Retourner les descripteurs HOG

# Fonction pour créer les jeux de données
def create_dataset(image_dir, annotation_dir):
    features = []
    labels = []
    
    # Lister toutes les images dans le dossier
    image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg') or f.endswith('.png')]
    
    for image_path in image_paths:
        # Trouver l'annotation correspondante
        txt_file = os.path.join(annotation_dir, os.path.basename(image_path).replace('.jpg', '.txt').replace('.png', '.txt'))
        
        if os.path.exists(txt_file):
            # Extraire les caractéristiques HOG
            hog_features = extract_hog_features(image_path)
            features.append(hog_features)
            
            # Extraire le nombre d'objets à partir des annotations DOTA
            num_objects = count_objects_in_dota_annotation(txt_file)
            labels.append(num_objects)
    
    return np.array(features), np.array(labels)

# Fonction pour entraîner le modèle
def train_model(features, labels):
    """Entraîner un modèle SVM avec les caractéristiques HOG."""
    # Normaliser les caractéristiques (standardisation)
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(features)
    
    # Créer un modèle SVM pour prédire le nombre d'objets
    model = SVR(kernel='linear')  # Utilisation de SVR (Support Vector Regression)
    model.fit(features_scaled, labels)
    
    return model, scaler

def main():
    # Chemins vers les images et annotations
    image_dir = 'path_to_images'  # Répertoire des images
    annotation_dir = 'path_to_annotations'  # Répertoire des annotations DOTA
    
    # Créer le dataset
    features, labels = create_dataset(image_dir, annotation_dir)
    
    # Diviser en jeux d'entraînement (ici on n'utilise pas de test, car peu de données)
    features_train, labels_train = features, labels
    
    # Entraîner le modèle
    model, scaler = train_model(features_train, labels_train)
    
    print("Modèle entraîné avec succès !")
    
    # Sauvegarder le modèle et le scaler pour une utilisation future
    import joblib
    joblib.dump(model, 'svm_model.pkl')
    joblib.dump(scaler, 'scaler.pkl')

if __name__ == '__main__':
    main()


model = joblib.load('svm_model.pkl')
scaler = joblib.load('scaler.pkl')

# Pour une nouvelle image
new_image_path = 'new_image.jpg'
hog_features = extract_hog_features(new_image_path)
hog_features_scaled = scaler.transform([hog_features])
predicted_num_objects = model.predict(hog_features_scaled)

print(f"Nombre d'objets prédit dans l'image : {predicted_num_objects[0]}")
